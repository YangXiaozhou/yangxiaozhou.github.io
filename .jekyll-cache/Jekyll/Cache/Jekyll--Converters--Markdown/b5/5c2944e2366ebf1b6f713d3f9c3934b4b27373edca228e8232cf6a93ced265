I"B'<p>This is a collection of my notes on various topics of statistical learning. It is intended as a knowledge repository for myself and a work in progress that I will periodically update.</p>

<ul id="markdown-toc">
  <li><a href="#ridge-regularization" id="markdown-toc-ridge-regularization">Ridge regularization</a>    <ul>
      <li><a href="#relationship-between-ols-ridge-regression-and-pca" id="markdown-toc-relationship-between-ols-ridge-regression-and-pca">Relationship between OLS, Ridge regression, and PCA</a></li>
    </ul>
  </li>
</ul>
<hr />

<h1 id="ridge-regularization">Ridge regularization</h1>
<h2 id="relationship-between-ols-ridge-regression-and-pca">Relationship between OLS, Ridge regression, and PCA</h2>
<p>Simple yet elegant relationships between OLS estimates, ridge estimates and PCA can be found through the lens of spectral decomposition. We see these relationships through Exercise 8.8.1 of MA<sup id="fnref:MA"><a href="#fn:MA" class="footnote">1</a></sup>.</p>

<h3 id="set-up">Set-up</h3>

<p>Given the following regression model:</p>

<script type="math/tex; mode=display">\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\mu \mathbf{1}+\mathbf{u}, \quad \mathbf{u} \sim N_{\mathrm{n}}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right),</script>

<p>consider the columns of $\mathbf{X}$ have been standardized to have mean 0 and variance 1. Then the ridge estimate of $\boldsymbol{\beta}$ is $\boldsymbol{\beta}^* = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^{\top} \mathbf{y}$ where for given $\mathbf{X}$, $\lambda \ge 0$ is a small fixed ridge regularization parameter. Note that when $\lambda = 0$, it is just the OLS formulation. Also consider the spectral decomposition of the var-cov matrix $\mathbf{X}^{\top} \mathbf{X} = \mathbf{G} \mathbf{L} \mathbf{G}^{\top}$. Let $\mathbf{W} = \mathbf{X}\mathbf{G}$ be the principal component transformation of the original data matrix.</p>

<h3 id="result-11">Result 1.1</h3>

<p>If $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ represents the parameter vector of principal components, then we can show that the ridge estimates $\boldsymbol{\alpha}^*$ can be obtained from OLS estimates $\hat{\boldsymbol{\alpha}}$ by simply scaling them with the ridge regularization parameter:</p>

<script type="math/tex; mode=display">\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p.</script>

<p>This result shows us two important insights:</p>
<ol>
  <li>For PC-transformed data, we can obtain the ridge estimates through a simple element-wise scaling of the OLS estimates.</li>
  <li>The shrinking effect of ridge regularization depends on both $\lambda$ and eigenvalues of the corresponding PC:
    <ul>
      <li>Larger $\lambda$ corresponds to heavier shrinking for every parameter.</li>
      <li>However, given the same $\lambda$, principal components corresponding to larger eigenvalues receive the least shrinking.</li>
    </ul>
  </li>
</ol>

<p>The two shrinking effects are clearly visible from this figure. 
<img src="/assets/learning-repo/pca_ridge_lambda_effect.png" alt="lambda_effect" /></p>

<details>
    <summary>Proof of Result 1.1</summary>
Since $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ and $\mathbf{W} = \mathbf{X}\mathbf{G}$, then 

$$
\begin{align*}
\boldsymbol{\alpha}^* &amp;= (\mathbf{W}^{\top}\mathbf{W} + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= (\Lambda + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= \operatorname{diag}((l_{i} + \lambda)^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

Hence 

$$
\alpha^*_{i} = (l_i + \lambda)^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$ 

for $i=1, \ldots, p$, where $\mathbf{w}_{(i)}$ is the $i$th column of $\mathbf{W}$. Since 

$$
\begin{align*}
\hat{\boldsymbol{\alpha}} &amp;= (\mathbf{W}^{\top}\mathbf{W})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= \operatorname{diag}(l_{i}^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

We have 

$$
\hat{\alpha}_{i} = l_i^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$

for $i=1, \ldots, p$. Therefore, by comparing the two estimate expressions, we have the result

$$
\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p. \blacksquare
$$


</details>

<h3 id="result-12">Result 1.2</h3>

<p>It follows from Result 1.1 that we can establish a direct link between the OLS estimates $\hat{\boldsymbol{\beta}}$ and the ridge estimates $\boldsymbol{\beta}^*$ through spectral decomposition of the var-cov matrix. Specifically, we have</p>

<script type="math/tex; mode=display">\boldsymbol{\beta}^{*}=\mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}, \quad \text { where } \mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right),</script>

<p>for $i=1, \ldots, p$.</p>

<details>
    <summary>Proof of Result 1.2</summary>
Since $\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}$, for $i=1, \ldots, p$, and $\hat{\boldsymbol{\alpha}} = \mathbf{G}^{\top}\hat{\boldsymbol{\beta}}$, then

$$
\boldsymbol{\alpha}^{*} =  \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \,,
$$

by writing in matrix form. Also, since 

$$
\boldsymbol{\alpha}^* = \mathbf{G}^{\top}\boldsymbol{\beta}^*
$$

where $\boldsymbol{\beta}^*$ is the ridge estimates of $\boldsymbol{\beta}$, then we have

$$
\begin{align*}
\mathbf{G}^{\top}\boldsymbol{\beta}^* &amp;= \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
\boldsymbol{\beta}^* &amp;= \mathbf{G} \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
&amp;= \mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}
\end{align*}
$$

where $\mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right)$, for $i=1, \ldots, p$. $\blacksquare$
</details>

<h3 id="result-13">Result 1.3</h3>

<p>One measure of the quality of the estimators $\boldsymbol{\beta}^*$ is the trace mean square error (MSE):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\phi(\lambda) &= \operatorname{tr} E[(\boldsymbol{\beta}^* - \boldsymbol{\beta})(\boldsymbol{\beta}^* - \boldsymbol{\beta})^{\top}] \\
&= \sum_{i=1}^{p} E[(\beta_{i}^* - \beta_{i})^2] \,.
\end{align*} %]]></script>

<p>Now, from the previous two results, we can show that the trace MSE of the ridge estimates can be decomposed into two parts: <strong>variance</strong> and <strong>bias</strong>, and obtain explicit formula for them. The availability of exact formula for MSE allows things like regularization path to be computed easily.</p>

<p>Specifically, we have</p>

<script type="math/tex; mode=display">\phi(\lambda) = \gamma_1(\lambda) + \gamma_2(\lambda)</script>

<p>where the first component is the sum of variances:</p>

<script type="math/tex; mode=display">\gamma_1(\lambda) = \sum_{i=1}^{p} V(\beta_{i}^*) = \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,,</script>

<p>and the second component is the sum of squared biases:</p>

<script type="math/tex; mode=display">\gamma_2(\lambda) = \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 = \lambda^2 \sum_{i=1}^{p} \frac{\alpha_i^2}{(l_i + \lambda)^2} \,.</script>

<details><summary>Proof of Result 1.3</summary>
First let's look at the sum of variances. We start by writing out the expression for the variance of the ridge estimates:

$$
\begin{align*}
V(\boldsymbol{\beta}^*) &amp;= \mathbf{GDG}^{\top} V(\hat{\boldsymbol{\beta}}) \mathbf{GDG}^{\top} \\
&amp;= \mathbf{GDG}^{\top} \sigma^2 \mathbf{X^{\top}X}^{-1} \mathbf{GDG}^{\top} \\
&amp;= \sigma^2 \mathbf{GDG}^{\top} (\mathbf{GLG})^{-1} \mathbf{GDG}^{\top} \\
&amp;= \sigma^2 \mathbf{GD} L^{-1} \mathbf{DG}^{\top} \\
&amp;= \sigma^2 \mathbf{G} \operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \,, \quad i=1, \ldots, p.
\end{align*}
$$ 

Hence, by extracting out the variances from the diagonal, we obtain the first expression:

$$
\begin{align*}
\gamma_1(\lambda) &amp;= \sum_{i=1}^{p} V(\beta_{i}^*) \\
&amp;= \operatorname{tr} V(\boldsymbol{\beta}^*) \\
&amp;= \sigma^2 \operatorname{tr}(\operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \mathbf{G}) \\
&amp;= \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,.
\end{align*}
$$

Now let's look at the bias term. We write it in matrix form to see that:

$$
\begin{align*}
\gamma_2(\lambda) &amp;= \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 \\
&amp;= (E[\boldsymbol{\beta}^*] - \boldsymbol{\beta})^{\top}(E[\boldsymbol{\beta}^*] - \boldsymbol{\beta}) \\
&amp;= (\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta})^{\top}(\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta}) \\
&amp;= \mathbf{B}^{\top}\mathbf{GD}^2\mathbf{G}^{\top}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\top}\mathbf{GDG^{\top}}\boldsymbol{\beta} + \boldsymbol{\beta}^{\top}\boldsymbol{\beta} \\
&amp;= \boldsymbol{\alpha}^{\top}\mathbf{D}^2\boldsymbol{\alpha} - 2\boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha} + \boldsymbol{\alpha}^{\top}\mathbf{G^{\top}G}\boldsymbol{\alpha} \\
&amp;= \boldsymbol{\alpha}^{\top} (\mathbf{D}^2 - 2\mathbf{D} + 1) \boldsymbol{\alpha} \\
&amp;= \boldsymbol{\alpha}^{\top} \operatorname{diag}(\lambda^2/(l_i + \lambda)^2) \boldsymbol{\alpha} \\
&amp;= \lambda^2 \sum_{i=1}^{p}(\alpha_{i}^2/(l_i + \lambda)^2) \,.
\end{align*}
$$

Combining the two gamma terms completes the proof. $\blacksquare$
</details>

<h3 id="result-14">Result 1.4</h3>

<p>This is a quick but revealing result that follows from Result 1.3. Taking a partial derivative of the trace MSE function with respect to $\lambda$ and take $\lambda = 0$, we get</p>

<script type="math/tex; mode=display">\partial{s}{s}</script>

<h1 class="no_toc" id="references">References</h1>

<div class="footnotes">
  <ol>
    <li id="fn:MA">
      <p>Mardia, K. V., Kent, J. T., &amp; Bibby, J. M. <em>Multivariate Analysis</em>. 1979. Probability and mathematical statistics. Academic Press Inc.Â <a href="#fnref:MA" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET