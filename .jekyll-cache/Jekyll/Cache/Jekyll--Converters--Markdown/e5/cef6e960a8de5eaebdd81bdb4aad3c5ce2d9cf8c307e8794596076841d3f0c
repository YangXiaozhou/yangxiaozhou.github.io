I"‚9<p>By now, you‚Äôve probably seen a few, if not dozens of, articles on how deep learning could help detect COVID-19[refs]. In particular, convolutional neural networks (CNNs) have been studied as a faster and cheaper alternative to the gold-standard PCR test by just analyzing the patient‚Äôs computed tomography (CT) scan. It‚Äôs not surprising since CNN is excellent at image recognition; Many places have CT scanners rather than COVID-19 testing kits (at least initially)[refs].</p>

<p>But, despite its success on image recognition tasks such as the ImageNet challenge, can CNN really help doctors detect COVID-19? If it can, how accurately can it do so? It‚Äôs well known that CT scans are sensitive but not specific to COVID-19[refs]. That is, COVID-19 almost always produces abnormal lung patterns visible from CT scans. However, other pneumonia can produce the same abnormal patterns. Can the powerful and sometimes magical CNN tackle this ambiguity issue?</p>

<p>We had a chance to answer these questions ourselves (with my colleague <a href="">Yuchen</a> and advisor <a href="">A/P Chen</a>). I‚Äôll walk you through a COVID-19 classifier that we‚Äôve built as an entry to the 2020 INFORMS QSR Data <a href="https://connect.informs.org/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=f404f7b8-fcd6-75d5-f7a7-d262eab132e7">Challenge</a>. If you are not familiar with CNN, or would like a refresher on the key features of CNNs, I highly recommend reading <a href="https://yangxiaozhou.github.io/data/2020/09/24/intro-to-cnn.html">Convolutional Neural Network: How is it different from the other networks?</a> first. Also, if you‚Äôd like to get hands-on, you can get all the code and data from my Github <a href="https://github.com/YangXiaozhou/CNN-COVID-19-classification-using-chest-CT-scan">repo</a>.</p>

<h2 id="key-takeaways">Key takeaways</h2>
<ol>
  <li>Transfer learning using pre-trained CNN can achieve really strong baseline performance on COVID-19 classification (85% accuracy).</li>
  <li>However, domain expertise is required to elevate the performance of the CNN (or other ML methods).</li>
</ol>

<hr />
<h1 id="whats-the-challenge">What‚Äôs the challenge?</h1>

<p>The COVID-19 pandemic has changed lives around the world. This is the current situation as of 2020/09/26 according to <a href="https://covid19.who.int/">WHO</a>. 
<img src="/assets/cnn-covid-19/covid-19-pandemic.png" alt="current situation" /></p>

<p>CT scans have been used for screening and diagnosing COVID-19, especially in areas where swab test resources are severely lacking. The goal of this data challenge is to diagnose COVID-19 using the chest CT scans. Therefore, we need to build a <strong>classification model</strong> that can classify patients to COVID or NonCOVID based on their chest CT scans, <strong>as accurately as possible</strong>.</p>

<h2 id="whats-provided">What‚Äôs provided?</h2>
<p>Relatively even number of COVID and NonCOVID images are provided to train the model. While meta-information of these images are also provided, they will not be provided during testing. A constraint of the competition is that the training of the model with provided data must take less than one hour.</p>

<ul>
  <li>Training data set
    <ul>
      <li>251 COVID-19 CT images</li>
      <li>292 non-COVID-19 CT images</li>
    </ul>
  </li>
  <li>Meta-information
    <ul>
      <li>e.g., patient information, severity, image caption</li>
    </ul>
  </li>
</ul>

<p>All challenge data are taken from a public <a href="https://github.com/UCSD-AI4H/COVID-CT">data set</a>.</p>

<h1 id="model-performance">Model performance</h1>
<p>Let‚Äôs first take a look at the end result, shall we?</p>

<p>The trained model is evaluated with an independent set of test data. Here you can see the confusion matrix. The overall accuracy is about 85% with a slightly better sensitivity than specificity, i.e., true positive rate &gt; true negative rate. 
<img src="/assets/cnn-covid-19/confusion_matrix.png" alt="confusion" /></p>

<h1 id="implementation">Implementation</h1>
<p>Here are some of the provided NonCOVID and COVID CT scans. It‚Äôs important to note that the challenge is to distinguish between COVID and NonCOVID CT scans, rather than COVID and Normal scans. In fact, there may be some NonCOVID CT scans that belong to other pneumonia patients. 
<img src="/assets/cnn-covid-19/first_look.png" alt="first_look" /></p>

<h2 id="train-validation-split">Train-validation split</h2>
<p>We reserve 20% of the data for validation. Since some consecutive images come from the same patient, they tend to be similar to each other.  That is, many of our data are <strong>not independent</strong>. To prevent data leakage (information of training data spills over to validation data), we keep the original image sequence and hold out the last 20% as the validation set.</p>

<p>After the splitting, we have two pairs of data:</p>
<ol>
  <li><code class="highlighter-rouge">X_train</code>, <code class="highlighter-rouge">y_train</code></li>
  <li><code class="highlighter-rouge">X_val</code>, <code class="highlighter-rouge">y_val</code></li>
</ol>

<p>X is a list of CT scans, and y is a list of binary labels (0 for NonCOVID, 1 for COVID).</p>

<h2 id="data-augmentation">Data augmentation</h2>
<p>Data augmentation is a common way to include more random variations into the training data. This helps to prevent overfitting. For image-related learning problems, augmentation typically means applying <strong>random</strong> geometric (e.g., crop, flip, rotate and etc.) and appearance transformation (e.g., contrast, edge filter, Gaussian blur and etc.). Here we use <code class="highlighter-rouge">tf.keras.Sequential</code> to create a pipeline in which the input image is randomly transformed through the following operations:</p>
<ol>
  <li>Random horizontal and vertical flip</li>
  <li>Rotation by a random degree in the range of $[-5\%, 5\%]*2\pi$</li>
  <li>Random zoom in height by $5\%$</li>
  <li>Random translation by $5\%$</li>
  <li>Random contrast adjustment by $5\%$</li>
</ol>

<p>This is how they look like after the augmentation. 
<img src="/assets/cnn-covid-19/augmented_scans.png" alt="augmented_scans" /></p>

<h2 id="using-pre-trained-cnn-as-the-backbone">Using pre-trained CNN as the backbone</h2>
<p>We do not build a CNN from scratch. For an image-related problem that has only modest number of training images, it is recommended to use a pre-trained model as the backbone and do <a href="https://cs231n.github.io/transfer-learning/">transfer learning</a> on that. The chosen model is <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0">EfficientNetB0</a>. It belongs to the family of models called <a href="https://arxiv.org/abs/1905.11946">EfficientNets</a> proposed by reseachers from Google. EfficientNets are among the current state-of-the-art CNNs for computer vision tasks. They</p>
<ol>
  <li>require considerably lower number of parameters,</li>
  <li>achieved very high accuracies on ImageNet,</li>
  <li>transferred well to other image classification tasks.</li>
</ol>

<p>Here‚Äôs a performance <a href="https://arxiv.org/abs/1905.11946">comparison</a> between EfficientNets and other well-known models:<img src="/assets/cnn-covid-19/efficientnets.png" alt="EfficientNet" /></p>

<p>EfficientNets, and other well-known pre-trained models, can be easily loaded from <code class="highlighter-rouge">tf.keras.applications</code>. We first import the pre-trained EfficientNetB0 and use it as our model backbone. We remove the original output layer of EfficientNetB0 since it was trained for 1000-class classification. Also, we freeze the model‚Äôs weights so that they won‚Äôt be updated during the initial training.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Create a base model from the pre-trained EfficientNetB0
base_model = keras.applications.EfficientNetB0(input_shape=IMG_SHAPE, include_top=False)
base_model.trainable = False
</code></pre></div></div>

<h2 id="wrap-our-model-around-it">Wrap our model around it</h2>

<p>With EfficientNet imported, we can use it to our problem by wrapping our classification model around it. You can think of the EfficientNetB0 as a trained feature extractor. The final model has:</p>
<ol>
  <li>An input layer</li>
  <li><strong>EfficientNetB0 base model</strong></li>
  <li>An average pooling layer: Pool the information by average operation</li>
  <li>A dropout layer: Set a percentage of inputs to zero</li>
  <li>A classification layer: Output the probability of NonCOVID</li>
</ol>

<p>We can also use <code class="highlighter-rouge">tf.keras.utils.plot_model</code> to visualize our model. <img src="/assets/cnn-covid-19/model.png" alt="model" /></p>

<p>We can see that:</p>
<ol>
  <li>The <code class="highlighter-rouge">?</code> in input and output shape is a reserved place for the number of samples, which the model does not know yet.</li>
  <li>EfficientNetB0 sits right after the input layer.</li>
  <li>The last (classification) layer has an output of dimension 1: The probability for NonCOVID.</li>
</ol>

<h2 id="training-our-model">Training our model</h2>
<p><strong>Public data pre-training</strong>: To help EfficientNetB0 adapt to COVID vs NonCOVID image classification, we‚Äôve actually trained our model on another public CT scan data <a href="https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset">set</a>. The hope is that training the model on CT scans would allow it to learn features specific to our COVID-19 classification task. We will not go into the public data training part, but the procedure is essentially the same as what I am going to show below.</p>

<p><strong>Transfer learning workflow</strong>: We use a typical transfer-learning workflow:</p>
<ol>
  <li>Phase 1 (Feature extraction): Fix EfficientNetB0‚Äôs weights, only update the last classification layer‚Äôs weights.</li>
  <li>Phase 2 (Fine tuning): Allow some of EfficientNetB0‚Äô weights to update as well.</li>
</ol>

<p>You can read more about the workflow <a href="https://www.tensorflow.org/guide/keras/transfer_learning#the_typical_transfer-learning_workflow">here</a>.</p>

<p><strong>Key configurations</strong>: We use the following metrics and loss function:</p>
<ol>
  <li><strong>Metrics</strong>: to evaluate model performance
    <ul>
      <li>Binary accuracy</li>
      <li>False and true positives</li>
      <li>False and true negatives</li>
    </ul>
  </li>
  <li><strong>Loss function</strong>: to guide gradient search
    <ul>
      <li>Binary crossentropy</li>
    </ul>
  </li>
</ol>

<p>We use the <code class="highlighter-rouge">Adam</code> optimizer, the learning rates is set to <code class="highlighter-rouge">[1e-3, 1e-4]</code> and the number of training epochs is set to <code class="highlighter-rouge">[10, 30]</code> for the two phases, respectively. The two-phase training is iterated for two times.</p>

<p><strong>Training history</strong>: Let‚Äôs visualize the training history: <img src="/assets/cnn-covid-19/training_history.png" alt="training_history" /></p>

<p>Here you can see that after we‚Äôve allowed some layers of EfficientNets to update (after Epoch 10), we obtain a significant improvement in classification accuracy. The final training and validation accurcy is around 98% and 82%.</p>

<h1 id="how-does-it-perform-on-test-data">How does it perform on test data?</h1>
<p>From the same data repo, we can obtain a set of test data that contains 105 NonCOVID and 98 COVID images. Let‚Äôs see how the trained model perform on them. Here‚Äôs a result breakdown using <code class="highlighter-rouge">sklearn.metrics.classification_report</code>.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support

       COVID       0.85      0.83      0.84        98
    NonCOVID       0.84      0.87      0.85       105

    accuracy                           0.85       203
   macro avg       0.85      0.85      0.85       203
weighted avg       0.85      0.85      0.85       203
</code></pre></div></div>
<p>And the ROC curve: <img src="/assets/cnn-covid-19/roc_curve.png" alt="roc_curve" /></p>

<h2 id="what-are-the-correctly-and-incorrectly-classified-ct-scans">What are the correctly and incorrectly classified CT scans?</h2>

<p>We can dive into the classification result and see which are the ones identified correctly, and which are identified incorrectly. <strong>Potential patterns</strong> found could be leveraged to help further improve the model.</p>

<p><strong>Could you identify some patterns?</strong>
<img src="/assets/cnn-covid-19/true_positives.png" alt="true_positives" />
<img src="/assets/cnn-covid-19/true_negatives.png" alt="true_negatives" /></p>

<p>And here are the incorrect ones:
<img src="/assets/cnn-covid-19/false_positives.png" alt="false_positives" />
<img src="/assets/cnn-covid-19/false_negatives.png" alt="false_negatives" /></p>

<p>We can probably make several observations here:</p>
<ol>
  <li>True positives have obvious abnormal patterns and the lung structures are well-preserved.</li>
  <li>Many of the true negatives have complete black lungs (no abnormal pattern).</li>
  <li>Lung boundaries of many false positives are not clear.</li>
</ol>

<p>The point is, to a non-medical person like me, many of the COVID and NonCOVID images look the same. The ambiguity is even more severe when some images have unclear lung boundaries. It looks like our CNN is also having trouble distinguishing those images.</p>

<h1 id="where-do-we-go-from-here">Where do we go from here?</h1>

<p>It is clear from the above results that a pre-trained CNN can be adapted to achieve a really strong baseline performance. However, to take it 
There are several directions for which we could make the model better:</p>

<ol>
  <li><strong>Lung segmentation</strong>: Process each image and retain only the lung area of the CT scan, for example, see <a href="https://pubs.rsna.org/doi/full/10.1148/rg.2015140232">here</a>.</li>
  <li>More sophisticated <strong>transfer learning</strong> design: For example, see <a href="https://ruder.io/multi-task/">multi-task learning</a> or <a href="https://en.wikipedia.org/wiki/Domain_adaptation#The_different_types_of_domain_adaptation">supervised domain adaptation</a>.</li>
  <li><strong>Ensemble</strong> model: This seems like a common belief, especially among Kaggle users, that building an <a href="https://scikit-learn.org/stable/modules/ensemble.html">ensemble model</a> would almost always give you extra few percent increase in accuracy.</li>
</ol>

<p><em>That‚Äôs it for our CNN COVID-19 CT scan classification! Thank you!</em> üëèüëèüëè</p>
:ET