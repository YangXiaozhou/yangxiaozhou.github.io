I"< <p>the Expectation Maximization algorithm (EM, for short)</p>

<p>If you are in the data science/ML “bubble”, you probably have came across EM at some time and wondered: What is EM and why do I need to know it?</p>

<p>My take on EM, what it is, how it works, how it’s related to other techniques, and how it might be improved.</p>

<ul id="markdown-toc">
  <li><a href="#motivating-examples-why-do-we-care" id="markdown-toc-motivating-examples-why-do-we-care">Motivating examples: Why do we care?</a></li>
  <li><a href="#what-is-em" id="markdown-toc-what-is-em">What is EM?</a>    <ul>
      <li><a href="#what-can-em-do" id="markdown-toc-what-can-em-do">What can EM do?</a></li>
      <li><a href="#where-is-em-in-the-big-picture" id="markdown-toc-where-is-em-in-the-big-picture">Where is EM in the big picture?</a></li>
    </ul>
  </li>
  <li><a href="#em-in-action-does-it-really-work" id="markdown-toc-em-in-action-does-it-really-work">EM in action: Does it really work?</a>    <ul>
      <li><a href="#solving-gmm-for-clustering" id="markdown-toc-solving-gmm-for-clustering">Solving GMM for clustering</a></li>
      <li><a href="#estimating-allele-frequencies" id="markdown-toc-estimating-allele-frequencies">Estimating allele frequencies</a></li>
    </ul>
  </li>
  <li><a href="#why-does-it-work" id="markdown-toc-why-does-it-work">Why does it work?</a>    <ul>
      <li><a href="#intuitive-explanation" id="markdown-toc-intuitive-explanation">Intuitive explanation</a></li>
      <li><a href="#mathematical-proof" id="markdown-toc-mathematical-proof">Mathematical proof</a></li>
    </ul>
  </li>
  <li><a href="#sois-it-perfect" id="markdown-toc-sois-it-perfect">So…is it perfect?</a>    <ul>
      <li><a href="#improving-e-step" id="markdown-toc-improving-e-step">Improving E-step</a></li>
      <li><a href="#improving-m-step" id="markdown-toc-improving-m-step">Improving M-step</a></li>
    </ul>
  </li>
  <li><a href="#uncertainty-going-beyond-point-estimate" id="markdown-toc-uncertainty-going-beyond-point-estimate">Uncertainty: Going beyond point estimate</a></li>
</ul>
<hr />

<h1 id="motivating-examples-why-do-we-care">Motivating examples: Why do we care?</h1>

<p>Maybe you already know why you want to use EM, or maybe you don’t. Either way, let me use two motivating examples to set the stage for EM. These are quite lengthy, I know, but they perfectly highlight the unique feature of the problems that EM is invented to solve. One thing in common for these problems: There is important information which is missing. Expectation maximization (EM) algorithm is designed to tackle these kind of problems.</p>

<h4 id="unsupervised-learning-solving-gaussian-mixture-model-for-clustering">Unsupervised learning: Solving Gaussian mixture model for clustering</h4>

<p>Suppose you have a data set with n number of data points. It could be a group of customers visiting your website (customer profiling) or an image with different objects in it (image segmentation). Clustering is the task of finding out k number of natural groups for your data when you don’t know (or don’t specify) the real grouping. In general, this is an unsupervised learning problem because no ground-truth labels are used.</p>

<p>Such clustering problem can be tackled by several types of algorithms, for example, combinatorial type such as k-means or hierarchical type such as Ward’s hierarchical clustering. However, if you believe that your data could be better modeled as a mixture of normal distributions, then you would go for Gaussian mixture model (<strong>GMM</strong>), another popular clustering approach. The underlying idea of GMM is this, you assume that behind your data, there’s a data generating mechanism. This mechanism first choses one of the k normal distributions (with a certain probability) and then delivers a sample from that normal distribution. Therefore, once you have estimated the parameters of each normal distribution, you could easily cluster each data point by selecting the one that gives the highest likelihood.</p>

<p>
  <img width="1024" alt="ClusterAnalysis Mouse" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/1024px-ClusterAnalysis_Mouse.svg.png" />
</p>
<p><i>An example of mixture of Gaussian data and clustering using k-means and EM algorithm. <a href="https://commons.wikimedia.org/wiki/File:ClusterAnalysis_Mouse.svg">Source</a></i></p>

<p>However, estimating the parameters is not a simple task since we do not know which distribution generated which points. EM is an algorithm that can help us solve exactly this problem. This is why EM is the underlying algorithm for solving GMMs in the scikit-learn’s <a href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture">implementation</a>.</p>

<h4 id="parameter-estimation-estimating-moth-allele-frequencies-to-observe-natural-selection">Parameter estimation: Estimating moth allele frequencies to observe natural selection</h4>

<p>Have you heard the phrase “industrial melanism” before? It’s a term coined by biologists in the 19th century to describe the phenomenon that animals change their skin color due to the heavy industrialization in the cities. In particular, they observed that previously rare dark peppered moth started to dominate the population in industrialized coal-fueled cities. Scientists at the time were surprised and fascinated by this observation. Further research suggests that the industrialized cities tend to have darker tree barks which disguise darker moths better than the light ones.</p>

<p align="center">
  <img src="/assets/intro-to-EM/dark_light_moth.png" alt="pepper_moths" style="zoom: 100%;" />
</p>
<p><i>Dark (top) and light (bottom) peppered moth. Image by Jerzy Strzelecki via Wikimedia Commons</i></p>

<p>As a result, dark moths survive the predation better and pass on their genes, giving rise to a predominantly dark peppered moth population.  To prove the natural selection theory, scientists first need to estimate the percentage of black-producing and light-producing genes/alleles present in the moth population. The gene responsible for the moth’s color has three types of alleles: C, I and T. Genotypes CC, CI, and CT produce dark peppered moth (<em>Carbonaria</em>); TT produces light peppered moth (<em>Typica</em>); II and IT produce moths with intermediate color (<em>Insularia</em>).</p>

<p>Here’s a hand-drawn graph that shows the <strong>observed</strong> and <strong>missing</strong> information.</p>

<p>We wish to know the percentages of C, I, and T in the population. However, we can only observe the number of <em>Carbonaria</em>, <em>Typica</em>, and <em>Insularia</em> moths by capturing them, but not the genotypes. That fact that we do not observe the genotypes and multiple genotypes produce the same subspecies make the calculation of the allele frequencies difficult. This is where EM algorithm comes in to the play. With EM, we can easily estimate the allele frequencies and provide concrete evidence of the microevolution that happens on a human time scale due to enviromental  pollution.</p>

<h1 id="what-is-em">What is EM?</h1>

<h2 id="what-can-em-do">What can EM do?</h2>

<h2 id="where-is-em-in-the-big-picture">Where is EM in the big picture?</h2>

<p>Connections of EM to other techniques</p>

<ol>
  <li>Clustering: k-means</li>
  <li>Hidden Markov model inference: Baum-Welch algorithm</li>
  <li>Bayesian inference: Gibbs sampling</li>
</ol>

<h1 id="em-in-action-does-it-really-work">EM in action: Does it really work?</h1>

<h2 id="solving-gmm-for-clustering">Solving GMM for clustering</h2>

<h2 id="estimating-allele-frequencies">Estimating allele frequencies</h2>

<p>Peppered moth <a href="https://askabiologist.asu.edu/peppered-moths-game/play.html">game</a></p>

<h1 id="why-does-it-work">Why does it work?</h1>

<h2 id="intuitive-explanation">Intuitive explanation</h2>

<h2 id="mathematical-proof">Mathematical proof</h2>

<h1 id="sois-it-perfect">So…is it perfect?</h1>

<h2 id="improving-e-step">Improving E-step</h2>
<h2 id="improving-m-step">Improving M-step</h2>

<h1 id="uncertainty-going-beyond-point-estimate">Uncertainty: Going beyond point estimate</h1>
:ET