I"8W<p>This is a collection of my notes on various topics of statistical learning. It is intended as a knowledge repository for myself and a work in progress that I will periodically update.</p>

<ul id="markdown-toc">
  <li><a href="#principal-component-analysis" id="markdown-toc-principal-component-analysis">Principal Component Analysis</a></li>
  <li><a href="#ridge-regularization" id="markdown-toc-ridge-regularization">Ridge regularization</a>    <ul>
      <li><a href="#relationship-between-ols-ridge-regression-and-pca" id="markdown-toc-relationship-between-ols-ridge-regression-and-pca">Relationship between OLS, Ridge regression, and PCA</a></li>
    </ul>
  </li>
  <li><a href="#clustering" id="markdown-toc-clustering">Clustering</a></li>
</ul>
<hr />

<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<ol>
  <li>One disadvantage of principal component analysis is that it is not scale-invariant.
    <ol>
      <li>Ratio between variables are not preserved under PCA transformation.</li>
      <li>Principle loadings are different when done through var-cov matrix and through correlation matrix (standardized)</li>
      <li>In other words, the eigenvectors are not scale-invariant.</li>
      <li>One way is to ensure common units, another way is to standardize and use correlation matrix.
        <ol>
          <li>However, the statistical properties of (sample) correlation matrix are way more complicated to study than that of covariance matrix.</li>
          <li>Technically, correlation matrix is still nonnegative-definite matrix.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Looking at the covariance between X and Y (transformed x) could help explain how much variance of each x is explained by any set of y.
    <ol>
      <li>And also how much variance of any set of Xs is explained by any set of Ys.</li>
      <li>This is visualized on a 2d plot (with first two PCs) and the Xs would fall inside an unit circle (correlations).</li>
      <li>Each y might explains different levels of variance for different Xs, therefore, throwing away PCA components might lead to unbalanced information loss for different Xs.</li>
    </ol>
  </li>
  <li>Scree plot/graph: Explained variance  as a function of number of PCs, the elbow plot</li>
  <li>Theorem: If the covariance matrix is positive-definite, then all eigenvalues of the sample covariance matrix are distinct w.p. 1.</li>
  <li>Principal component regression
    <ol>
      <li>Using principal components instead of the usual xs</li>
      <li>Advantage: The fitted values should be close to the ones by usual regression with a much smaller p (if PCs are chosen carefully).</li>
      <li>Disadvantage: Questions regarding the variable importance to the response become more difficult (y instead of x), at least not as straightforward.</li>
    </ol>
  </li>
  <li>Good for visualization, exploratory analysis, pattern finding (customize subsequent solutions), dimension reduction
    <ol>
      <li>e.g. PC1 and PC2 in the hand written digit problem indicates the thickness of the writing and the length of the lower tail, that indicates these properties are important to identify the object.</li>
    </ol>
  </li>
</ol>

<h1 id="ridge-regularization">Ridge regularization</h1>
<h2 id="relationship-between-ols-ridge-regression-and-pca">Relationship between OLS, Ridge regression, and PCA</h2>
<p>Simple yet elegant relationships between OLS estimates, ridge estimates and PCA can be found through the lens of spectral decomposition. We see these relationships through Exercise 8.8.1 of MA<sup id="fnref:MA"><a href="#fn:MA" class="footnote">1</a></sup>.</p>

<h3 id="set-up">Set-up</h3>

<p>Given the following regression model:</p>

<script type="math/tex; mode=display">\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\mu \mathbf{1}+\mathbf{u}, \quad \mathbf{u} \sim N_{\mathrm{n}}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right),</script>

<p>consider the columns of $\mathbf{X}$ have been standardized to have mean 0 and variance 1. Then the ridge estimate of $\boldsymbol{\beta}$ is</p>

<script type="math/tex; mode=display">\boldsymbol{\beta}^* = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^{\top} \mathbf{y}</script>

<p>where for given $\mathbf{X}$, $\lambda \ge 0$ is a small fixed ridge regularization parameter. Note that when $\lambda = 0$, it is just the OLS formulation. Also, consider the spectral decomposition of the var-cov matrix $\mathbf{X}^{\top} \mathbf{X} = \mathbf{G} \mathbf{L} \mathbf{G}^{\top}$. Let $\mathbf{W} = \mathbf{X}\mathbf{G}$ be the principal component transformation of the original data matrix.</p>

<h3 id="result-11">Result 1.1</h3>

<p>If $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ represents the parameter vector of principal components, then we can show that the ridge estimates $\boldsymbol{\alpha}^*$ can be obtained from OLS estimates $\hat{\boldsymbol{\alpha}}$ by simply scaling them with the ridge regularization parameter:</p>

<script type="math/tex; mode=display">\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p.</script>

<p>This result shows us two important insights:</p>
<ol>
  <li>For PC-transformed data, we can obtain the ridge estimates through a simple element-wise scaling of the OLS estimates.</li>
  <li>The shrinking effect of ridge regularization depends on both $\lambda$ and eigenvalues of the corresponding PC:
    <ul>
      <li>Larger $\lambda$ corresponds to heavier shrinking for every parameter.</li>
      <li>However, given the same $\lambda$, principal components corresponding to larger eigenvalues receive the least shrinking.</li>
    </ul>
  </li>
</ol>

<p>To demonstrate the two shrinking effects, I plotted the percentage shrunken ($1- l_{i}/(l_{i}+\lambda)$) as a function of the ordered principal components as well as the value of the ridge regularization parameter. The two shrinking effects are clearly visible from this figure.
<img src="/assets/learning-repo/pca_ridge_lambda_effect.png" alt="lambda_effect" /></p>

<details>
    <summary>Proof of Result 1.1:</summary>
Since $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ and $\mathbf{W} = \mathbf{X}\mathbf{G}$, then 

$$
\begin{align*}
\boldsymbol{\alpha}^* &amp;= (\mathbf{W}^{\top}\mathbf{W} + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= (\Lambda + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= \operatorname{diag}((l_{i} + \lambda)^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

Hence 

$$
\alpha^*_{i} = (l_i + \lambda)^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$ 

for $i=1, \ldots, p$, where $\mathbf{w}_{(i)}$ is the $i$th column of $\mathbf{W}$. Since 

$$
\begin{align*}
\hat{\boldsymbol{\alpha}} &amp;= (\mathbf{W}^{\top}\mathbf{W})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;= \operatorname{diag}(l_{i}^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

We have 

$$
\hat{\alpha}_{i} = l_i^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$

for $i=1, \ldots, p$. Therefore, by comparing the two estimate expressions, we have the result

$$
\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p. \blacksquare
$$

</details>

<h3 id="result-12">Result 1.2</h3>

<p>It follows from Result 1.1 that we can establish a direct link between the OLS estimate $\hat{\boldsymbol{\beta}}$ and the ridge estimate $\boldsymbol{\beta}^*$ through spectral decomposition of the var-cov matrix. Specifically, we have</p>

<script type="math/tex; mode=display">\boldsymbol{\beta}^{*}=\mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}, \quad \text { where } \mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right),</script>

<p>for $i=1, \ldots, p$.</p>

<details>
    <summary>Proof of Result 1.2:</summary>
Since $\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}$, for $i=1, \ldots, p$, and $\hat{\boldsymbol{\alpha}} = \mathbf{G}^{\top}\hat{\boldsymbol{\beta}}$, then

$$
\boldsymbol{\alpha}^{*} =  \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \,,
$$

by writing in matrix form. Also, since 

$$
\boldsymbol{\alpha}^* = \mathbf{G}^{\top}\boldsymbol{\beta}^*
$$

where $\boldsymbol{\beta}^*$ is the ridge estimate of $\boldsymbol{\beta}$, then we have

$$
\begin{align*}
\mathbf{G}^{\top}\boldsymbol{\beta}^* &amp;= \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
\boldsymbol{\beta}^* &amp;= \mathbf{G} \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
&amp;= \mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}
\end{align*}
$$

where $\mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right)$, for $i=1, \ldots, p$. $\blacksquare$
</details>

<h3 id="result-13">Result 1.3</h3>

<p>One measure of the quality of the estimators $\boldsymbol{\beta}^*$ is the trace mean square error (MSE):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\phi(\lambda) &= \operatorname{tr} E[(\boldsymbol{\beta}^* - \boldsymbol{\beta})(\boldsymbol{\beta}^* - \boldsymbol{\beta})^{\top}] \\
&= \sum_{i=1}^{p} E[(\beta_{i}^* - \beta_{i})^2] \,.
\end{align*} %]]></script>

<p>Now, from the previous two results, we can show that the trace MSE of the ridge estimates can be decomposed into two parts: <strong>variance</strong> and <strong>bias</strong>, and obtain an explicit formula for them. The availability of the exact formula for MSE allows things like regularization path to be computed easily.</p>

<p>Specifically, we have</p>

<script type="math/tex; mode=display">\phi(\lambda) = \gamma_1(\lambda) + \gamma_2(\lambda)</script>

<p>where the first component is the sum of variances:</p>

<script type="math/tex; mode=display">\gamma_1(\lambda) = \sum_{i=1}^{p} V(\beta_{i}^*) = \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,,</script>

<p>and the second component is the sum of squared biases:</p>

<script type="math/tex; mode=display">\gamma_2(\lambda) = \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 = \lambda^2 \sum_{i=1}^{p} \frac{\alpha_i^2}{(l_i + \lambda)^2} \,.</script>

<details>
    <summary>Proof of Result 1.3:</summary>
First let's look at the sum of variances. We start by writing out the expression for the variance of the ridge estimates:

$$
\begin{align*}
V(\boldsymbol{\beta}^*) &amp;= \mathbf{GDG}^{\top} V(\hat{\boldsymbol{\beta}}) \mathbf{GDG}^{\top} \\
&amp;= \mathbf{GDG}^{\top} \sigma^2 \mathbf{X^{\top}X}^{-1} \mathbf{GDG}^{\top} \\
&amp;= \sigma^2 \mathbf{GDG}^{\top} (\mathbf{GLG})^{-1} \mathbf{GDG}^{\top} \\
&amp;= \sigma^2 \mathbf{GD} L^{-1} \mathbf{DG}^{\top} \\
&amp;= \sigma^2 \mathbf{G} \operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \,, \quad i=1, \ldots, p.
\end{align*}
$$ 

Hence, by extracting out the variances from the diagonal, we obtain the first expression:

$$
\begin{align*}
\gamma_1(\lambda) &amp;= \sum_{i=1}^{p} V(\beta_{i}^*) \\
&amp;= \operatorname{tr} V(\boldsymbol{\beta}^*) \\
&amp;= \sigma^2 \operatorname{tr}(\operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \mathbf{G}) \\
&amp;= \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,.
\end{align*}
$$

Now let's look at the bias term. We write it in matrix form to see that:

$$
\begin{align*}
\gamma_2(\lambda) &amp;= \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 \\
&amp;= (E[\boldsymbol{\beta}^*] - \boldsymbol{\beta})^{\top}(E[\boldsymbol{\beta}^*] - \boldsymbol{\beta}) \\
&amp;= (\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta})^{\top}(\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta}) \\
&amp;= \mathbf{B}^{\top}\mathbf{GD}^2\mathbf{G}^{\top}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\top}\mathbf{GDG^{\top}}\boldsymbol{\beta} + \boldsymbol{\beta}^{\top}\boldsymbol{\beta} \\
&amp;= \boldsymbol{\alpha}^{\top}\mathbf{D}^2\boldsymbol{\alpha} - 2\boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha} + \boldsymbol{\alpha}^{\top}\mathbf{G^{\top}G}\boldsymbol{\alpha} \\
&amp;= \boldsymbol{\alpha}^{\top} (\mathbf{D}^2 - 2\mathbf{D} + 1) \boldsymbol{\alpha} \\
&amp;= \boldsymbol{\alpha}^{\top} \operatorname{diag}(\lambda^2/(l_i + \lambda)^2) \boldsymbol{\alpha} \\
&amp;= \lambda^2 \sum_{i=1}^{p}(\alpha_{i}^2/(l_i + \lambda)^2) \,.
\end{align*}
$$

Combining the two gamma terms completes the proof. $\blacksquare$
</details>

<h3 id="result-14">Result 1.4</h3>

<p>This is a quick but revealing result that follows from Result 1.3. Taking a partial derivative of the trace MSE function with respect to $\lambda$ and take $\lambda = 0$, we get</p>

<script type="math/tex; mode=display">\frac{\partial{\phi(\lambda)}}{\partial{\lambda}} = -2 \sigma^2 \sum_{i=1}^{p} 1/l_i^2 \,.</script>

<p>Notice that the gradient of the trace MSE function is negative when $\lambda$ is 0. This tells us two things:</p>
<ol>
  <li>We can reduce the trace MSE by taking a non-zero $\lambda$ value. In particular, we are trading a bit of bias for a reduction in variance as the variance function ($\gamma_1$) is monotonically decreasing in $\lambda$. However, we need to find the right balance between variance and bias so that the overall trace MSE is minimized.</li>
  <li>The reduction in trace MSE by ridge regularization is higher when some $l_i$s are small. That is, when there is considerable collinearity among the predictors, ridge regularization can achieve much smaller trace MSE than OLS.</li>
</ol>

<h3 id="visualization">Visualization</h3>

<p>Using the <code class="highlighter-rouge">sklearn.metrics.make_regression</code> function, I generated a noisy regression data set with 50 samples and 10 features. However, most of the variances (in PCA sense) are explained by 5 of those 10 features, i.e. last 5 eigenvalues are relatively small. Here are the regularization path and the coefficient error plot.</p>

<p><img src="/assets/learning-repo/ridge_lambda_mse.png" alt="ridge_error" /></p>

<p>From the figure, we can clearly see that</p>
<ul>
  <li>Increasing $\lambda$ shrinks every coefficient towards 0. </li>
  <li>OLS procedure (left-hand side of both figures) produces erroneous (and with a large variance) estimate. the estimator MSE is significantly larger than that of the ridge regression. </li>
  <li>An optimal $\lambda$ is found at around 1 where the MSE of ridge estimated coefficients is minimized. </li>
  <li>On the other hand, $\lambda$ values larger and smaller than 1 are suboptimal as they lead to over-regularization and under-regularization in this case.</li>
</ul>

<details>
<summary>Click here for the script to generate the above plot, credit to <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py">scikit-learn</a>.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                          <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                          <span class="n">effective_rank</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Train the model with different regularisation strengths
</span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">:</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>

<span class="c1"># Display results
</span><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s">'seaborn-talk'</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'$</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weights'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Ridge coefficients as a function of $</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">errors</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'$</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'error'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Coefficient error as a function of $</span><span class="err">\</span><span class="s">lambda$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>
</details>

<h1 id="clustering">Clustering</h1>

<h1 class="no_toc" id="references">References</h1>

<div class="footnotes">
  <ol>
    <li id="fn:MA">
      <p>Mardia, K. V., Kent, J. T., &amp; Bibby, J. M. <em>Multivariate Analysis</em>. 1979. Probability and mathematical statistics. Academic Press Inc. <a href="#fnref:MA" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET