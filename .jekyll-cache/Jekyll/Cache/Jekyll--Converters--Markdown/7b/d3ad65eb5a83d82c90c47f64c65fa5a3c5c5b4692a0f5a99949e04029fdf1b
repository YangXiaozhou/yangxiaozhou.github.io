I"¥<p>If you are in the data science/ML ‚Äúbubble‚Äù, you probably have came across EM at some time and wondered: What is EM and why do I need to know it?</p>

<p>My take on EM, what it is, how it works, how it‚Äôs related to other techniques, and how it might be improved.</p>

<ul id="markdown-toc">
  <li><a href="#motivating-examples-why-do-we-care" id="markdown-toc-motivating-examples-why-do-we-care">Motivating examples: Why do we care?</a></li>
  <li><a href="#what-is-em" id="markdown-toc-what-is-em">What is EM?</a>    <ul>
      <li><a href="#what-can-em-do" id="markdown-toc-what-can-em-do">What can EM do?</a></li>
      <li><a href="#where-is-em-in-the-big-picture" id="markdown-toc-where-is-em-in-the-big-picture">Where is EM in the big picture?</a></li>
    </ul>
  </li>
  <li><a href="#em-in-action-does-it-really-work" id="markdown-toc-em-in-action-does-it-really-work">EM in action: Does it really work?</a>    <ul>
      <li><a href="#solving-gmm-for-clustering" id="markdown-toc-solving-gmm-for-clustering">Solving GMM for clustering</a></li>
      <li><a href="#estimating-allele-frequencies" id="markdown-toc-estimating-allele-frequencies">Estimating allele frequencies</a></li>
    </ul>
  </li>
  <li><a href="#why-does-it-work" id="markdown-toc-why-does-it-work">Why does it work?</a>    <ul>
      <li><a href="#intuitive-explanation" id="markdown-toc-intuitive-explanation">Intuitive explanation</a></li>
      <li><a href="#mathematical-proof" id="markdown-toc-mathematical-proof">Mathematical proof</a></li>
    </ul>
  </li>
  <li><a href="#sois-it-perfect" id="markdown-toc-sois-it-perfect">So‚Ä¶is it perfect?</a>    <ul>
      <li><a href="#improving-e-step" id="markdown-toc-improving-e-step">Improving E-step</a></li>
      <li><a href="#improving-m-step" id="markdown-toc-improving-m-step">Improving M-step</a></li>
    </ul>
  </li>
  <li><a href="#uncertainty-going-beyond-point-estimate" id="markdown-toc-uncertainty-going-beyond-point-estimate">Uncertainty: Going beyond point estimate</a></li>
</ul>
<hr />

<h1 id="motivating-examples-why-do-we-care">Motivating examples: Why do we care?</h1>

<h4 id="unsupervised-learning-solving-gaussian-mixture-model-for-clustering">Unsupervised learning: Solving Gaussian mixture model for clustering</h4>

<p>Suppose you have a data set with n number of data points. It could be a group of customers visiting your website (customer profiling) or an image with different objects in it (image segmentation). Clustering is the task of finding out k number of natural groups for your data when you don‚Äôt know (or don‚Äôt specify) the real grouping. In general, this is an unsupervised learning problem because no ground-truth labels are used.</p>

<p>Such clustering problem can be tackled by several types of algorithms, for example, combinatorial type such as k-means or hierarchical type such as Ward‚Äôs hierarchical clustering. However, if you believe that your data could be better modeled as a mixture of normal distributions, then you would go for Gaussian mixture model (<strong>GMM</strong>), another popular clustering approach. The underlying idea of GMM is this, you assume that behind your data, there‚Äôs a data generating mechanism. This mechanism first choses one of the k normal distributions (with a certain probability) and then delivers a sample from that normal distribution. Therefore, once you have estimated the parameters of each normal distribution, you could easily cluster each data point by selecting the one that gives the highest likelihood.</p>

<p>However, estimating the parameters is not a simple task since we do not know which distribution generated which points. EM is an algorithm that can help us solve exactly this problem. This is why EM is the underlying algorithm for solving GMMs in the scikit-learn‚Äôs <a href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture">implementation</a>.</p>

<h4 id="parameter-estimation-estimating-peppered-moth-allele-frequencies-to-observe-natural-selection">Parameter estimation: Estimating peppered moth allele frequencies to observe natural selection</h4>

<p>Have you heard the term ‚Äúindustrial melanism‚Äù? It‚Äôs a term coined by biologists in the 19th century to describe the phenomenon that animals change their skin color due to heavy industrialization in the cities. In particular, they observed that previously rare dark peppered moth started to dominant their population in</p>

<p>Peppered moth <a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution">evolution</a></p>

<p align="center">
  <img src="/assets/intro-to-EM/dark_light_moth.png" alt="pepper_moths" style="zoom: 75%;" />
  <i>Dark (top) and light (bottom) peppered moth. Image by Jerzy Strzelecki via Wikimedia Commons</i>
</p>

<p>One thing in common for these problems: There is important information which is missing. Expectation maximization (EM) algorithm is designed to tackle these kind of problems.</p>

<h1 id="what-is-em">What is EM?</h1>

<h2 id="what-can-em-do">What can EM do?</h2>

<h2 id="where-is-em-in-the-big-picture">Where is EM in the big picture?</h2>

<p>Connections of EM to other techniques</p>

<ol>
  <li>Clustering: k-means</li>
  <li>Hidden Markov model inference: Baum-Welch algorithm</li>
  <li>Bayesian inference: Gibbs sampling</li>
</ol>

<h1 id="em-in-action-does-it-really-work">EM in action: Does it really work?</h1>

<h2 id="solving-gmm-for-clustering">Solving GMM for clustering</h2>

<h2 id="estimating-allele-frequencies">Estimating allele frequencies</h2>

<p>Peppered moth <a href="https://askabiologist.asu.edu/peppered-moths-game/play.html">game</a></p>

<h1 id="why-does-it-work">Why does it work?</h1>

<h2 id="intuitive-explanation">Intuitive explanation</h2>

<h2 id="mathematical-proof">Mathematical proof</h2>

<h1 id="sois-it-perfect">So‚Ä¶is it perfect?</h1>

<h2 id="improving-e-step">Improving E-step</h2>
<h2 id="improving-m-step">Improving M-step</h2>

<h1 id="uncertainty-going-beyond-point-estimate">Uncertainty: Going beyond point estimate</h1>
:ET