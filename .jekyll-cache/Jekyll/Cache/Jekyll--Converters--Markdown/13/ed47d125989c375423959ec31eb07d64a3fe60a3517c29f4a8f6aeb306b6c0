I"9<p>the Expectation-maximization algorithm (EM, for short)</p>

<p><strong>Write about the problem that the readers care</strong></p>

<p><strong>Create instability and offer solution</strong></p>

<p>If you are in the data science/ML ‚Äúbubble‚Äù, you probably have came across EM at some time and wondered: What is EM and why do I need to know it?</p>

<p>My take on EM, what it is, how it works, how it‚Äôs related to other techniques, and how it might be improved.</p>

<ol>
  <li>Motivating examples</li>
  <li>What is EM?</li>
  <li>EM in action: Does it really work?</li>
  <li>Intuition and theory: Why does it work?</li>
  <li>So‚Ä¶is it perfect?</li>
  <li>Uncertainty: Going beyond point estimateüò±</li>
</ol>

<h2 id="motivating-examples-why-do-we-care">Motivating examples: Why do we care?</h2>

<p>Maybe you already know why you want to use EM, or maybe you don‚Äôt. Either way, let me use two motivating examples to set the stage for EM. These are quite lengthy, I know, but they perfectly highlight the common feature of the problems that EM is best at solving: the presence of <strong>missing information</strong>.</p>

<h3 id="unsupervised-learning-solving-gaussian-mixture-model-for-clustering">Unsupervised learning: Solving Gaussian mixture model for clustering</h3>

<p>Suppose you have a data set with n number of data points. It could be a group of customers visiting your website (customer profiling) or an image with different objects in it (image segmentation). Clustering is the task of finding out k number of natural groups for your data when you don‚Äôt know (or don‚Äôt specify) the real grouping. This is an unsupervised learning problem because no ground-truth labels are used.</p>

<p>Such clustering problem can be tackled by several types of algorithms, e.g., combinatorial type such as k-means or hierarchical type such as Ward‚Äôs hierarchical clustering. However, if you believe that your data could be better modeled as a mixture of normal distributions, then you would go for Gaussian mixture model (<strong>GMM</strong>), another popular clustering approach.</p>

<p>The underlying idea of GMM is this, you assume that behind your data, there‚Äôs a data generating mechanism. This mechanism first choses one of the k normal distributions (with a certain probability) and then delivers a sample from that distribution. Therefore, once you have estimated the parameters of each normal distribution, you could easily cluster each data point by selecting the one that gives the highest likelihood.</p>

<p>
  <img width="1024" alt="ClusterAnalysis Mouse" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/1024px-ClusterAnalysis_Mouse.svg.png" />
</p>
<p><i>An example of mixture of Gaussian data and clustering using k-means and GMM (solved by EM).¬†<a href="https://commons.wikimedia.org/wiki/File:ClusterAnalysis_Mouse.svg">Source</a></i></p>

<p>However, estimating the parameters is not a simple task since we do not know which distribution generated which points (<strong>missing information</strong>). EM is an algorithm that can help us solve exactly this problem. This is why EM is the underlying algorithm for solving GMMs in scikit-learn‚Äôs <a href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture">implementation</a>.</p>

<h3 id="population-genetics-estimating-moth-allele-frequencies-to-observe-natural-selection">Population genetics: Estimating moth allele frequencies to observe natural selection</h3>

<p>Have you heard the phrase ‚Äúindustrial melanism‚Äù before? It‚Äôs a term coined by biologists in the 19th century to describe the phenomenon that animals change their skin color due to the heavy industrialization in the cities. In particular, they observed that previously rare dark peppered moth started to dominate the population in industrialized coal-fueled cities. Scientists at the time were surprised and fascinated by this observation. Subsequent research suggests that the industrialized cities tend to have darker tree barks which disguise darker moths better than the light ones.</p>

<p align="center">
  <img src="/assets/intro-to-EM/dark_light_moth.png" alt="pepper_moths" style="zoom: 75%;" />
</p>
<p><i>Dark (top) and light (bottom) peppered moth. Image by Jerzy Strzelecki via Wikimedia Commons</i></p>

<p>As a result, dark moths survive the predation better and pass on their genes, giving rise to a predominantly dark peppered moth population.  To prove their natural selection theory, scientists first need to estimate the percentage of black-producing and light-producing genes/alleles present in the moth population. The gene responsible for the moth‚Äôs color has three types of alleles: C, I and T. Genotypes <strong>C</strong>C, <strong>C</strong>I, and <strong>C</strong>T produce dark peppered moth (<em>Carbonaria</em>); <strong>T</strong>T produces light peppered moth (<em>Typica</em>); <strong>I</strong>I and <strong>I</strong>T produce moths with intermediate color (<em>Insularia</em>).</p>

<p>Here‚Äôs a hand-drawn graph that shows the <strong>observed</strong> and <strong>missing</strong> information.</p>

<p align="center">
  <img src="/assets/intro-to-EM/moth_relationship.jpg" alt="moth_relationship" style="zoom: 100%;" />
</p>
<p><i>Relationship between peppered moth alleles, genotypes, and phenotypes. We observed phenotypes, but wish to estimate percentges of alleles in the population. Image by author</i></p>

<p>We wish to know the percentages of C, I, and T in the population. However, we can only observe the number of <em>Carbonaria</em>, <em>Typica</em>, and <em>Insularia</em> moths by capturing them, but not the genotypes (<strong>missing information</strong>). The fact that we do not observe the genotypes and multiple genotypes produce the same subspecies make the calculation of the allele frequencies difficult. This is where EM comes in to play. With EM, we can easily estimate the allele frequencies and provide concrete evidence for the microevolution that happens on a human time scale due to environmental  pollution.</p>

<p>How does EM tackle the GMM problem and the peppered moth problem in the presence of missing information? We will illustrate these in the later section. But first, let‚Äôs see what EM is really about.</p>

<h2 id="what-is-em">What is EM?</h2>

<p>At this point, you must be thinking (I hope): All these examples are wonderful, but what is really EM? Let‚Äôs dive into it.</p>

<p>EM algorithm is an iterative optimization method that finds the maximum likelihood estimate (MLE) of parameters in problems where hidden/missing/latent variables are present. It was first introduced in its full generality by Dempster, Laird, and Rubin in their famous paper<sup id="fnref:Dempster"><a href="#fn:Dempster" class="footnote">1</a></sup>. Since then, it has been widely used for its easy implementation, numerical stability, and strong empirical performance.</p>

<p>Let‚Äôs set up the EM for a general problem and introduce some notations. Suppose that $Y$ are our observed variables, $X$ are hidden variables, and we say that $(X, Y)$ is the complete data. We also denote any unknown parameter of interest as $\theta \in \Theta$. The objective of most parameter estimation problems is to find the most probable $\theta$ given our model and data, i.e.,</p>

<script type="math/tex; mode=display">\begin{equation}
\theta = \arg\max_{\theta \in \Theta} p_\theta(\mathbf{y}) \,,
\end{equation}</script>

<p>where  $p_\theta(\mathbf{y})$ is the incomplete-data likelihood. Using the law of total probability, we can also express the incomplete-data likelihood as</p>

<script type="math/tex; mode=display">p_\theta(\mathbf{y}) = \int p_\theta(\mathbf{x}, \mathbf{y}) d\mathbf{x} \,,</script>

<p>where $p_\theta(\mathbf{x}, \mathbf{y})$ is known as the complete-data likelihood.</p>

<p>What‚Äôs with all these complete- and incomplete-data likelihoods? In many problems, the maximization of the incomplete-data likelihood $p_\theta(\mathbf{y})$ is difficult because of the missing information. On the other hand, it‚Äôs often easier to work with complete-data likelihood. EM algorithm is designed to take advantage of this obsercarion. It iterates between an expectation step (E-step) and a maximization step (M-step) to find the MLEs. Assuming $\theta^{(n)}$ is the estimate $n$th iteration, the algorithm proceeds as follows:</p>

<ul>
  <li>
    <p>E-step: define 
$Q(\theta | \theta^{(n)})$ as the conditional expectation of the complete-data log-likelihood
<script type="math/tex">\begin{align}
Q(\theta | \theta^{(n)}) = \mathbb{E}_{\mathbf{X}|\mathbf{y}, \theta^{(n)}}[\ln p_\theta(\mathbf{x}, \mathbf{y})] \,.
\end{align}</script></p>
  </li>
  <li>
    <p>M-step: find $\theta$ that maximizes the above expectation</p>

    <script type="math/tex; mode=display">\begin{equation}
\theta^{(n+1)} = \arg\max_{\theta \in \Theta} Q(\theta | \theta^{(n)}) \,.
\end{equation}</script>
  </li>
</ul>

<p>The algorithm iterates between the two steps until the estimation has reached a stopping criterion, i.e., until convergence.</p>

<h4 id="where-is-em-in-the-big-picture">Where is EM in the big picture?</h4>

<p>Connections of EM to other techniques</p>

<ol>
  <li>Clustering: k-means</li>
  <li>Hidden Markov model inference: Baum-Welch algorithm</li>
  <li>Bayesian inference: Gibbs sampling</li>
</ol>

<h2 id="em-in-action-does-it-really-work">EM in action: Does it really work?</h2>

<h4 id="solving-gmm-for-clustering">Solving GMM for clustering</h4>

<h4 id="estimating-allele-frequencies">Estimating allele frequencies</h4>

<p>Peppered moth <a href="https://askabiologist.asu.edu/peppered-moths-game/play.html">game</a></p>

<h2 id="why-does-it-work">Why does it work?</h2>

<h4 id="intuitive-explanation">Intuitive explanation</h4>

<details>
    <summary><b>Mathematical proof:</b></summary>
  Here we show why the above iterative scheme can find the maximum likelihood estimate of the parameter. Let $\ell(\theta) = \ln p_\theta(\mathbf{y})$, thus we have


$$
\ell(\theta) - \ell(\theta^{(n)}) = \ln p_\theta(\mathbf{y}) - \ln p_{\theta^{(n)}}(\mathbf{y}) \,.
$$


We wish to compute an updated $\theta$ such that the above relationship holds above zero. Using $p_\theta(\mathbf{y}) = \int p_\theta(\mathbf{x}, \mathbf{y}) \, \mathrm{d}\mathbf{x}$, we have


$$
\begin{align*} \ell(\theta) - \ell(\theta^{(n)}) &amp;= \ln \int p_\theta(\mathbf{x}, \mathbf{y}) \, \mathrm{d}\mathbf{x} - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
&amp;= \ln \int p_\theta(\mathbf{x}, \mathbf{y}) \frac{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})} \, \mathrm{d}\mathbf{x} 

- \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;= \ln \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}\right] - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;\ge \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}\right] - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;= \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y} ) p_{\theta^{(n)}}(\mathbf{y})}\right] \\
  &amp;:= \Delta(\theta | \theta^{(n)}) \,.
  \end{align*}
$$


The inequality step follows by Jensen's inequality and the fact that $\ln(\cdot)$ is concave on $[0, \infty]$. The second last step follows since $p_{\theta^{(n)}}(\mathbf{y})$ does not depend on $\mathbf{X}$. Therefore, we have 
$$
\ell(\theta) \ge \ell(\theta^{(n)}) + \Delta(\theta|\theta^{(n)}) \,.
$$
Define 


$$
\ell(\theta | \theta^{(n)}) := \ell(\theta^{(n)}) + \Delta(\theta|\theta^{(n)}) \,,
$$
 

then 
$\ell(\theta) \ge \ell(\theta|\theta^{(n)})$. 
That is, 
$\ell(\theta|\theta^{(n)})$ 
is upper-bounded by $\ell(\theta)$ for all $\theta \in \Theta$. The equality holds when $\theta = \theta^{(n)}$ since


$$
\begin{align*}
\ell(\theta^{(n)}|\theta^{(n)}) &amp;= \ell(\theta^{(n)}) + \Delta(\theta^{(n)}|\theta^{(n)}) \\
&amp;= \ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y} ) p_{\theta^{(n)}}(\mathbf{y})}\right] \\
&amp;= \ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}\right] \\
&amp;= \ell(\theta^{(n)}) \,.
\end{align*}
$$


Therefore, when computing an updated $\theta$, any increase in 
$\ell(\theta|\theta^{(n)})$ leads to an increase in $\ell(\theta)$ by at least 
$\Delta(\theta|\theta^{(n)})$. The observation is that, by selecting the $\theta$ that maximizes 
$\Delta(\theta|\theta^{(n)})$, we can achieve the largest increase in $\ell(\theta)$. Formally, we have 


$$
\begin{align*}
\theta^{(n+1)} &amp;= \arg\max_{\theta\in\Theta} \ell(\theta | \theta^{(n)}) \\
&amp; = \arg\max_{\theta\in\Theta} 
\left\lbrace
\ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}} 
\left[
\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y}) p_{\theta^{(n)}}(\mathbf{y})}
\right]
\right\rbrace\\
&amp; = \underbrace{\arg\max_{\theta\in\Theta}}_{\text{Maximization}} \underbrace{\mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}}_{\text{Expectation}}[\ln p_\theta(\mathbf{x}, \mathbf{y})] \\
&amp; = \arg\max_{\theta\in\Theta} Q(\theta | \theta^{(n)}) \,,
\end{align*}
$$


where the second last step follows by dropping terms constant with respect to $\theta$. Thus, the E-step and M-step are made apparent in the formulation. Also, by maximizing 
  $\ell(\theta | \theta^{(n)})$ instead of $\ell(\theta)$, we have made use of the information of hidden variables $\mathbf{X}$ in the complete-data likelihood. 

</details>

<h2 id="sois-it-perfect">So‚Ä¶is it perfect?</h2>

<h4 id="improving-e-step">Improving E-step</h4>
<h4 id="improving-m-step">Improving M-step</h4>

<h2 id="uncertainty-going-beyond-point-estimate-">Uncertainty: Going beyond point estimate üò±</h2>

<hr />
<h2 id="references">References</h2>
<div class="footnotes">
  <ol>
    <li id="fn:Dempster">
      <p>Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <em>39</em>(1), 1-22.¬†<a href="#fnref:Dempster" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET