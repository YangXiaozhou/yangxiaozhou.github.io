I"@<p><em>Tagline</em></p>

<!-- ![mda](/assets/2019-10-02/mda.png) -->

<ul id="markdown-toc">
  <li><a href="#key-takeaways" id="markdown-toc-key-takeaways">Key takeaways</a></li>
  <li><a href="#introduction-to-convolutional-neural-network" id="markdown-toc-introduction-to-convolutional-neural-network">Introduction to convolutional neural network</a>    <ul>
      <li><a href="#deep-neural-network" id="markdown-toc-deep-neural-network">Deep neural network</a></li>
      <li><a href="#convolutional-neural-network" id="markdown-toc-convolutional-neural-network">Convolutional neural network</a></li>
    </ul>
  </li>
  <li><a href="#covid-19-classifier-using-cnn" id="markdown-toc-covid-19-classifier-using-cnn">COVID-19 classifier using CNN</a>    <ul>
      <li><a href="#informs-data-challenge" id="markdown-toc-informs-data-challenge">INFORMS data challenge</a></li>
      <li><a href="#classifier-via-transfer-learning" id="markdown-toc-classifier-via-transfer-learning">Classifier via transfer learning</a></li>
    </ul>
  </li>
  <li><a href="#where-do-we-go-from-here" id="markdown-toc-where-do-we-go-from-here">Where do we go from here?</a></li>
</ul>
<hr />

<h1 id="key-takeaways">Key takeaways</h1>
<p>1.
2.
3.</p>

<h1 id="introduction-to-convolutional-neural-network">Introduction to convolutional neural network</h1>

<p>To get everyone on the same page, I am going to introduce very briefly what are <strong>deep neural networks</strong> (DNNs) and <strong>convolutional neural networks</strong> (CNNs). At the end of this section, you can gain an intuitive understanding of the motivation behind CNN and the key components that define a CNN.</p>

<p>If you are already familiar with DNNs, this section should be a good refresher. Of course, you’re welcome to skip to <a href="#Convolutional neural network">Convolutional neural network</a> or <a href="#COVID-19 classifier using CNN">COVID-19 classifier using CNN</a>.</p>

<h2 id="deep-neural-network">Deep neural network</h2>

<p>Before diving into neural networks, let’s first see the machine learning big picture:</p>
<blockquote>
  <p><strong>Machine learning</strong> (ML) is the study of computer algorithms that improve automatically through experience. – Wikipedia</p>
</blockquote>

<p>Looking at the problems that ML tries to solve, ML is often sliced into</p>
<ul>
  <li>Supervised learning: predicting a label, e.g. classification, or a continuous variable;</li>
  <li>Unsupervised learning: pattern recognition for unlabeled data, e.g., clustering;</li>
  <li>Reinforcement learning: algorithms learn the best way to “behave”, e.g. AlphaGo, self-driving cars.</li>
</ul>

<p>Deep learning, among others, is a powerful form of machine learning that has garnered much attention for its successes in computer vision (e.g. image recognition), natural language processing, and beyond. Neural network is originally inspired by information processing and communication nodes in biological systems. By design, input data is passed through layers of the network, which contain a number of nodes, analogous to “neurons”. The system then outputs a certain representation of the information. DNN is probably the most well-known network for deep learning and it can be trained to learn the features of the data very well.</p>

<p><img src="/assets/cnn-covid-19/deep-nn.jpg" alt="Deep neural network" />Image <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347705/">credit</a>.</p>

<p>Roughly speaking, there are two important operations that make a neural network.</p>
<ol>
  <li><strong>Forward propagation</strong></li>
  <li><strong>Backpropagation</strong></li>
</ol>

<h3 id="forward-propagation">Forward propagation</h3>
<p>This is the <strong>prediction</strong> step. The network reads the input data, computes its values across the network and gives a final output value.</p>

<p>But how does the network computes an output value? Let’s see what happens in a single layer network when it does one prediction. It takes in inputs as a vector of numbers. Each node in the layer has its own weight. When the input value is passed through the layer, the network computes its weighted sum. This is usually followed by a (usually nonlinear) activation function, e.g. step function, where the weighted sum is “activated”.</p>

<p><img src="/assets/cnn-covid-19/perceptron.jpg" alt="perceptron" /> Image <a href="https://deepai.org/machine-learning-glossary-and-terms/perceptron">credit</a>.</p>

<p>If you know a bit about algebra, this is what the operation is doing:</p>

<script type="math/tex; mode=display">y = f(\mathbf{w}\cdot \mathbf{x} + b)</script>

<p>where $\mathbf{w}\cdot \mathbf{x} + b$ is the weighted sum, $f(\cdot)$ is the activation function, and $y$ is the output. Now, in a deeper neural network, the procedure is essentially the same, i.e., the <strong>input –&gt; weighted sum –&gt; activation</strong> process is repeated for each layer.</p>

<p><img src="/assets/cnn-covid-19/mlp.png" alt="mlp" /> Image <a href="https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2020/lectures/03/MLP_and_backprop.html">credit</a>.</p>

<h3 id="backpropagation">Backpropagation</h3>
<p>This is the <strong>training</strong> step. By comparing the network’s predictions/outputs and the ground truth values, i.e., compute loss, the network adjusts its parameters so that the performance is improved.</p>

<p>How does the network adjust the parameters (weights and biases) through training? This is done through an operation called <strong>backpropagation</strong>, or backprop. The network takes the loss and recursively calculates the slope of the loss function with respect to each network parameter. Calculating these slopes requires the usage of chain rule from calculus, you can read more about it <a href="https://sebastianraschka.com/faq/docs/backprop-arbitrary.html">here</a>.</p>

<p>An optimization algorithm is then used to update network parameters using the gradient information until the performance cannot be improved anymore. One commonly used optimizer is stochastic gradient descent.</p>

<p>One analogy often used to explain gradient-based optimization is hiking. Training the network so that its loss is minimized is like trying to get down to the lowest point on the ground from a mountain. Backprop operation finding the loss function gradients is like finding the path on your way down. Optimization algorithm is the step where you actually take the path and eventually reach the lowest point. <img src="/assets/cnn-covid-19/gradient-descent.png" alt="gradient-descent" /> Image <a href="https://www.datasciencecentral.com/profiles/blogs/alternatives-to-the-gradient-descent-algorithm">credit</a>.</p>

<p>I am glossing over many details, but I hope you now know that DNN</p>
<ul>
  <li>is a powerful <strong>machine learning</strong> technique;</li>
  <li>can be used to tackle <strong>supervised</strong>, <strong>unsupervised</strong> and <strong>reinforcement learning</strong> problems;</li>
  <li>consists of forward propagation (<strong>input to output</strong>) and backpropagation (<strong>error to parameter update</strong>).</li>
</ul>

<p>We are ready to talk about CNN!</p>

<h2 id="convolutional-neural-network">Convolutional neural network</h2>

<p>Ordinary neural networks that we’ve talked about above expect input data to be a <strong>vector of numbers</strong>, i.e., $\mathbf{x} = [x_1, x_2, x_3, \dots]$. What if we want to train an <strong>image classifier</strong>, i.e. use image as the input? Let’s talk about some digital image basics.</p>

<ul>
  <li>An image is a <strong>collection of pixels</strong>. For example, a 32-by-32 image has $32 \times 32 = 1024$ pixels.</li>
  <li>Each pixel is an <strong>intensity represented by a number</strong> in the range $[0, 255]$, $0$ is black and $255$ is white.</li>
  <li>Color images have three dimensions: <strong>[width, height, depth]</strong> where depth is usually 3.</li>
  <li>Why is depth 3? That’s because it encodes the intensity of [<strong>R</strong>ed, <strong>G</strong>reen, <strong>B</strong>lue], i.e. RGB values.</li>
</ul>

<p>Therefore, to a computer program, this black and white Lincoln image is just a matrix of integers. 
<img src="/assets/cnn-covid-19/image_pixel.png" alt="image_pixel" /> Image <a href="https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html">credit</a>.</p>

<p>Since a digital image can be represented as a 2D grid of pixel values, we could stretch out/flatten the grid, make it into a vector of numbers and feed it into a neural network. That solves the problem…right?</p>

<p>However, there are two major limitations to this approach.</p>
<ol>
  <li><strong>It does not scale well to bigger images.</strong>
    <ul>
      <li>While it is still manageable for an input with $32\times32 = 1024$ dimensions, most real-life images are bigger than this.</li>
      <li>For example, a color image of size 320x320x3 would translate to an input with dimension <strong>307200</strong>!</li>
    </ul>
  </li>
  <li><strong>It does not consider the properties of an image.</strong>
    <ul>
      <li><em>Locality</em>: Nearby pixels are usually strongly correlated (e.g., see the outline of Lincoln’s face). Stretching it out breaks the pattern.</li>
      <li><em>Translation invariance</em>: Meaningful features could occur anywhere on an image, e.g., see the flying bird.</li>
    </ul>
  </li>
</ol>

<p><img src="/assets/cnn-covid-19/flying-bird.png" alt="bird" /> Image <a href="https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L3%20-%20UUCLxDeepMind%20DL2020.pdf">credit</a>.</p>

<h3 id="the-power-of-convolution">The power of convolution</h3>

<p>On the other hand, CNN is designed to scale well with images and take advantage of these unique properties. It does with two unique features:</p>
<ol>
  <li><strong>Weight sharing</strong>: All local parts of the image are processed with the same weights so that identical patterns could be detected at many locations, e.g., horizontal edges, curvs and etc.</li>
  <li><strong>Hierarchy of features</strong>: Lower-level patterns learned at the start are composed to form higher-level ones across layers, e.g., edges to contours to face outline.</li>
</ol>

<p>This is done through the operation of <strong>convolution</strong>:</p>
<ol>
  <li>Define a filter: a 2D weight matrix of a certain size, e.g. 3-by-3 filter.</li>
  <li>Convolve the whole image with the filter: multiply each pixel under the filter with the weight.</li>
  <li>Convolution output forms a new image: a feature map.</li>
  <li>By using multiple filters (each with a different weight matrix), different features can be captured.</li>
</ol>

<h4 id="convolution-example-mean-filter">Convolution example: mean filter</h4>
<p>Actually, let’s see the operation in numbers and images. It will be easier to understand what’s really going on. Here we create an image of a bright square using 0s and 1s. <code class="highlighter-rouge">matplotlib</code> interprets values in [0,1] the same as in [0, 255].</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original image pixel values: 
 [[0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]]
</code></pre></div></div>
<p>And this is how the image looks like: <img src="/assets/cnn-covid-19/bright_square.png" alt="square" /></p>

<p>Recall that a filter is a 2D weight matrix. Let’s create an example filter, and call it the <strong>“mean filter”</strong>:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.11 0.11 0.11]
 [0.11 0.11 0.11]
 [0.11 0.11 0.11]]
</code></pre></div></div>
<p>In a convolution, this “mean filter” actually slides across the image, takes the values of 9 connected pixels, multiplies each with the weight (0.11) and returns the sum, i.e., the weighted average of the original 9 values, hence the name “mean filter”: 
<img src="/assets/cnn-covid-19/convolution.gif" alt="convolution" /></p>

<p>You can see the averaging effect from the filtered image pixel values. It kind of blurs out any edges in the image.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Filtered image pixel values: 
 [[0.11 0.22 0.33 0.22 0.11]
 [0.22 0.44 0.67 0.44 0.22]
 [0.33 0.67 1.   0.67 0.33]
 [0.22 0.44 0.67 0.44 0.22]
 [0.11 0.22 0.33 0.22 0.11]]
</code></pre></div></div>

<p><strong>What’s this to do with a convolutional neural network?</strong></p>

<p>Well, CNN essentially applies the same convolution procedure, but the key difference is it <strong>learns the filter weights</strong> through backpropagation (training).</p>

<p>Also, for each layer, there are usually many filters, each with a different weight matrix, applied to the same image. Each filter would capture a different pattern of the same image. A CNN could also have many layers of convolution. The complexity of the network allows features at different scales to be captured. This is the hierarchy of features mentioned above.</p>

<p>For example, here’s an illustration of features learned by filters from early to latter part of the network.</p>
<ul>
  <li>Early filters capture edges and textures. (<strong>General</strong>)</li>
  <li>Latter filters form parts and objects. (<strong>Specific</strong>)
<img src="/assets/cnn-covid-19/feature.png" alt="title" /> Image <a href="https://distill.pub/2017/feature-visualization/">credit</a>.</li>
</ul>

<h3 id="key-features-of-cnn">Key features of CNN</h3>

<p>While DNN uses many fully-connected layers, CNN contains mostly convolutional layers.</p>

<ol>
  <li>In its simplest form, CNN is a list of layers that transform an image to a list of class probabilities.</li>
  <li>Some of the most popular types of layers are:
    <ul>
      <li><strong>Convolutional layer</strong> (CONV): image undergoes a convolution with filters.</li>
      <li><strong>RELU layer</strong> (RELU): element-wise nonlinear activation function (same as those in DNN before).</li>
      <li><strong>Pooling layer</strong> (POOL): image undergoes a convolution with a mean (or max) filter, so it’s down-sampled.</li>
      <li><strong>Fully-connected layer</strong> (FC): usually used as the last layer to output a class probability prediction.</li>
    </ul>
  </li>
</ol>

<p>Now, if you are <em>designing your own CNN</em>, there are many elements to play with. They generally fall into two categories:</p>

<ol>
  <li>Type of convolutional layer
    <ul>
      <li><strong>Depth</strong>: number of filters to use for each layer</li>
      <li><strong>Stride</strong>: how big of a step to take when sliding the filter across the image, usually 1 (see the convolution GIF above) or 2.</li>
      <li><strong>Size</strong>: size of each convolution filter, e.g., the mean filter is 3-by-3.</li>
      <li><strong>Padding</strong>: whether to use paddings around images when doing convolution. This determines the output image size.</li>
      <li>And others.</li>
    </ul>
  </li>
  <li>How to connect each layer
    <ul>
      <li>The actual architecture of your CNN.
        <ul>
          <li>This is an active field of research, e.g. what’s a better architecture? or can we automatically search for a better architecture? Check “neural architecture search” out if you are interested.</li>
        </ul>
      </li>
      <li>A commonly used architecture goes like this:
        <ul>
          <li>$\text{INPUT} \rightarrow [ [\text{CONV} \rightarrow \text{RELU}]^N \rightarrow \text{POOL}]^M \rightarrow [\text{FC} \rightarrow \text{RELU}]^K \rightarrow \text{FC}$</li>
          <li>The power $N, M, K$ means that the operation is repeated those number of times.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h1 id="covid-19-classifier-using-cnn">COVID-19 classifier using CNN</h1>

<h2 id="informs-data-challenge">INFORMS data challenge</h2>
<h2 id="classifier-via-transfer-learning">Classifier via transfer learning</h2>
<h3 id="pre-processing">Pre-processing</h3>
<h3 id="transfer-learning">Transfer learning</h3>

<h1 id="where-do-we-go-from-here">Where do we go from here?</h1>

<hr />
<h2 class="no_toc" id="references">References</h2>

:ET