<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xiaozhou's Notes</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2021-12-20T08:32:07+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Yang Xiaozhou</name>
   <email></email>
 </author>

 
 <entry>
   <title>Introduction to Python Programming: Course Review and Notebooks</title>
   <link href="http://localhost:4000/learning/2021/12/20/review-intro-to-Python.html"/>
   <updated>2021-12-20T00:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2021/12/20/review-intro-to-Python</id>
   <content type="html">&lt;p&gt;Suddenly, it seems like everybody is learning how to program and analyze data. One reason is the  opportunity presented by highly accessible computing power and the ease of capturing data. Another is the advent of massive open online course movement (MOOC) and ensuing online learning platforms. To learn something, we are no longer limited to physical classrooms or print (often expensive) textbooks. Educators from around the world design classes that reach student cohort of millions rather than hundreds, and students can come from incredibly diverse backgrounds. Many of the classes offered teach similar contents; They are in a market and “compete” with each other for better student reviews.&lt;/p&gt;

&lt;p&gt;In this article, I would like to recommend such a MOOC: &lt;a href=&quot;https://www.edx.org/professional-certificate/introduction-to-python-programming&quot;&gt;Introduction to Python Programming&lt;/a&gt;, a four-part course run by Dr. David Joyner from the Georgia Institute of Technology. It is not the most popular course on Python programming; It may not even be in &lt;strong&gt;the top 10 list&lt;/strong&gt;. However, I would highly recommend anyone who is or has been learning Python on their own to give this class a try. To quote the course overview:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The series is designed to take you from no computer science (CS) background whatsoever to proficiency in the basics of computing and programming, specifically in the popular programming language Python. Rated as one of the most in-demand and beginner-friendly programming languages, Python training will give you a solid foundation not only for Python code but for further studies in computer science.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So the questions is, why did I take this course? Certainly, I am not new to programming. I’ve been introduced to CS foundations through C, C++, and Java in my undergraduate days. I picked up Python for my Bachelor thesis on imbalanced data classification. Ever since then, I’ve been learning and using Python for the most part of my Ph.D..&lt;/p&gt;

&lt;p&gt;However, there are unfortunately not many memories left from my C and C++ classes (I guess it’s really use it or lose it); My learning of Python has mostly been sporadic and oriented towards solving something or using something. It was always about programming rather than the bigger picture of computing (This is one of the distinctions that Dr. Joyner makes early in the course). In short, I was looking for a formal introduction to the basics of computing and programming in Python.&lt;/p&gt;

&lt;p&gt;This course does exactly that. Dr. Joyner takes us through essential concepts in computing in a way that feels natural, paced, and yet progressive. Each chapter and subchapter builds on the previous one. I can’t applaud enough for Dr. Joyner’s delivery neither. He always uses common sense analogies to illustrate a particular computing concept. The result is that we walk away not only knowing how to code, but why.&lt;/p&gt;

&lt;p&gt;What sets this course apart though is the &lt;strong&gt;amount&lt;/strong&gt; of &lt;strong&gt;carefully designed&lt;/strong&gt; exercise problems that we get to practice on. For any learning to happen, practice is the key. This course excels on that by providing numerous exercises in each section. Straightforward multiple-choice questions and short coding exercises are provided early in a chapter. As we progress, they become a bit more challenging and yet still completely doable. Each coding exercise is also accompanied by detailed instructions and tips to help anyone in need. Looking back, I would say the videos and texts accounted for 50% of the learning and exercises the remaining 50%. There are many great features of this course that I did not mention, e.g., a companion smart book, real teaching assistants going through exercise problems, all the benefits of a self-paced online course.&lt;/p&gt;

&lt;p&gt;I have learnt about computing as much as I have reviewed in this course. Therefore, I would highly recommend it to anyone currently learning or has learnt Python programming one their own. To close the loop of “read, recite, and review”, I try to write a tutorial for each topic I learn. Rather than a replication of Dr. Joyner’s course reading, it is a concise set of notes written to summarize my own understanding. Additional materials are also occasionally added from further reading on the topic. Lastly, I’ve prepared these tutorials in Jupyter notebooks so that plenty of helpful coding exercises could be run concurrent to the text. My hope is that this set of notebooks can help me and you to quickly review fundamental Python computing concepts and techniques whenever we need in the future.&lt;/p&gt;

&lt;p&gt;You can find all the Jupyter notebooks in this Github &lt;a href=&quot;https://github.com/YangXiaozhou/Introduction_to_Python_programming&quot;&gt;repo&lt;/a&gt;, or go straight to one of them:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/YangXiaozhou/Introduction_to_Python_programming/blob/master/1_Foundamentals_and_procedural_programming.ipynb&quot;&gt;Fundamentals and procedural programming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/YangXiaozhou/Introduction_to_Python_programming/blob/master/2_Control_structures.ipynb&quot;&gt;Control structures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/YangXiaozhou/Introduction_to_Python_programming/blob/master/3_Data_structures.ipynb&quot;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/YangXiaozhou/Introduction_to_Python_programming/blob/master/4_Objects_and_algorithms.ipynb&quot;&gt;Objects and algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Or if you would like to read the text without any coding exercises, you can go here.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes for Introduction to Python Programming</title>
   <link href="http://localhost:4000/learning/2021/12/20/introduction_to_Python_programming.html"/>
   <updated>2021-12-20T00:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2021/12/20/introduction_to_Python_programming</id>
   <content type="html">&lt;p&gt;This is a four-part series of Introduction to Python Programming, based on Dr. David Joyner’s Edx &lt;a href=&quot;https://www.edx.org/professional-certificate/introduction-to-python-programming&quot;&gt;course&lt;/a&gt;. To see my review of the course, go here. To see the next Chapter, go here.&lt;/p&gt;

&lt;p&gt;[TOC]&lt;/p&gt;

&lt;h1 id=&quot;fundamentals-and-procedural-programming&quot;&gt;Fundamentals and procedural programming&lt;/h1&gt;

&lt;h2 id=&quot;computing&quot;&gt;Computing&lt;/h2&gt;

&lt;p&gt;First up, is computing and programming the same thing?&lt;/p&gt;

&lt;p&gt;Computing is loosely anything we do that involves computers. Programming is the act of giving a set of instructions for computers to act on. Computing is about what we want to do with computers using the programming language that we learnt.&lt;/p&gt;

&lt;p&gt;Here are some commonly used programming vocabularies to distinguish:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Program vs code
    &lt;ul&gt;
      &lt;li&gt;Program: A set of lines of code that usually serve one overall function.&lt;/li&gt;
      &lt;li&gt;A line of code: The smallest unit of programming that instructs computer to do something.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Input vs output
    &lt;ul&gt;
      &lt;li&gt;Input: What a program takes in to work with.&lt;/li&gt;
      &lt;li&gt;Output: What a program produces in return.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Compiling vs executing
    &lt;ul&gt;
      &lt;li&gt;Compiling: Compilers look for syntax errors in our code, like proofreading an article.&lt;/li&gt;
      &lt;li&gt;Executing: Actually running the program written.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Console vs graphical user interface (GUI):
    &lt;ul&gt;
      &lt;li&gt;Console: Command-line interface that is solely based on text input and output, e.g., Terminal on Mac.&lt;/li&gt;
      &lt;li&gt;GUI: Much more complex and commonly used interface of computer programs, e.g., Microsoft Word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What are programming languages?&lt;/p&gt;

&lt;p&gt;To be able to write Japanese essays, we need to learn Japanese, Similarly, to write computer programs, we need to learn programming languages. However, languages differ in many ways, it’s important to keep in mind the &lt;a href=&quot;https://www.realpythonproject.com/you-need-to-know-compiled-interpreted-static-dynamic-and-strong-weak-typing/&quot;&gt;language spectrum&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static and compiled: e.g., C, C++, Java. These languages do not allow variable type changes after declaration. The program is translated into machine-readable code at once.&lt;/li&gt;
  &lt;li&gt;Dynamic and interpreted: e.g., Python, JavaScript. These languages allow dynamically changing variable types. The program is translated into machine-readable code one line at a time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction-to-python&quot;&gt;Introduction to Python&lt;/h2&gt;

&lt;p&gt;Python was created in the 90s, became popular in 2000s. It is one of the most popular languages today. It is a high-level language in the sense that it abstracts aways from core computer processor and memory and is more portable across different operating systems. It is also an interpreted language as it runs code line by line without trying to compile the whole program first.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Python programming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The common work flow of programming is: Writing code –&amp;gt; Running code –&amp;gt; Evaluating results –&amp;gt; Repeat. Multiple instructions are chained together through lines of code. When we wish to see the value of a variable or the result of a program, &lt;code class=&quot;highlighter-rouge&quot;&gt;print()&lt;/code&gt; function is usually used. Also, it is recommended to work in small chunks of code at a time for Python programming.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First Python program: Hello, world!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We wish to write a program that outputs a message, “Hello, world!”. Note that &lt;code class=&quot;highlighter-rouge&quot;&gt;print&lt;/code&gt; is in lower case. Also, the message to be printed is enclosed by parenthesis&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compiling vs executing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The difference between compiling and executing a program can be understood through the analogy of building a table with parts and instructions. Compiling means reading the instructions to make sure every parts are in place and the instructions make sense. Executing means actually building the table with the instructions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compiling&lt;/em&gt;: Practically, this process looks over our code to see if everything makes sense. Technically, it also translates our code into lower-level machine-understandable code. Compiling before running is not required for every language, .e.g., interpreted languages like Python and JavaScript.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Executing&lt;/em&gt;: Actually running the code and letting it do when we want it to do.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluating results&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After executing our program, three scenarios can happen:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Run as intended.&lt;/li&gt;
  &lt;li&gt;Did not do what we intend it to do.&lt;/li&gt;
  &lt;li&gt;Run into errors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the first case, we obtain correct results and we’re happy. In the second case, the program executes successfully but produces incorrect results, e.g., instead of printing 1 to 10, it prints 1 to 9. In this case, we need to locate the code that causes the error and revise it. Finally, the program runs into errors and aborts right away. In another words, our program crashes. We need to locate the error-causing code and correct them.&lt;/p&gt;

&lt;h2 id=&quot;debugging&quot;&gt;Debugging&lt;/h2&gt;

&lt;p&gt;What is debugging?&lt;/p&gt;

&lt;p&gt;When our program causes error or produces incorrect results, we want to do debugging. Debugging is the process of finding out why our code doesn’t behave the way we want it to. It is also like doing research on our code; the aim is to gather as much information as necessary to debug.&lt;/p&gt;

&lt;p&gt;Debugging sometimes is like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/frs/Downloads/2_Data_science/0_Foundation/4_Computer_science/Introduction_to_Python_programming/assets/debugging.gif&quot; alt=&quot;debugging&quot; /&gt;&lt;/p&gt;

&lt;p&gt;or like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Users/frs/Downloads/2_Data_science/0_Foundation/4_Computer_science/Introduction_to_Python_programming/assets/debugging2.png&quot; alt=&quot;debugging&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;error-types-and-common-errors&quot;&gt;Error types and common errors&lt;/h3&gt;

&lt;p&gt;There are three types of errors with which we need to debug. We can still think of them in terms of building a table from instructions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compilation errors: These are the errors when we say: “The instructions do not make sense.” For example, the code syntax is incorrect (syntax error).&lt;/li&gt;
  &lt;li&gt;Runtime error: These are errors encountered when we are building the table/running the code. For example, if one line of code tries to divide by zero, it would cause divide-by-zero error.&lt;/li&gt;
  &lt;li&gt;Semantic error: If the table is successfully built but perhaps with reversed tabletop surface, we’ve encountered semantic error. For example, an incorrect code logic might not crash the program but produces an incorrect result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are four commonly encountered errors in Python:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;NameError&lt;/code&gt;: Using a variable that does not exist.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TypeError&lt;/code&gt;: Doing something that does not make sense for the type of variable.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AttributeError&lt;/code&gt;: Using attribute of some type of variable which does not make sense.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SyntaxError&lt;/code&gt;: A catch-all name for all sorts of syntax errors in Python.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;basic-debugging-techniques&quot;&gt;Basic debugging techniques&lt;/h3&gt;

&lt;p&gt;Here are some basic debugging techniques that people use frequently.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Print debugging/tracing: Printing out the status of the code throughout the program to identify which part is not running as expected.&lt;/li&gt;
  &lt;li&gt;Scope debugging: Debugging incrementally on the basis of small sections of code.&lt;/li&gt;
  &lt;li&gt;Rubber duck debugging: Explaining in detail the logic, goal, and operations of the code to a third person/object (e.g., a rubber duck).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;advanced-debugging-techniques&quot;&gt;Advanced debugging techniques&lt;/h3&gt;

&lt;p&gt;In some modern integrated development environments (IDEs), more advanced debugging support is provided.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Step-by-step execution: The program is run one line at a time, stopping anywhere we wish.&lt;/li&gt;
  &lt;li&gt;Variable visualization: We could also visualize all active variables as the program runs.&lt;/li&gt;
  &lt;li&gt;In-line debugging: Automatic checking of errors is done while we’re writing the code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To learn more about testing and debugging our code, check out the &lt;a href=&quot;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures/lecture-11/&quot;&gt;Testing and Debugging lecture&lt;/a&gt; from MIT OpenCourseware’s Introduction to Computer Science and Programming.&lt;/p&gt;

&lt;h2 id=&quot;procedural-programming&quot;&gt;Procedural programming&lt;/h2&gt;

&lt;p&gt;There are different programming paradigms to use when we write code for a program. Here are four common paradigms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Procedural programming: Giving lines of instructions to the computer to carry out in order.&lt;/li&gt;
  &lt;li&gt;Functional programming: Writing programs based on functions/methods with inputs and outputs.&lt;/li&gt;
  &lt;li&gt;Object-oriented programming: Writing programs with custom data types that have custom methods to express complex concepts.&lt;/li&gt;
  &lt;li&gt;Event-driven programming: Writing programs which wait and react to events rather than running code linearly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this course, we start with procedural programming, then functional programming. In the last Chapter, we introduce objected-oriented programming.&lt;/p&gt;

&lt;p&gt;Another important part of the code that we write is not computer code but comments and documentation. They are there to help future us and others understand the code so that future improvements can be made. There are mainly two kinds of comments.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-line comment: Alongside lines of code, to explain the specific line of code.&lt;/li&gt;
  &lt;li&gt;Code-block comment: before segments of code, to explain what the segment of code does.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variables&quot;&gt;Variables&lt;/h2&gt;

&lt;p&gt;What is a variable?&lt;/p&gt;

&lt;p&gt;It is a name in our program that holds some value. Variables are central to programming; Like variables in mathematics, they hold pieces of information. However, variables in computer programs can contain numbers and other things, e.g., characters, list of things, &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; values. The type of the value is called data type, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; is an integer. We use &lt;code class=&quot;highlighter-rouge&quot;&gt;type(a_variable)&lt;/code&gt; to access the type of &lt;code class=&quot;highlighter-rouge&quot;&gt;a_variable&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here are four basic data types in Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;int&lt;/code&gt;: Integer, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;10&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-100&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;: Floating point number, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;0.3&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-1000.0&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;str&lt;/code&gt;: A string of characters, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;'a'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;'a &amp;amp; b'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;'word'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;'A sentence is also a string.'&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bool&lt;/code&gt;: Boolean variable, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If possible, we could also convert data between different types. Here are the functions to do it in Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;int()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;float()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;str()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bool()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;operators&quot;&gt;Operators&lt;/h2&gt;

&lt;p&gt;Operators are the simplest ways to act on data, mostly on basic data types introduced above. There are two types of operators.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mathematical operators: Perform mathematical operations, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;. They are commonly known but not the most useful in programming.&lt;/li&gt;
  &lt;li&gt;Logical operators: Perform logical judgements, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;. They are less known but play a bigger role in programming.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Logical operators can be further broken down into relational (larger, smaller) and boolean (true, false) operators.&lt;/p&gt;

&lt;h3 id=&quot;mathematical-operators&quot;&gt;Mathematical operators&lt;/h3&gt;

&lt;p&gt;Most of us are familiar with mathematical operators:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;%&lt;/code&gt; (remainder), &lt;code class=&quot;highlighter-rouge&quot;&gt;//&lt;/code&gt; (floor division), &lt;code class=&quot;highlighter-rouge&quot;&gt;**&lt;/code&gt; (exponentiation)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A special meaning is attached to the equal sign &lt;code class=&quot;highlighter-rouge&quot;&gt;=&lt;/code&gt;: Assignment operator. We use &lt;code class=&quot;highlighter-rouge&quot;&gt;=&lt;/code&gt; to assign value to a variable.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Normal assignment: The format is &lt;code class=&quot;highlighter-rouge&quot;&gt;a_variable = a_value&lt;/code&gt;, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;a = 3&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Self-assignment: A Python way of assigning a new value to a variable based on the variable’s current value, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;+=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*=&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;number = number + 1&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;number += 1&lt;/code&gt;``&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;number = number / 3&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;number /= 3&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relational-operators&quot;&gt;Relational operators&lt;/h3&gt;

&lt;p&gt;Unlike mathematical operators, we always receive Boolean values (&lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;) as answers when using logical operators. There are two specific types of logical operators: relational and Boolean.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Relational operators: &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;==&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;in&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Relational operators compare the two sides and ask questions like: is the left hand side larger/smaller/equal to the right hand side? Specifically, &lt;code class=&quot;highlighter-rouge&quot;&gt;==&lt;/code&gt; is asking whether the two things are equal; &lt;code class=&quot;highlighter-rouge&quot;&gt;in&lt;/code&gt; is asking whether the left hand side is an element in the right hand side (e.g., if &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; $\in$ &lt;code class=&quot;highlighter-rouge&quot;&gt;[a, b, c]&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;There are some operators that work on strings as well. For example, relational comparison of two strings is done based on the order of their constituent characters, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;'abc &amp;gt; bac'&lt;/code&gt; would produce &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; since the position of &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; is before &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Strings work with: &lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;==&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;=&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt; concatenates two strings; &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; duplicates a string by an integer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;boolean-operators&quot;&gt;Boolean operators&lt;/h3&gt;

&lt;p&gt;Another kind of logical operators is Boolean operators. They act on Boolean values and also return Boolean values.&lt;/p&gt;

&lt;p&gt;The are &lt;code class=&quot;highlighter-rouge&quot;&gt;not&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;and&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;or&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;not&lt;/code&gt;: Flips the Boolean value that follows &lt;code class=&quot;highlighter-rouge&quot;&gt;not&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;and&lt;/code&gt;: Only returns &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt; if both sides are true.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;or&lt;/code&gt;: Returns &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt; when at least one side is true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;not True&lt;/code&gt; would produce &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;True and False&lt;/code&gt; would produce &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;True or False&lt;/code&gt; would produce &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;. Sometimes we encounter a long chain of Boolean evaluation. Python has an internal order of evaluation: &lt;code class=&quot;highlighter-rouge&quot;&gt;not&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;and&lt;/code&gt; &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;or&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;control-structures&quot;&gt;Control structures&lt;/h1&gt;

&lt;p&gt;Control structures are programming statements that control, in what sequence, a series of code are run. They put some “human logic” in to a block of code. Until now, all the programs that we’ve written run from top to bottom in a linear sequence. It’s easier to interpret and debug. However, the power of programming shows significantly once we add in control structures. For example, control structures we learn next will be able do things like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If a pre-specified condition is met, run code block A; If not, run code block B.&lt;/li&gt;
  &lt;li&gt;If condition 1, 2, and 3 are all met, run code block A; If condition 1 is met, run code block B; If condition 2 is met, run code block C; if condition 3 and 4 are met, run code block D.&lt;/li&gt;
  &lt;li&gt;Repeatedly run the next block of code for 10 time.&lt;/li&gt;
  &lt;li&gt;Repeatedly run the next block of code until a pre-specified condition is met.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And many other powerful things.&lt;/p&gt;

&lt;h2 id=&quot;conditionals&quot;&gt;Conditionals&lt;/h2&gt;

&lt;p&gt;The first kind of control structures is called conditionals. They are statements that control which segment of code is executed based on the result of a condition evaluation.&lt;/p&gt;

&lt;p&gt;In Python, we work with three conditional keywords: &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;elif&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt;.  See the following example of a chained conditional.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Outside if-else block
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expression_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expression_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;Some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Outside if-else block
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice a few things in the conditional template above:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conditional code forms a block by itself. Code above it and below it are not affected by the control logic, i.e., they are not subject to any of the conditional evaluation.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;elif&lt;/code&gt; means “if something is true” and “&lt;em&gt;el&lt;/em&gt;se, &lt;em&gt;if&lt;/em&gt; something is true”.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;expression&amp;gt;&lt;/code&gt; follows &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;elif&lt;/code&gt; to specify what that “something” is. It is a Boolean value or the equivalent of it, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;10 &amp;gt; 0&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;True and False&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;expression_1&amp;gt;&lt;/code&gt; is not true, the program skips Code block A and continues to the next conditional evaluation.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt; means all the previous conditionals are rejected, as a last resort, we’ll run Code block C, without evaluating any condition.&lt;/li&gt;
  &lt;li&gt;In a chained conditional like the above, at most one block of code is run. Which code block runs depend on the condition evaluation result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides Boolean evaluations, there are a few special things that are considered as &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; in Python:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Empty strings: &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&quot;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;''&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Empty list: &lt;code class=&quot;highlighter-rouge&quot;&gt;[]&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Singleton none: &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python supports a shorter way to express single if-else conditional evaluation. Instead of the sequential way above, we can write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;a_line_of_code if &amp;lt;expression&amp;gt; else another_line_of_code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code runs &lt;code class=&quot;highlighter-rouge&quot;&gt;a_line_of_code&lt;/code&gt; if &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;expression&amp;gt;&lt;/code&gt; is true. If not, it runs &lt;code class=&quot;highlighter-rouge&quot;&gt;another_line_of_code&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;loops&quot;&gt;Loops&lt;/h2&gt;

&lt;p&gt;What if we wish to execute a segment of code repeatedly, e.g., for ten times, or until a condition is met? Loops allow us to do exactly that. There are two Python keywords to construct loops: &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;while&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; constructs loops where the number of repeats is known; &lt;code class=&quot;highlighter-rouge&quot;&gt;while&lt;/code&gt; is used when we only know a terminating condition.&lt;/p&gt;

&lt;p&gt;Here’s a template for &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; loops:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# For each number i in the list of [1, 2, 3]
&lt;/span&gt;		&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
  
&lt;span class=&quot;c1&quot;&gt;# The code above prints
# 1
# 2
# 3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the template for &lt;code class=&quot;highlighter-rouge&quot;&gt;while&lt;/code&gt; loop:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Repeat while number is less than or equal to 3
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;# The code above prints
# 1
# 2
# 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are some keywords that we can use inside the loop for special control purposes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;continue&lt;/code&gt;: Ignore the rest of the code in this current loop, continue with the next round of execution.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;break&lt;/code&gt;: Ignore the rest of the code in the current loop and exit the loop.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pass&lt;/code&gt;: Do nothing, continue with the next iteration in the loop.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;functions&quot;&gt;Functions&lt;/h2&gt;

&lt;p&gt;Function is a block of of related code grouped together to perform a specific task. They replace repetitive codes with a simple line of function call. They help to make our program concise and organized. Functions can be built-in by a language or used-defined.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An example&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Define a function
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Add a, b, and other things in args.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
  	
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
  
&lt;span class=&quot;c1&quot;&gt;# Calling(using) a function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# b has a default value of 10.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above is an example of defining a function in Python:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The function is declared using the keyword &lt;code class=&quot;highlighter-rouge&quot;&gt;def&lt;/code&gt; followed by the function name that we give it.&lt;/li&gt;
  &lt;li&gt;A function can take as many arguments as necessary separated by comma.&lt;/li&gt;
  &lt;li&gt;Functions docstring using &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&quot;&quot;&lt;/code&gt; details what the function does and how to use it.&lt;/li&gt;
  &lt;li&gt;The function returns some value through the keyword &lt;code class=&quot;highlighter-rouge&quot;&gt;return&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Types of arguments&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are four different types of arguments that we can define in a Python function.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keyword arguments: They do not have pre-assigned values in the function definition. While calling a function, these positional arguments have to be given values in their order of appearance. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; in the above example is such an argument.&lt;/li&gt;
  &lt;li&gt;Default arguments: They are arguments which are given default values during function definition. Hence, it may not be necessary to given these values when calling the function. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; in the above example is such an argument.&lt;/li&gt;
  &lt;li&gt;Variable-length arguments&lt;/li&gt;
  &lt;li&gt;These are lists of arguments which we do not known in advance how long they are. They are defined in the function definition using *args.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The concept of scope&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scope means the portion of program that a variable can be seen and accessed.&lt;/li&gt;
  &lt;li&gt;Compiled languages are generally different in scope design from scripted languages.&lt;/li&gt;
  &lt;li&gt;Scope is usually defined by control structures.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scoping in Python: Built-in scope &amp;gt;= global scope &amp;gt;= enclosing scope &amp;gt;= local scope&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Local scope: Variables created inside Python functions.&lt;/li&gt;
  &lt;li&gt;Enclosing scope: Not local nor global scope, hence nonlocal scope, e.g. inside one function but outside another.&lt;/li&gt;
  &lt;li&gt;Global scope: Variables which can be accessed anywhere in a program.&lt;/li&gt;
  &lt;li&gt;Build-in scope: Largest scope, all variable names loaded into Python interpreter, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;print()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;len()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;int()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Parameter: Variable that functions expect to receive an input.&lt;/p&gt;

&lt;p&gt;Argument: Value of the function parameter passed as input.&lt;/p&gt;

&lt;h2 id=&quot;error-handling&quot;&gt;Error handling&lt;/h2&gt;

&lt;p&gt;Exception handling: Structure that anticipates and catches errors to avoid crashing and execute certain code.&lt;/p&gt;

&lt;p&gt;Python error handling:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;try&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;except&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;finally&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;try&lt;/code&gt; to execute this segment of code, &lt;code class=&quot;highlighter-rouge&quot;&gt;except&lt;/code&gt; some error occurs, then do that instead. &lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt;, if no error occurred, run this segment of code. &lt;code class=&quot;highlighter-rouge&quot;&gt;finally&lt;/code&gt;, regardless of the occurrence of errors, run this segment of code.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;finally&lt;/code&gt;: segment of code to run no matter what happens above, e.g., closing a file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nested exception-handling structure&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Errors escalate up incrementally to see if they can be caught.&lt;/li&gt;
  &lt;li&gt;Errors also rise through function calls, e.g., an error occurred inside a function.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;data-structures&quot;&gt;Data structures&lt;/h1&gt;

&lt;h2 id=&quot;data-structures-1&quot;&gt;Data structures&lt;/h2&gt;

&lt;h3 id=&quot;passing-by-value-vs-passing-by-reference&quot;&gt;Passing by value vs passing by reference:&lt;/h3&gt;

&lt;p&gt;By value: Giving variable values to functions. Functions can can’t change the original value.&lt;/p&gt;

&lt;p&gt;By reference: Giving variable references to functions. Functions can access and change the original value.&lt;/p&gt;

&lt;h3 id=&quot;passing-by-value-vs-passing-by-reference-in-python&quot;&gt;Passing by value vs passing by reference in Python&lt;/h3&gt;

&lt;p&gt;Different languages have different ways of differentiating these two types.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;C: variableName (value) vs *variableName (reference)&lt;/li&gt;
  &lt;li&gt;Java: primitive types (value) vs non-primitive types (reference)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python seems to handle everything via pass-by-value.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;However, not all types of data are passed by value, e.g., integers, strings.&lt;/li&gt;
  &lt;li&gt;The key to discern Python’s behavior is: Mutability of the data type.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mutability-in-python&quot;&gt;Mutability in Python&lt;/h3&gt;

&lt;p&gt;Mutability: Whether or not a variable’s value can change after being declared.&lt;/p&gt;

&lt;p&gt;In general, all variables in Python are passed by reference. However, only values of mutable data types can be changed, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;list&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;set&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;dict&lt;/code&gt;. Immutable data types in Python include: &lt;code class=&quot;highlighter-rouge&quot;&gt;int&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tuple&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;bool&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;str&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;How Python actually work for immutable data types?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_int = 5&lt;/code&gt;: Python grabs a memory spot and stores ‘5’ in that spot. &lt;code class=&quot;highlighter-rouge&quot;&gt;my_int&lt;/code&gt; is asked to point to that memory spot with &lt;code class=&quot;highlighter-rouge&quot;&gt;id(my_int)&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_int_2 = my_int&lt;/code&gt;: Python asks &lt;code class=&quot;highlighter-rouge&quot;&gt;my_int_2&lt;/code&gt; point to the same memory spot as &lt;code class=&quot;highlighter-rouge&quot;&gt;my_int&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_int_2 = my_int_2 + 1&lt;/code&gt;: Python creates a new value ‘6’ and grabs a new memory spot to store it. Then it asks &lt;code class=&quot;highlighter-rouge&quot;&gt;my_int_2&lt;/code&gt; point to the new memory spot as ‘6’ is an immutable data type.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basically, for immutable data types, when Python is asked to change its value, it does not change it on the spot. It stores the new value at a new memory location and asks the variable point to the new location instead. Therefore:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Identical values have identical memory address for immutable data types.&lt;/li&gt;
  &lt;li&gt;Each new variable has a new memory address for mutable data types, regardless of the value.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;strings&quot;&gt;Strings&lt;/h2&gt;

&lt;h3 id=&quot;what-are-strings&quot;&gt;What are strings?&lt;/h3&gt;

&lt;p&gt;Some relevant concepts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;String: A data structure that holds a list, or string of characters, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Hello world!&quot;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Character: A single letter, number, symbol, or special character, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;$&quot;&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Unicode: A computing industry standard that sets the correspondence between hexadecimal codes and characters, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;U+0024&lt;/code&gt; means &lt;code class=&quot;highlighter-rouge&quot;&gt;$&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Special characters in programming: new line character, tab, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The true complexity lies in trying to teach computers that way we human view and use these strings of characters. For example, how do we teach computers the concepts of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ordering: &lt;code class=&quot;highlighter-rouge&quot;&gt;'A' &amp;lt; 'B'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;'Aa' &amp;lt; 'Ab'&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Capitalization: To capitalize a sentence means &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Do this instead&quot;&lt;/code&gt; -&amp;gt;  &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Do This Instead&quot;.&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Cases: Recognize different cases, and conversion between different cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;declaring-strings-in-python&quot;&gt;Declaring strings in Python&lt;/h3&gt;

&lt;p&gt;Python seeks the start and end of the string by matching &lt;code class=&quot;highlighter-rouge&quot;&gt;'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&lt;/code&gt;, or &lt;code class=&quot;highlighter-rouge&quot;&gt;'''&lt;/code&gt; characters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A normal string: &lt;code class=&quot;highlighter-rouge&quot;&gt;a = &quot;a string&quot;&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;a = 'a string'&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;A string with quotation: &lt;code class=&quot;highlighter-rouge&quot;&gt;a = &quot;'a string'&quot;&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;a = '&quot;a string&quot;'&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;A string with double quotation: &lt;code class=&quot;highlighter-rouge&quot;&gt;a = ''' &quot; 'a string' &quot; '''&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, we would run into trouble if we wish to declare special characters such as the new line character &lt;code class=&quot;highlighter-rouge&quot;&gt;\n&lt;/code&gt; or the tab character &lt;code class=&quot;highlighter-rouge&quot;&gt;\t&lt;/code&gt;. The trick is to use &lt;code class=&quot;highlighter-rouge&quot;&gt;\&lt;/code&gt; as the escape character. It tells the computer to treat these special characters as normal, e.g, &lt;code class=&quot;highlighter-rouge&quot;&gt;'\\n'&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;'\\t'&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;string-concatenation-and-slicing-in-python&quot;&gt;String concatenation and slicing in Python&lt;/h3&gt;

&lt;p&gt;Two of the commonly used operations on strings are concatenation and slicing.&lt;/p&gt;

&lt;p&gt;Concatenation: Put two or more strings together to form a new one.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_string_1 + my_string_2&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;a string&quot; + my_string_1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_string_1 += my_string_2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slicing: Obtain substrings from a string in Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using single index: &lt;code class=&quot;highlighter-rouge&quot;&gt;[index]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[0]&lt;/code&gt; gets the first, &lt;code class=&quot;highlighter-rouge&quot;&gt;[-1]&lt;/code&gt;gets the last, and &lt;code class=&quot;highlighter-rouge&quot;&gt;[i]&lt;/code&gt; gets the (i+1)th character.&lt;/li&gt;
      &lt;li&gt;Zero-based indexing: In early computing age, a variable name points to the first item in the string/list. The index means “how many items do I skip in order to get the target item”, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;[3]&lt;/code&gt; says skipping three items from the first item, i.e., to obtain the fourth item.&lt;/li&gt;
      &lt;li&gt;Zero-based indexing
        &lt;ul&gt;
          &lt;li&gt;Legacy reason, in earlier primitive languages, variable pointers point to the first element in a list. Thus index then means how many items to skip to reach the target item, e.g. to get 5th item, we need to skip 4 items.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using index range: &lt;code class=&quot;highlighter-rouge&quot;&gt;[start:end]&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;[start:]&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;[:end]&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;[start:end:space]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Start from the item at index &lt;code class=&quot;highlighter-rouge&quot;&gt;start&lt;/code&gt; and get a substring with length &lt;code class=&quot;highlighter-rouge&quot;&gt;end - start&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;space&lt;/code&gt; to get alternate characters, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;[::2]&lt;/code&gt; gets all items at even index.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using negative index
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[-n:]&lt;/code&gt;: Starts from the $n$th item from the end.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[:-n]&lt;/code&gt;: Ends at the $n$th item from the end.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;string-searching&quot;&gt;String searching&lt;/h3&gt;

&lt;p&gt;Another commonly used operation is searching a certain substring in another string. For example, we would like to know if &lt;code class=&quot;highlighter-rouge&quot;&gt;my_string_1&lt;/code&gt; is contained in &lt;code class=&quot;highlighter-rouge&quot;&gt;my_string_2&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using Python keyword &lt;code class=&quot;highlighter-rouge&quot;&gt;in&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;not in&lt;/code&gt; for a boolean answer.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'David' in &quot;Today is David's birthday&quot;&lt;/code&gt; would give &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;'star' not in 'A lot of stairs' &lt;/code&gt; would also give &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;find()&lt;/code&gt;: A method defined for the string data type, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;my_string_1.find('star')&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;Find the index of the first instance of the substring. If not found, &lt;code class=&quot;highlighter-rouge&quot;&gt;-1&lt;/code&gt; is returned.&lt;/li&gt;
      &lt;li&gt;We could specify the start and end of the original string to conduct the search.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;count()&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Count the number of times a substring is found in a string.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;useful-string-methods-in-python&quot;&gt;Useful string methods in Python&lt;/h3&gt;

&lt;p&gt;There are some other useful methods that we could use on strings in Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;split(separator)&lt;/code&gt;: Split a string into a list of substrings using the separator.&lt;/li&gt;
  &lt;li&gt;Case conversion: Convert a string to a different case.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;capitalize()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;lower()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;upper()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;title()&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;strip()&lt;/code&gt;: Strip out all white spaces in a string.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replace(old, new)&lt;/code&gt;: Replace all &lt;code class=&quot;highlighter-rouge&quot;&gt;old&lt;/code&gt; substrings with &lt;code class=&quot;highlighter-rouge&quot;&gt;new&lt;/code&gt; substrings in a string.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;-&quot;.join(a_list)&lt;/code&gt;: Use “-“ to join all string items in a list and form a new string.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;list-list-structures&quot;&gt;List-list structures&lt;/h2&gt;

&lt;p&gt;A type of data structure that holds multiple individual values under one variable name, accessed by numeric indices. Examples in Python include tuple, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;(1,2,'cat',6)&lt;/code&gt;, and list, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;['a', 2, 'b']&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tuple: An immutable form of list-like structure.&lt;/li&gt;
  &lt;li&gt;List: A mutable form of list-like structure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both of them support holding non-homogenous data types, e.g., a tuple has both integers and strings.&lt;/p&gt;

&lt;h3 id=&quot;tuples-in-python&quot;&gt;Tuples in Python&lt;/h3&gt;

&lt;p&gt;There are two ways which we can use to access items in a tuple:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using index: Exactly like how we use indices to access substrings in strings.&lt;/li&gt;
  &lt;li&gt;Using unpacking technique: &lt;code class=&quot;highlighter-rouge&quot;&gt;(a, b, c, d) = a_tuple&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usefulness of tuple: Return multiple items from a function, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;return (a,b,c,d)&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lists-in-python&quot;&gt;Lists in Python&lt;/h3&gt;

&lt;p&gt;Lists are much more commonly used in Python programming. All the things that work with tuples as mention previously work with lists. The big difference is that lists are mutable. Hence we can do all sorts of operations on lists. For example, here are some common ones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a_list.sort()&lt;/code&gt;: Sort the items in &lt;code class=&quot;highlighter-rouge&quot;&gt;a_list&lt;/code&gt;, default in ascending order.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a_list.reverse()&lt;/code&gt;: Reverse the order of the items.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;del a_list[i:j]&lt;/code&gt;: Delete &lt;code class=&quot;highlighter-rouge&quot;&gt;j-i&lt;/code&gt; items from &lt;code class=&quot;highlighter-rouge&quot;&gt;a_list &lt;/code&gt; starting from index &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a_list.remove(x)&lt;/code&gt;: Delete item &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;a_list&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a_list[i] = x&lt;/code&gt;: Assignment value of &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; to the list item at index &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;advanced-list-like-structures&quot;&gt;Advanced list-like structures&lt;/h3&gt;

&lt;p&gt;There are more types of list-like structures that have specific advantage for certain applications. They are usually lists with certain restrictions on how items can be accessed or modified.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stacks
    &lt;ul&gt;
      &lt;li&gt;Only the most recently added item can be accessed. Each subsequent item is accessed by  removing the next recently added one. In inventory management language, it’s called first in last out, like stacks of finished goods.&lt;/li&gt;
      &lt;li&gt;One example is the code execution sequence in procedural programming.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Queues
    &lt;ul&gt;
      &lt;li&gt;Items added first are accessed first. Each subsequent item is accessed by removing the previous one. This is also called first in first out, like queues of customers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Linked-lists
    &lt;ul&gt;
      &lt;li&gt;Linked-lists are different from other lists where each item has an index and can be accessed via a sequence of indices.&lt;/li&gt;
      &lt;li&gt;The location of an item in a linked-list is only contained in the previous item in the list.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;file-input-and-output&quot;&gt;File input and output&lt;/h2&gt;

&lt;p&gt;If we want to have some data of our program persist even after the session ends, we need a way to store and read data from our program.  File input and output are complementary processes of saving data to a file and loading data from a file. A file’s encoding specifies a set of rules about writing data to that file and loading data from it. In the most basic form, we would concentrate on dealing with text (&lt;code class=&quot;highlighter-rouge&quot;&gt;.txt&lt;/code&gt;) files.&lt;/p&gt;

&lt;h3 id=&quot;reading-writing-and-appending&quot;&gt;Reading, writing, and appending&lt;/h3&gt;

&lt;p&gt;The basic flow of interacting with files involve:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Opening a file and assign it to a variable.&lt;/li&gt;
  &lt;li&gt;Do whatever operations we want with the file, e.g., reading data from it, writing data to it.&lt;/li&gt;
  &lt;li&gt;Close the file after we are done.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To open and close files in Python, we use these functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file = open(file_name, mode)&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;file_name&lt;/code&gt; is the address of the write and &lt;code class=&quot;highlighter-rouge&quot;&gt;mode &lt;/code&gt; is one of the three described below.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.close()&lt;/code&gt;: Closing the file after we’re done with our operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we open the file in a program, we usually specify a mode of interaction. There are three modes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reading, &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;: It means that we are only reading the file and do not intend to modify it.&lt;/li&gt;
  &lt;li&gt;Writing, &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;: We are going to erase everything on it and write some new data into it.&lt;/li&gt;
  &lt;li&gt;Appending, &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;: We are going to append some new data to whatever content the file already has.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s important to specify the mode. Also, opening a file prevents other programs from modifying it. That’s why we need to close it too after we’re done.&lt;/p&gt;

&lt;h3 id=&quot;writing-files-in-python&quot;&gt;Writing files in Python&lt;/h3&gt;

&lt;p&gt;Basic functions involved in writing data to files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.write()&lt;/code&gt;: Write one string of text into the file.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.writelines()&lt;/code&gt;: Write a collection of items into the file.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;print(text, file = file_name)&lt;/code&gt;: This is equivalent to writing &lt;code class=&quot;highlighter-rouge&quot;&gt;text + '\n'&lt;/code&gt; to the file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reading-files-in-python&quot;&gt;Reading files in Python&lt;/h3&gt;

&lt;p&gt;The reverse process of writing data to a file is reading it from a file. By default, Python treats contents read from a file as text. We would need to convert them to other types if needed. Here are some basic functions of reading from a file:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.readline()&lt;/code&gt;: Read the content of a line in the file until the start of the next line.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.readlines()&lt;/code&gt;: Read the content of the file line by line and store them in a list of strings.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;file.read().splitlines()&lt;/code&gt;: Read the content of the file and split the content into a list of strings using new line character.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dictionaries&quot;&gt;Dictionaries&lt;/h2&gt;

&lt;p&gt;Dictionary is one of many powerful data structures. It is a data structure that holds multiple key-value pairs where keys can be used to look up corresponding values. The idea of dictionary is common in different programming languages. They are similarly known as maps, associative arrays, hashmaps, or hashtables. In dictionary, there’s a one-to-one correspondence between a key and a value. Notably, the order of the collection of key-value pairs do not matter in a dictionary, unlike lists.&lt;/p&gt;

&lt;h3 id=&quot;dictionaries-in-python&quot;&gt;Dictionaries in Python&lt;/h3&gt;

&lt;p&gt;While strings are declared with &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&quot;&lt;/code&gt; and lists are declared with &lt;code class=&quot;highlighter-rouge&quot;&gt;[]&lt;/code&gt;, dictionaries are declared with &lt;code class=&quot;highlighter-rouge&quot;&gt;{}&lt;/code&gt; in Python. For example, a simple dictionary with two arbitrary keys and values can be declared as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a_dict = {key_1: value_1; key_2: value_2}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python requires that dictionary keys have to be an immutable data type such as string or integer. However, dictionaries themselves are mutable. Hence, we can perform various operations on the values of items in a dictionary, via their keys:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Modifying a value: &lt;code class=&quot;highlighter-rouge&quot;&gt;a_dict[key] = new_value&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Adding a new item: &lt;code class=&quot;highlighter-rouge&quot;&gt;a_dict[new_key] = new_value&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Deleting an item: &lt;code class=&quot;highlighter-rouge&quot;&gt;del a_dict[key]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Checking the presence of an item: &lt;code class=&quot;highlighter-rouge&quot;&gt;a_key in a_dict&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another important operation that we often use is traversing through items in a dictionary. We can do so in three common ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for key in a_dict.keys()&lt;/code&gt;: Go through the list of keys.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for value in a_dict.values()&lt;/code&gt;: Go through the list of values.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for (key, value) in a_dict.items()&lt;/code&gt;: Go through the list of (key, value) pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;objects-and-algorithms&quot;&gt;Objects and algorithms&lt;/h1&gt;

&lt;p&gt;This chapter introduces us to two fundamental pillars of knowledge in computer science: object-oriented programming (OOP) and algorithms. Breadth over depth is emphasized in this chapter.&lt;/p&gt;

&lt;h2 id=&quot;objects&quot;&gt;Objects&lt;/h2&gt;

&lt;h3 id=&quot;what-are-objects&quot;&gt;What are objects?&lt;/h3&gt;

&lt;p&gt;Objects are concepts from OOP paradigm.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In contrast to procedural or functional programming, OOP is a paradigm where custom data types are defined and used with custom methods defined for them.&lt;/li&gt;
  &lt;li&gt;Class: We usually refer to such a data type as a class with custom methods and variables (also known as class attributes).&lt;/li&gt;
  &lt;li&gt;Objects and instances: An object or instance is what we usually refer to a particular “variable” created from a class. For example:
    &lt;ul&gt;
      &lt;li&gt;From a &lt;code class=&quot;highlighter-rouge&quot;&gt;person&lt;/code&gt; class, we can create a particular person, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;David&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Mary&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;From a &lt;code class=&quot;highlighter-rouge&quot;&gt;book&lt;/code&gt; class, we can create a particular book, e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;book_A&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;book_B&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, to understand the class-instance relationship, we can think of the relationship between an unfilled form and a filled one: An unfilled form represents a template, a class; The form can then be filled with different information by different people to create different instances.&lt;/p&gt;

&lt;h3 id=&quot;objects-and-instances-in-python&quot;&gt;Objects and instances in Python&lt;/h3&gt;

&lt;p&gt;Just like how we need to declare a function before using it, to work with objects and instances, we first need to declare the class. Declaring classes in Python is essentially teaching the computer what kind of concepts make sense for the class object. For example&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To make a &lt;code class=&quot;highlighter-rouge&quot;&gt;book&lt;/code&gt; class, each book instance could have a title, an author, a publication date, etc.&lt;/li&gt;
  &lt;li&gt;To make a &lt;code class=&quot;highlighter-rouge&quot;&gt;person&lt;/code&gt; class, each person instance could have a name, an age, an eye color, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During declaration, Python uses the dot syntax &lt;code class=&quot;highlighter-rouge&quot;&gt;self.&lt;/code&gt; to assign or access its attributes such as title or author.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For example, to set a book instance’s name as &lt;code class=&quot;highlighter-rouge&quot;&gt;'Python Programming'&lt;/code&gt;, we can write: &lt;code class=&quot;highlighter-rouge&quot;&gt;self.name = 'Python Programming'&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;encapsulating-methods-in-classes&quot;&gt;Encapsulating methods in classes&lt;/h3&gt;

&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;

</content>
 </entry>
 
 <entry>
   <title>2021年1月吃：慢烤手撕猪肉和椒麻鸡</title>
   <link href="http://localhost:4000/learning/2021/01/26/journal-kitchen-2021-01.html"/>
   <updated>2021-01-26T00:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2021/01/26/journal-kitchen-2021-01</id>
   <content type="html">&lt;p&gt;记录且分享1月享受的两道菜。&lt;/p&gt;

&lt;h2 id=&quot;slow-cooked-smoky-pulled-pork--慢烤手撕猪肉&quot;&gt;Slow-cooked smoky pulled pork | 慢烤手撕猪肉&lt;/h2&gt;

&lt;p&gt;搬了新家之后厨房第一次有了烤箱，迫不及待想要用它来做个菜，搜寻一番，决定做个&lt;a href=&quot;https://youtu.be/hFd8vijqBzI?t=332&quot;&gt;慢烤手撕猪肉&lt;/a&gt; （roasted smoky pulled pork）。这道菜似乎经常在节日和派对的餐桌上出现，原因我也不知道，可能因为这不是一道随便进个餐厅就能吃到的菜，也可能因为如果一顿吃不完，剩下的也非常适合做三明治、早餐什么的，不会用担心浪费。&lt;/p&gt;

&lt;p&gt;烤猪肉最重要的当然是猪肉了，选哪儿的肉倒还是有一番讲究。一般是推荐用猪的肩胛肉（梅花肉，pork butt, Boston butt），在猪的后颈肉之后里脊肉之前偏上的一部分，大约在人的背阔肌的部分吧（瞎说的）。这块肉的特点是肥瘦相间并且脂肪的纹路跟和牛相似（大理石纹路的油花），经过长时间的烤或煮，肉脂融掉，风味全部释放出来非常浓郁。要是没有pork butt, pork shoulder也行，但脂肪含量会相对低一些，没那么多汁。我这次就是在Fairprice finest的鲜肉柜台买到的去骨的pork shoulder，一般摆出来的是切成小块，可以让店员从厨房拿一整块的按量切给你。&lt;/p&gt;

&lt;p&gt;既然是整块肉慢烤，提前腌制肯定少不了了。蒜蓉、盐、胡椒、烟红椒粉、糖、橄榄油，再加上些新鲜香料（迷迭香、百里香之类），全招呼上，打碎拌匀涂抹在肉的外表，在香气中忍住口水，把肉密封放进冰箱，隔夜腌制。肉的准备到此就结束了。第二天提前六小时从冰箱里拿出来，放半小时等肉恢复一下温度，然后切点洋葱，放点新鲜香料、蒜铺在烤盘底下，猪肉盖住洋葱，140度五个半小时到六个小时，让烤箱慢慢发挥它的魔力。&lt;/p&gt;

&lt;p&gt;等肉快好的时候，就可以准备蘸料和配菜了。 一般手撕猪肉会配上蛋黄酱为主的蘸料（蛋黄酱，盐胡椒，蜂蜜，辣酱，芥末酱），然后下着面包、饼等面食，夹点新鲜洋葱、腌黄瓜、番茄吃，这样的吃法很适合派对。不过我想再配上些蔬菜，模仿Jamie Oliver的&lt;a href=&quot;https://www.youtube.com/watch?v=V67aiurjnAE&amp;amp;ab_channel=JamieOliver&quot;&gt;做法&lt;/a&gt;，准备些叶子类的蔬菜，盐水中汆烫后沥水切成小条备用。把烤过的洋葱和大蒜跟焗豆和鹰嘴豆（超市买的罐装）放在一起小火慢煎，放点红酒（或者其他酒），按喜好放点Crème fraîche（法式酸奶油，类似勾芡？让汤汁变粘稠，而且有香甜的奶味），最后出来的复合味十分诱人。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/recipes/pulled_pork_1.jpg&quot; alt=&quot;pulled_pork_1&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/recipes/pulled_pork_2.jpg&quot; alt=&quot;pulled_pork_2&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/assets/recipes/pulled_pork_3.jpg&quot; alt=&quot;pulled_pork_3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;乐山椒麻鸡--leshan-numbing-and-spicy-chicken&quot;&gt;乐山椒麻鸡 | Leshan Numbing and Spicy Chicken&lt;/h2&gt;

&lt;p&gt;淅淅沥沥的一月，我们需要一点唤醒所有感官的食物。这个月第二道值得记录的菜来自老饭骨里乐山师傅介绍的&lt;a href=&quot;https://www.youtube.com/watch?v=nqHMWAbLldU&amp;amp;ab_channel=%E8%80%81%E9%A5%AD%E9%AA%A8&quot;&gt;乐山椒麻鸡&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一般来说这类凉拌、爆炒的鸡肉似乎都推荐用小公鸡，就是三个月左右的公鸡（童子鸡）或四五个月左右已经会打鸣的公鸡，整个也就两三公斤，因为肉质比较鲜嫩？而炖汤则推荐使用老母鸡，一般是三年以上的母鸡，寿命更久，脂肪更多（油香），呈味核苷酸更多（鲜），当然这些是理论上的结论，我也没亲身实践测试过。而我秉承就地取材的原则，用的是Fairprice肉柜里最普通的当天处理好的整鸡肉，肉质的量和口感也是不错的。&lt;/p&gt;

&lt;p&gt;椒麻鸡除了鸡，另外一大重要的原材料就是油。椒麻椒麻，第一是要突出它的麻，其次还得有辣。做过（吃过）川菜的朋友应该就知道了，我们需要花椒油和辣椒油。花椒油最好是用新鲜的藤椒（油脂丰富，香与麻兼具），重庆四川那边以前很多人家家门口就会种一棵花椒树，做菜需要了，摘两把新鲜的藤椒，淋上热油，就做成了藤椒油，没有新鲜的也可以用超市里能买到的藤椒油，或者如果有袋装的颗粒青花椒，也可以放些花椒在油里用小火慢煎出香麻味，沥掉花椒粒，剩下的油就是花椒油了。辣椒油就是川菜经常会用到的香辣红油，不管是做凉拌菜还是做面食的调料都很好使，制作可以参考这个&lt;a href=&quot;https://www.youtube.com/watch?v=im89-TJoks4&amp;amp;ab_channel=%E5%BB%9A%E5%AD%90%E8%AA%AA%E8%8F%9C&quot;&gt;视频&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;有了鸡和好的花椒油、辣椒油，这道菜就基本成功一半了。剩下所需的材料都是家常的，并且可以按个人口味调整。做时主要分两个步骤：1. 处理鸡 2. 准备调料和小料。乐山师傅推荐煮鸡要用姜、葱、花椒、黄酒（料酒）以及白芷，白芷我至今没找到新加坡哪里有卖，所以就用了一片香叶代替，味道也不错。用几分钟把鸡煮熟后注意要让它在锅里继续闷个二十来分钟，与此同时准备调料和小料，别嫌麻烦，每一样都很重要：姜水，蒜水，味精，盐，酱油，糖，鸡汤原汤，花椒粉，花椒油，辣椒油，小葱花，大葱丁，川菜就是这样吃着才香的嘛。等调料和小料都准备好了，把鸡肉拿出来在冰水里面拔一下，让皮更紧致，沥水，改刀切成方便吃的小块。最后把调料跟鸡肉拌在一起，出菜前撒上小葱花和花椒粉，就可以大快朵颐了！记得配上香喷喷的饭。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/recipes/numbing_spicy_chicken.jpg&quot; alt=&quot;numbing_spicy_chicken&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Expectation-maximization algorithm, explained</title>
   <link href="http://localhost:4000/data/2020/10/20/EM-algorithm-explained.html"/>
   <updated>2020-10-20T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/10/20/EM-algorithm-explained</id>
   <content type="html">&lt;p&gt;&lt;em&gt;A comprehensive guide to the EM algorithm with intuitions, examples, Python implementation, and maths&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yes! Let’s talk about the expectation-maximization algorithm (EM, for short). If you are in the data science “bubble”, you’ve probably come across EM at some point in time and wondered: What is EM, and do I need to know it?&lt;/p&gt;

&lt;p&gt;It’s the algorithm that solves &lt;strong&gt;Gaussian mixture models&lt;/strong&gt;, a popular clustering approach. The Baum-Welch algorithm essential to &lt;strong&gt;hidden Markov models&lt;/strong&gt; is a special type of EM. It works with both big and small data; it thrives when there is missing information while other techniques fail. It’s such a classic, powerful, and versatile statistical learning technique that it’s taught in almost all computational statistics classes. After reading this article, you could gain a strong understanding of the EM algorithm and know when and how to use it.&lt;/p&gt;

&lt;p&gt;We start with two motivating examples (unsupervised learning and evolution). Next, we see what EM is in its general form. We jump back in action and use EM to solve the two examples. We then explain both intuitively and mathematically why EM works like a charm. Lastly, a summary of this article and some further topics are presented.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivating-examples-why-do-we-care&quot; id=&quot;markdown-toc-motivating-examples-why-do-we-care&quot;&gt;Motivating examples: Why do we care?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#general-framework-what-is-em&quot; id=&quot;markdown-toc-general-framework-what-is-em&quot;&gt;General framework: What is EM?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#em-in-action-does-it-really-work&quot; id=&quot;markdown-toc-em-in-action-does-it-really-work&quot;&gt;EM in action: Does it really work?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#explained-why-does-it-work&quot; id=&quot;markdown-toc-explained-why-does-it-work&quot;&gt;Explained: Why does it work?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#further-topics&quot; id=&quot;markdown-toc-further-topics&quot;&gt;Further topics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;motivating-examples-why-do-we-care&quot;&gt;Motivating examples: Why do we care?&lt;/h2&gt;

&lt;p&gt;Maybe you already know why you want to use EM, or maybe you don’t. Either way, let me use two motivating examples to set the stage for EM. These are quite lengthy, I know, but they perfectly highlight the common feature of the problems that EM is best at solving: the presence of &lt;strong&gt;missing information&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;unsupervised-learning-solving-gaussian-mixture-model-for-clustering&quot;&gt;Unsupervised learning: Solving Gaussian mixture model for clustering&lt;/h3&gt;

&lt;p&gt;Suppose you have a data set with $n$ number of data points. It could be a group of customers visiting your website (customer profiling) or an image with different objects (image segmentation). Clustering is the task of finding out $k$ natural groups for your data when you don’t know (or don’t specify) the real grouping. This is an unsupervised learning problem because no ground-truth labels are used.&lt;/p&gt;

&lt;p&gt;Such clustering problem can be tackled by several types of algorithms, e.g., combinatorial type such as k-means or hierarchical type such as Ward’s hierarchical clustering. However, if you believe that your data could be better modeled as a mixture of normal distributions, you would go for Gaussian mixture model (GMM).&lt;/p&gt;

&lt;p&gt;The underlying idea of GMM is that you assume there’s a data generating mechanism behind your data. This mechanism first chooses one of the $k$ normal distributions (with a certain probability) and then delivers a sample from that distribution. Therefore, once you have estimated each distribution’s parameters, you could easily cluster each data point by selecting the one that gives the highest likelihood.&lt;/p&gt;

&lt;p&gt;
  &lt;img width=&quot;1024&quot; alt=&quot;ClusterAnalysis Mouse&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/1024px-ClusterAnalysis_Mouse.svg.png&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FIGURE 1.&lt;/strong&gt;&lt;i&gt; An &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:ClusterAnalysis_Mouse.svg&quot;&gt;example&lt;/a&gt; of mixture of Gaussian data and clustering using k-means and GMM (solved by EM).&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;However, estimating the parameters is not a simple task since we do not know which distribution generated which points (&lt;strong&gt;missing information&lt;/strong&gt;). EM is an algorithm that can help us solve exactly this problem. This is why EM is the underlying solver in scikit-learn’s GMM &lt;a href=&quot;https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture&quot;&gt;implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;population-genetics-estimating-moth-allele-frequencies-to-observe-natural-selection&quot;&gt;Population genetics: Estimating moth allele frequencies to observe natural selection&lt;/h3&gt;

&lt;p&gt;Have you heard the phrase “industrial melanism” before? Biologists coined the term in the 19th century to describe how animals change their skin color due to the massive industrialization in the cities. They observed that previously rare dark peppered moths started to dominate the population in coal-fueled industrialized towns. Scientists at the time were surprised and fascinated by this observation. Subsequent research suggests that the industrialized cities tend to have darker tree barks that disguise darker moths better than the light ones. You can play this peppered moth &lt;a href=&quot;https://askabiologist.asu.edu/peppered-moths-game/play.html&quot;&gt;game&lt;/a&gt; to understand the phenomenon better.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/intro-to-EM/dark_light_moth.png&quot; alt=&quot;pepper_moths&quot; style=&quot;zoom: 75%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FIGURE 2.&lt;/strong&gt; &lt;i&gt;Dark (top) and light (bottom) peppered moth. Image by Jerzy Strzelecki via Wikimedia Commons&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;As a result, dark moths survive the predation better and pass on their genes, giving rise to a predominantly dark peppered moth population.  To prove their natural selection theory, scientists first need to estimate the percentage of black-producing and light-producing genes/alleles present in the moth population. The gene responsible for the moth’s color has three types of alleles: C, I, and T. Genotypes &lt;strong&gt;C&lt;/strong&gt;C, &lt;strong&gt;C&lt;/strong&gt;I, and &lt;strong&gt;C&lt;/strong&gt;T produce dark peppered moth (&lt;em&gt;Carbonaria&lt;/em&gt;); &lt;strong&gt;T&lt;/strong&gt;T produces light peppered moth (&lt;em&gt;Typica&lt;/em&gt;); &lt;strong&gt;I&lt;/strong&gt;I and &lt;strong&gt;I&lt;/strong&gt;T produce moths with intermediate color (&lt;em&gt;Insularia&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Here’s a hand-drawn graph that shows the &lt;strong&gt;observed&lt;/strong&gt; and &lt;strong&gt;missing&lt;/strong&gt; information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/intro-to-EM/moth_relationship.jpg&quot; alt=&quot;moth_relationship&quot; /&gt;
&lt;strong&gt;FIGURE 3.&lt;/strong&gt;&lt;i&gt; Relationship between peppered moth alleles, genotypes, and phenotypes. We observed phenotypes, but wish to estimate percentages of alleles in the population. Image by author&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;We wish to know the percentages of C, I, and T in the population. However, we can only observe the number of &lt;em&gt;Carbonaria&lt;/em&gt;, &lt;em&gt;Typica&lt;/em&gt;, and &lt;em&gt;Insularia&lt;/em&gt; moths by capturing them, but not the genotypes (&lt;strong&gt;missing information&lt;/strong&gt;). The fact that we do not observe the genotypes and multiple genotypes produce the same subspecies make the calculation of the allele frequencies difficult. This is where EM comes in to play. With EM, we can easily estimate the allele frequencies and provide concrete evidence for the micro-evolution happening on a human time scale due to environmental pollution.&lt;/p&gt;

&lt;p&gt;How does EM tackle the GMM problem and the peppered moth problem in the presence of missing information? We will illustrate these in the later section. But first, let’s see what EM is really about.&lt;/p&gt;

&lt;h2 id=&quot;general-framework-what-is-em&quot;&gt;General framework: What is EM?&lt;/h2&gt;

&lt;p&gt;At this point, you must be thinking (I hope): All these examples are wonderful, but what is really EM? Let’s dive into it.&lt;/p&gt;

&lt;p&gt;EM algorithm is an iterative optimization method that finds the maximum likelihood estimate (MLE) of parameters in problems where hidden/missing/latent variables are present. It was first introduced in its full generality by Dempster, Laird, and Rubin (1977) in their famous paper&lt;sup id=&quot;fnref:Dempster&quot;&gt;&lt;a href=&quot;#fn:Dempster&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; (currently 62k citations). It has been widely used for its easy implementation, numerical stability, and robust empirical performance.&lt;/p&gt;

&lt;p&gt;Let’s set up the EM for a general problem and introduce some notations. Suppose that $Y$ are our observed variables, $X$ are hidden variables, and we say that the pair $(X, Y)$ is the complete data. We also denote any unknown parameter of interest as $\theta \in \Theta$. The objective of most parameter estimation problems is to find the most probable $\theta$ given our model and data, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\theta = \arg\max_{\theta \in \Theta} p_\theta(\mathbf{y}) \,,
\end{equation}&lt;/script&gt;

&lt;p&gt;where  $p_\theta(\mathbf{y})$ is the incomplete-data likelihood. Using the law of &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_total_probability&quot;&gt;total probability&lt;/a&gt;, we can also express the incomplete-data likelihood as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_\theta(\mathbf{y}) = \int p_\theta(\mathbf{x}, \mathbf{y}) d\mathbf{x} \,,&lt;/script&gt;

&lt;p&gt;where $p_\theta(\mathbf{x}, \mathbf{y})$ is known as the complete-data likelihood.&lt;/p&gt;

&lt;p&gt;What’s with all these complete- and incomplete-data likelihoods? In many problems, the maximization of the incomplete-data likelihood $p_\theta(\mathbf{y})$ is difficult because of the missing information. On the other hand, it’s often easier to work with complete-data likelihood. EM algorithm is designed to take advantage of this observation. It iterates between an &lt;strong&gt;expectation step&lt;/strong&gt; (E-step) and a &lt;strong&gt;maximization step&lt;/strong&gt; (M-step) to find the MLE.&lt;/p&gt;

&lt;p&gt;Assuming $\theta^{(n)}$ is the estimate obtained at the $n$th iteration, the algorithm iterates between the two steps as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;E-step&lt;/strong&gt;: define 
$Q(\theta | \theta^{(n)})$ as the conditional expectation of the complete-data log-likelihood w.r.t. the hidden variables, given observed data and current parameter estimate, i.e.,&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\label{eqn:e_step}
Q(\theta | \theta^{(n)}) = \mathbb{E}_{X|\mathbf{y}, \theta^{(n)}}\left[\ln p_\theta(\mathbf{x}, \mathbf{y})\right] \,.
\end{align}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;M-step&lt;/strong&gt;: find a new $\theta$ that maximizes the above expectation and set it to $\theta^{(n+1)}$, i.e.,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
  \label{eqn:m_step}
  \theta^{(n+1)} = \arg\max_{\theta \in \Theta} Q(\theta | \theta^{(n)}) \,.
  \end{align}&lt;/script&gt;

&lt;p&gt;The above definitions might seem hard-to-grasp at first. Some intuitive explanation might help:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;E-step&lt;/strong&gt;: This step is asking, given our observed data $\mathbf{y}$ and current parameter estimate $\theta^{(n)}$, what are the probabilities of different $X$? Also, under these probable $X$, what are the corresponding log-likelihoods?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;M-step&lt;/strong&gt;: Here we ask, under these probable $X$, what is the value of $\theta$ that gives us the maximum expected log-likelihood?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The algorithm iterates between these two steps until a stopping criterion is reached, e.g., when either the Q function or the parameter estimate has converged. The entire process can be illustrated in the following flowchart.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/intro-to-EM/em_flowchart.png&quot; alt=&quot;em_flowchart&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FIGURE 4.&lt;/strong&gt; &lt;i&gt;The EM algorithm iterates between E-step and M-step to obtain MLEs and stops when the estimates have converged. Image by author&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;That’s it! With two equations and a bunch of iterations, you have just unlocked one of the most elegant statistical inference techniques!&lt;/p&gt;

&lt;h2 id=&quot;em-in-action-does-it-really-work&quot;&gt;EM in action: Does it really work?&lt;/h2&gt;

&lt;p&gt;What we’ve seen above is the general framework of EM, not the actual implementation of it. In this section, we will see step-by-step just how EM is implemented to solve the two previously mentioned examples. After verifying that EM does work for these problems, we then see intuitively and mathematically why it works in the next section.&lt;/p&gt;

&lt;h3 id=&quot;solving-gmm-for-clustering&quot;&gt;Solving GMM for clustering&lt;/h3&gt;

&lt;p&gt;Suppose we have some data and would like to model the density of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/intro-to-EM/mixture_example.png&quot; alt=&quot;mixture_example&quot; /&gt;
&lt;strong&gt;FIGURE 5.&lt;/strong&gt; &lt;i&gt;400 points generated as a mixture of four different normal distributions. Image by author&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Are you able to see the different underlying distributions? Apparently, these data come from more than one distribution. Thus a single normal distribution would not be appropriate, and we use a mixture approach. In general, GMM-based clustering is the task of clustering $y_1, \dots, y_n$ data points into $k$ groups. We let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{ik}=\left\{\begin{array}{l}
1 \quad \text{if $y_i$ is in group $k$}\\
0 \quad \text{otherwise}
\end{array}\right.&lt;/script&gt;

&lt;p&gt;Thus, $x_i$ is the one-hot coding of data $y_i$, e.g., $x_i = [0, 0, 1]$ if $k = 3$ and $y_i$ is from group 3. In this case, the collection of data points $\mathbf{y}$ is the incomplete data, and $(\mathbf{x}, \mathbf{y})$ is the augmented complete data. We further assume that each group follows a normal distribution, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i \mid x_{ik} = 1 \sim N(\mu_k, \Sigma_k) \,.&lt;/script&gt;

&lt;p&gt;Following the usual mixture Gaussian model set up, a new point is generated from the $k$th group with probability $P(x_{ik} = 1) = w_k$ and $\sum_{i=1}^{k} w_i = 1$. Suppose we are only working with the incomplete data $\mathbf{y}$. The likelihood of one data point under a GMM is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align} p(y_i) = \sum_{j=1}^k w_j \phi(y_i; \mu_j, \Sigma_j) \,, \end{align}&lt;/script&gt;

&lt;p&gt;where $\phi(\cdot; \mu, \Sigma)$ is the PDF of a normal distribution with mean $\mu$ and variance-covariance $\Sigma$. The total log-likelihood of $n$ points is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\ln p(\mathbf{y}) = \sum_{i=1}^{n} \ln \sum_{j=1}^k w_j \phi(y_i; \mu_j, \Sigma_j) \,.
\end{align}&lt;/script&gt;

&lt;p&gt;In our problem, we are trying to estimate three groups of parameters: the group mixing probabilities ($\mathbf{w}$) and each distribution’s mean and covariance matrix ($\boldsymbol{\mu}, \boldsymbol{\Sigma}$). The usual approach to parameter estimation is by maximizing the above total log-likelihood function w.r.t. each parameter (MLE). However, this is difficult to do due to the summation inside the $\log$ term.&lt;/p&gt;

&lt;h4 id=&quot;expectation-step&quot;&gt;Expectation step&lt;/h4&gt;

&lt;p&gt;Let’s use the EM approach instead! Remember that we first need to define the Q function in the E-step, which is the conditional expectation of the complete-data log-likelihood. Since $(\mathbf{x}, \mathbf{y})$ is the complete data, the corresponding likelihood of one data point is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_i, y_i) = \Pi_{j=1}^k \{w_j \phi(y_i; \mu_j, \Sigma_j)\}^{x_{ij}} \,,&lt;/script&gt;

&lt;p&gt;and only the term with $x_{ij} = 1$ is active. Hence, our total complete-data log-likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln p(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n}\sum_{j=1}^k x_{ij}\ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\} \,.&lt;/script&gt;

&lt;p&gt;Denote $\theta$ as the collection of unknown parameters $(\mathbf{w}, \boldsymbol{\mu}, \boldsymbol{\Sigma})$, and $\theta^{(n)}$ as the estimates from the last iteration. Following the E-step formula in ($\ref{eqn:e_step}$), we obtain the Q function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\label{eqn:gmm_e_step}
Q(\theta | \theta^{(n)}) = \sum_{i=1}^{n}\sum_{j=1}^k z_{ij}^{(n)} \ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\}  
\end{align}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_{ij}^{(n)} = \frac{\phi(y_i; \mu_j^{(n)}, \Sigma_j^{(n)}) w_j^{(n)}}{\sum_{l=1}^k \phi(y_i; \mu_l^{(n)}, \Sigma_l^{(n)}) w_l^{(n)}} \,.&lt;/script&gt;

&lt;p&gt;Here $z_{ij}^{(n)}$ is the probability that data $y_i$ is in class $j$ with the current parameter estimates $\theta^{(n)}$. This probability is also called responsibility in some texts. It means the responsibility of each class to this data point. It’s also a constant given the observed data and $\theta^{(n)}$.&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Click here for the derivation of the Q function:&lt;/summary&gt;
$$
  \begin{align*}
Q(\theta | \theta^{(n)}) &amp;amp;= \mathbb{E}_{X|\mathbf{y}, \theta^{(n)}}\left[\ln p_\theta(\mathbf{x}, \mathbf{y})\right] \\
&amp;amp;= \mathbb{E}_{X|\mathbf{y}, \theta^{(n)}}\left[\sum_{i=1}^{n}\sum_{j=1}^k x_{ij}\ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\}\right] \\
&amp;amp;= \sum_{i=1}^{n}\sum_{j=1}^k \underbrace{\mathbb{E}_{X|\mathbf{y}, \theta^{(n)}}[x_{ij}]}_{\text{Expectation taken w.r.t. $X$}} \ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\} \\
&amp;amp;= \sum_{i=1}^{n}\sum_{j=1}^k p_{\theta^{(n)}}[x_{ij} = 1 \mid \mathbf{y}] \ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\} \\
&amp;amp;= \sum_{i=1}^{n}\sum_{j=1}^k
\underbrace{
\frac{p_{\theta^{(n)}}(y_{i} \mid x_{i} = j) p_{\theta^{(n)}}(x_i = j)}  {\sum_{l=1}^k{p_{\theta^{(n)}}(y_{i} \mid x_{i} = l) p_{\theta^{(n)}}(x_i = l)}}
}_{\text{Baye's rule}}
\ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\}  \\
&amp;amp;= \sum_{i=1}^{n}\sum_{j=1}^k
\underbrace{
\frac{\phi(y_i; \mu_j^{(n)}, \Sigma_j^{(n)}) w_j^{(n)}}{\sum_{l=1}^k \phi(y_i; \mu_l^{(n)}, \Sigma_l^{(n)}) w_l^{(n)}}
}_{\text{Substitue in current estimates}}
\ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\} \\
&amp;amp;= \sum_{i=1}^{n}\sum_{j=1}^k z_{ij}^{(n)} \ln \{w_j \phi(y_i; \mu_j, \Sigma_j)\}  
\end{align*}
$$
&lt;/details&gt;

&lt;h4 id=&quot;maximization-step&quot;&gt;Maximization step&lt;/h4&gt;

&lt;p&gt;Recall that the EM algorithm proceeds by iterating between the E-step and the M-step. We have obtained the latest iteration’s Q function in the E-step above. Next, we move on to the M-step and find a new $\theta$ that maximizes the Q function in ($\ref{eqn:gmm_e_step}$), i.e., we find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{(n+1)} = \arg\max_{\theta \in \Theta} Q(\theta | \theta^{(n)}) \,.&lt;/script&gt;

&lt;p&gt;A closer look at the obtained Q function reveals that it’s actually a weighted normal distribution MLE problem. That means, the new $\theta$ has closed-form formulas and can be verified easily using differentiation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
w_j^{(n+1)} &amp;= \frac{1}{n} \sum_{i=1}^{n} z_{ij}^{(n)} &amp;&amp; \text{New mixing probabilities}\\
\mu_j^{(n+1)} &amp;= \frac{\sum_{i=1}^{n} z_{ij}^{(n)} y_{i}}{\sum_{i=1}^{n} z_{ij}^{(n)}} &amp;&amp;\text{New means}\\
\Sigma_j^{(n+1)} &amp;= \frac{\sum_{i=1}^{n} z_{ij}^{(n)} (y_{i} - \mu_j^{(n+1)})(y_i - \mu_j^{(n+1)})^T}{\sum_{i}^{n} z_{ij}^{(n)}} &amp;&amp;\text{New var-cov matrices}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;for $j = 1, \dots, k$.&lt;/p&gt;

&lt;h4 id=&quot;how-does-it-perform&quot;&gt;How does it perform?&lt;/h4&gt;

&lt;p&gt;We go back to the opening problem in this section. I simulated 400 points using four different normal distributions. FIGURE 5 is what we see if we do not know the underlying true groupings. We run the EM procedure as derived above and set the algorithm to stop when the log-likelihood does not change anymore.&lt;/p&gt;

&lt;p&gt;In the end, we found the mixing probabilities and all four group’s means and covariance matrices. FIGURE 6 below shows the density contours of each distribution found by EM superimposed on the data, which are now color-coded by their ground-truth groupings. Both the locations (means) and the scales (covariances) of the four underlying normal distributions are correctly identified. Unlike k-means, EM gives us both the clustering of the data and the generative model (GMM) behind them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/intro-to-EM/mixture_example_result.png&quot; alt=&quot;mixture_example_result&quot; /&gt;
&lt;strong&gt;FIGURE 6.&lt;/strong&gt; &lt;i&gt; Density contours superimposed on samples from four different normal distributions. Image by author&lt;/i&gt;&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;Click here for the GMM-EM implementation, credit to &lt;a href=&quot;http://people.duke.edu/~ccc14/sta-663-2016/14_ExpectationMaximization.html#&quot;&gt;Cliburn Chan&lt;/a&gt;:&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;em_gmm_vect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    EM algorithm implementation for solving Gaussian mixture model inference problem.

    Parameter:
    xs: n-by-p, observed data
    pis: 1-by-k, group mixing probabilities
    mus: k-by-p, mean vector of k groups
    sigmas: k-by-p-by-p, variance-covariance matrix of k groups
    
    Return:
    ll_new: maximum log-likelihood found
    pis, mus, sigmas: parameter results
    &quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;ll_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# E-step
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mvn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# M-step
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;pis_hist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# update complete log likelihood
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mvn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ll_old&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ll_old&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ll_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;                                    
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script to run the above experiment:&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mvn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.core.umath_tests&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matrix_multiply&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mm&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;123&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create data set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_mus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; 
                    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; 
                    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; 
                    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;_pis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_mus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# visualize data without labels
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;# initial guesses for parameters
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# normalize
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# run EM
# remember to include em_gmm_vect function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ll2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;em_gmm_vect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# visualize results
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intervals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intervals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pis2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mus2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmas2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mvn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intervals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intervals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_aspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'equal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h3 id=&quot;estimating-allele-frequencies&quot;&gt;Estimating allele frequencies&lt;/h3&gt;

&lt;p&gt;We return to the population genetics problem mentioned earlier. Suppose we captured $n$ moths and of which there are three different types: &lt;em&gt;Carbonaria&lt;/em&gt;, &lt;em&gt;Typica&lt;/em&gt;, and &lt;em&gt;Insularia&lt;/em&gt;. However, we do not know the genotype of each moth except for &lt;em&gt;Typica&lt;/em&gt; moths, see FIGURE 3 above. We wish to estimate the population allele frequencies. Let’s speak in EM terms. Here’s what we know:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Observed:
    &lt;ul&gt;
      &lt;li&gt;$X = (n_{\mathrm{Car}}, n_{\mathrm{Typ}}, n_{\mathrm{Ins}})$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unobserved: the number of different genotypes
    &lt;ul&gt;
      &lt;li&gt;$Y = (n_{\mathrm{CC}}, n_{\mathrm{CI}}, n_{\mathrm{CT}}, n_{\mathrm{II}}, n_{\mathrm{IT}}, n_{\mathrm{TT}})$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;But we do know the relationship between them:
    &lt;ul&gt;
      &lt;li&gt;$n_{\mathrm{Car}} = n_{\mathrm{CC}} + n_{\mathrm{CI}} + n_{\mathrm{CT}}$&lt;/li&gt;
      &lt;li&gt;$n_{\mathrm{Typ}} = n_{\mathrm{TT}}$&lt;/li&gt;
      &lt;li&gt;$n_{\mathrm{Ins}} = n_{\mathrm{II}} + n_{\mathrm{IT}}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameter of interest: allele frequencies
    &lt;ul&gt;
      &lt;li&gt;$\theta = (p_\mathrm{C}, p_\mathrm{I}, p_\mathrm{T})$&lt;/li&gt;
      &lt;li&gt;and we know $p_\mathrm{C} + p_\mathrm{I} + p_\mathrm{T} = 1$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s another important modeling principle that we need to use: the Hardy–Weinberg principle, which says that the genotype frequency is the product of the corresponding allele frequency or double that when the two alleles are different. That is, we can expect the genotype frequencies of $n_{\mathrm{CC}}, n_{\mathrm{CI}}, n_{\mathrm{CT}}, n_{\mathrm{II}}, n_{\mathrm{IT}}, n_{\mathrm{TT}}$ to be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\mathrm{C}}^2, 2p_{\mathrm{C}}p_{\mathrm{I}}, 2p_{\mathrm{C}}p_{\mathrm{T}}, p_{\mathrm{I}}^2, 2p_{\mathrm{I}}p_{\mathrm{T}}, p_{\mathrm{T}}^2 \,.&lt;/script&gt;

&lt;p&gt;Good! Now we are ready to plug in the EM framework. What’s the first step?&lt;/p&gt;

&lt;h4 id=&quot;expectation-step-1&quot;&gt;Expectation step&lt;/h4&gt;
&lt;p&gt;Just like the GMM case, we first need to figure out the complete-data likelihood. Notice that this is actually a multinomial distribution problem. We have a population of moths, the chance of capturing a moth of genotype $\mathrm{CC}$ is $p_{\mathrm{C}}^2$, similarly for the other genotypes. Therefore, the complete-data 
likelihood is just the multinomial &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution#Probability_mass_function&quot;&gt;distribution PDF&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(\mathbf{x}, \mathbf{y}) &amp;= \mathrm{Pr}(N_{\mathrm{CC}} = n_{\mathrm{CC}}, N_{\mathrm{CI}} = n_{\mathrm{CI}}, \dots, N_{\mathrm{TT}} = n_{\mathrm{TT}}) \\
&amp;= \left(\begin{array}{cccc}
&amp;n&amp;&amp; \\
n_{\mathrm{CC}} &amp; n_{\mathrm{CI}} &amp; \dots &amp; n_{\mathrm{TT}}
\end{array} \right) (p_{\mathrm{C}}^2)^{n_{\mathrm{CC}}} (2p_\mathrm{C} p_\mathrm{I})^{n_{\mathrm{CI}}} \dots (p_{\mathrm{T}}^2)^{n_{\mathrm{TT}}} \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;And the complete-data log-likelihood can be written in the following decomposed form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p_{\theta}(\mathbf{x}, \mathbf{y}) &amp;= n_{\mathrm{CC}} \log \left\{p_{\mathrm{C}}^{2}\right\}+n_{\mathrm{CI}} \log \left\{2 p_{\mathrm{C}} p_{\mathrm{I}}\right\}+n_{\mathrm{CT}} \log \left\{2 p_{\mathrm{C}} p_{\mathrm{T}}\right\} \\
&amp;+n_{\mathrm{II}} \log \left\{p_{\mathrm{I}}^{2}\right\}+n_{\mathrm{IT}} \log \left\{2 p_{\mathrm{I}} p_{\mathrm{T}}\right\}+n_{\mathrm{TT}} \log \left\{p_{\mathrm{T}}^{2}\right\} \\
&amp;+\log \left(\begin{array}{llllll}
&amp; &amp; n &amp; &amp; \\
n_{\mathrm{CC}} &amp; n_{\mathrm{CI}} &amp; n_{\mathrm{CT}} &amp; n_{\mathrm{II}} &amp; n_{\mathrm{IT}} &amp; n_{\mathrm{TT}}
\end{array}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember that the E-step is taking a conditional expectation of the above likelihood w.r.t. the unobserved data $Y$, given the latest iteration’s parameter estimates $\theta^{(n)}$.  The Q function is found to be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q\left(\theta \mid \theta^{(n)}\right) &amp;= n_{\mathrm{CC}}^{(n)} \log \left\{p_{\mathrm{C}}^{2}\right\}+n_{\mathrm{CI}}^{(n)} \log \left\{2 p_{\mathrm{C}} p_{\mathrm{I}}\right\} \\
&amp;+n_{\mathrm{CT}}^{(n)} \log \left\{2 p_{\mathrm{C}} p_{\mathrm{T}}\right\}+n_{\mathrm{II}}^{(n)} \log \left\{p_{\mathrm{I}}^{2}\right\} \\
&amp;+n_{\mathrm{IT}}^{(n)} \log \left\{2 p_{\mathrm{I}} p_{\mathrm{T}}\right\}+n_{\mathrm{TT}} \log \left\{p_{\mathrm{T}}^{2}\right\}+k\left(n_{\mathrm{C}}, n_{\mathrm{I}}, n_{\mathrm{T}}, \theta^{(n)}\right) \,,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $n_{\mathrm{CC}}^{(n)}$ is expected number of $\mathrm{CC}$ type moth given the current allele frequency estimates, and similarly for the other types. $k(\cdot)$ is a function that does not involve $\theta$.&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Click here for the derivation of the Q function:&lt;/summary&gt;
&lt;div&gt;
    &lt;p&gt;Note that the expectation in the Q function is taken w.r.t. the unobserved variables, i.e., phenotype counts. Therefore, we just need to compute the expectation of $n_{\mathrm{CC}}, \dots, n_{\mathrm{TT}}$ since they are unobserved. Also notice that, given the current allele frequency estimates, the phenotype counts are also multinomial random variables. For example, the three phenotype counts for the &lt;em&gt;Carbonaria&lt;/em&gt; type have three-cell multinomial distribution with count parameter $n_{\mathrm{Car}}$ and probabilities proportional to $p_{\mathrm{C}}^{2}, 2 p_{\mathrm{C}} p_{\mathrm{I}}, 2 p_{\mathrm{C}} p_{\mathrm{T}}$.&lt;/p&gt;

    &lt;p&gt;Therefore, we can obtain the conditional expectation of all phenotype counts by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
n_{\mathrm{CC}}^{(n)} &amp;= n_{\mathrm{Car}} \frac{\left(p_{\mathrm{C}}^{(n)}\right)^{2}}{\left(p_{\mathrm{C}}^{(n)}\right)^{2}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{I}}^{(n)}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{T}}^{(n)}} \\
n_{\mathrm{CI}}^{(n)} &amp;=n_{\mathrm{Car}} \frac{2 p_{\mathrm{C}}^{(n)} p_{\mathrm{I}}^{(n)}}{\left(p_{\mathrm{C}}^{(n)}\right)^{2}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{I}}^{(n)}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{T}}^{(n)}} \\
n_{\mathrm{CT}}^{(n)} &amp;= n_{\mathrm{Car}} \frac{2  p_{\mathrm{C}}^{(n)} p_{\mathrm{T}}^{(n)}}{\left(p_{\mathrm{C}}^{(n)}\right)^{2}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{I}}^{(n)}+2 p_{\mathrm{C}}^{(n)} p_{\mathrm{T}}^{(n)}} \\
n_{\mathrm{II}}^{(n)} &amp;= n_{\mathrm{Ins}} \frac{\left(p_{\mathrm{I}}^{(n)}\right)^{2}}{\left(p_{\mathrm{I}}^{(n)}\right)^{2}+2 p_{\mathrm{I}}^{(n)} p_{\mathrm{T}}^{(n)}} \\
n_{\mathrm{IT}}^{(n)} &amp;= n_{\mathrm{Ins}}\frac{2  p_{\mathrm{I}}^{(n)} p_{\mathrm{T}}^{(n)}}{\left(p_{\mathrm{I}}^{(n)}\right)^{2}+2 p_{\mathrm{I}}^{(n)} p_{\mathrm{T}}^{(n)}} \,.
\end{align*} %]]&gt;&lt;/script&gt;

    &lt;p&gt;And we know $n_{\mathrm{TT}} = n_{\mathrm{Typ}}$.&lt;/p&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;maximization-step-1&quot;&gt;Maximization step&lt;/h4&gt;

&lt;p&gt;Since we obtained the expected number of each phenotype, e.g. $n_{\mathrm{CC}}^{(n)}, n_{\mathrm{CI}}^{(n)}$, estimating the allele frequencies is easy. Intuitively, the frequency of allele $\mathrm{C}$ is calculated as the ratio between the number of allele $\mathrm{C}$ present in the population and the total number of alleles. This works for the other alleles as well. Therefore, in the M-step, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    p_{\mathrm{C}}^{(n+1)} &amp;= \frac{2 n_{\mathrm{CC}}^{(n)} + n_{\mathrm{CI}}^{(n)} + n_{\mathrm{CT}}^{(n)}}{2n} \\
    p_{\mathrm{I}}^{(n+1)} &amp;= \frac{2 n_{\mathrm{II}}^{(n)} + n_{\mathrm{IT}}^{(n)} + n_{\mathrm{CI}}^{(n)}}{2n} \\
    p_{\mathrm{T}}^{(n+1)} &amp;= \frac{2 n_{\mathrm{TT}}^{(n)} + n_{\mathrm{IT}}^{(n)} + n_{\mathrm{CT}}^{(n)}}{2n} \,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In fact, we could obtain the same M-step formulas by differentiating the Q function and setting them to zero (usual optimization routine).&lt;/p&gt;

&lt;h4 id=&quot;how-does-it-perform-1&quot;&gt;How does it perform?&lt;/h4&gt;

&lt;p&gt;Let’s try solving the peppered moth problem using the above derived EM procedure. Suppose we captured 622 peppered moths. 85 of them are &lt;em&gt;Carbonaria&lt;/em&gt;, 196 of them are &lt;em&gt;Insularia&lt;/em&gt;, and 341 of them are &lt;em&gt;Typica&lt;/em&gt;. We run the EM iterations for 10 steps, FIGURE 7 shows that we obtain converged results in less than five steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/intro-to-EM/peppered_moth_em.png&quot; alt=&quot;peppered_moth_em&quot; /&gt;
&lt;strong&gt;FIGURE 7.&lt;/strong&gt; &lt;i&gt; EM algorithm converges in less than five steps and finds the allele frequencies.  Image by author&lt;/i&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script to run the above experiment:&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;e_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_typ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;CC_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;CI_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;CT_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;II_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;IT_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;C_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CC_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CI_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CT_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;I_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;II_prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IT_prob&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_car&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CC_prob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_car&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CI_prob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_car&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CT_prob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;II_prob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IT_prob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I_prob&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_TT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_typ&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_TT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;m_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_TT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Given observed information
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;85&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;196&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;341&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nT&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Initialize
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Record history for visualization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# E-step
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_TT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# M-step
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_CT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_II&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_IT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_TT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'o--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Carbonaria'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Insularia'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Typica'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h3 id=&quot;what-did-we-learn-from-the-examples&quot;&gt;What did we learn from the examples?&lt;/h3&gt;
&lt;p&gt;Estimating the allele frequencies is difficult because of the missing phenotype information. EM helps us to solve this problem by augmenting the process with exactly the missing information. If we look back at the E-step and M-step, we see that the E-step calculates the most probable phenotype counts given the latest frequency estimates; the M-step then calculates the most probable frequencies given the latest phenotype count estimates. This process is evident in the GMM problem as well: the E-step calculates the class responsibilities for each data given the current class parameter estimates; the M-step then estimates the new class parameters using those responsibilities as the data weights.&lt;/p&gt;

&lt;h2 id=&quot;explained-why-does-it-work&quot;&gt;Explained: Why does it work?&lt;/h2&gt;
&lt;p&gt;Working through the previous two examples, we see clearly that the essence of EM lies in the &lt;strong&gt;E-step/M-step&lt;/strong&gt; iterative process that augments the observed information with the missing information. And we see that it indeed finds the MLEs effectively. But why does this iterative process work? Is EM just a smart hack, or is it well-supported by theory? Let’s find out.&lt;/p&gt;

&lt;h4 id=&quot;intuitive-explanation&quot;&gt;Intuitive explanation&lt;/h4&gt;

&lt;p&gt;We start by gaining an intuitive understanding of why EM works. EM solves the parameter estimation problem by transferring the task of maximizing incomplete-data likelihood to maximizing complete-data likelihood in some small steps.&lt;/p&gt;

&lt;p&gt;Imagine you are hiking up Mt. Fuji 🗻 for the first time. There are nine stations to reach before the summit, but you do not know the route. Luckily, there are hikers coming down from the top, and they can give you a rough direction to the next station. Therefore, here’s what you can do to reach the top: start at the base station and ask people for the direction to the second station; go to the second station and ask the people there for the path to the third station, and so on. At the end of the day (or start of the day, if you are catching sunrise 🌄), there’s a high chance you’ll reach the summit.&lt;/p&gt;

&lt;p&gt;That’s very much what EM does to find the MLEs for problems where we have missing data. Instead of maximizing $\ln p(\mathbf{x})$ (find the route to summit), EM maximizes the Q function and finds the next $\theta$ that also increases $\ln p(\mathbf{x})$ (ask direction to the next station). FIGURE 8 below illustrates this process in two iterations. Note that the G function is just a combination of Q function and a few other terms constant w.r.t. $\theta$. Maximizing G function w.r.t. $\theta$ is equivalent to maximizing Q function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/intro-to-EM/optimization_transfer.png&quot; alt=&quot;optimization_transfer&quot; /&gt;
&lt;strong&gt;FIGURE 8.&lt;/strong&gt; &lt;i&gt; The iterative process of EM illustrated in two steps. As we build and maximize a G function (equivalently, Q function) from the current parameter estimate, we obtain the next parameter estimate. In the process, the incomplete-data log-likelihood is also increased.  Image by author&lt;/i&gt;&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;&lt;b&gt;Mathematical proof:&lt;/b&gt;&lt;/summary&gt;
Here we show why the iterative scheme can find the maximum likelihood estimate of the parameter with mathematical proof. Let $\ell(\theta) = \ln p_\theta(\mathbf{y})$, thus we have 

$$
\ell(\theta) - \ell(\theta^{(n)}) = \ln p_\theta(\mathbf{y}) - \ln p_{\theta^{(n)}}(\mathbf{y}) \,.
$$

We wish to compute an updated $\theta$ such that the above relationship holds above zero. Using $p_\theta(\mathbf{y}) = \int p_\theta(\mathbf{x}, \mathbf{y}) \, \mathrm{d}\mathbf{x}$, we have

$$
\begin{align*} \ell(\theta) - \ell(\theta^{(n)}) &amp;amp;= \ln \int p_\theta(\mathbf{x}, \mathbf{y}) \, \mathrm{d}\mathbf{x} - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
&amp;amp;= \ln \int p_\theta(\mathbf{x}, \mathbf{y}) \frac{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})} \, \mathrm{d}\mathbf{x} 

- \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;amp;= \ln \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}\right] - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;amp;\ge \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y})}\right] - \ln p_{\theta^{(n)}}(\mathbf{y}) \\
  &amp;amp;= \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y} ) p_{\theta^{(n)}}(\mathbf{y})}\right] \\
  &amp;amp;:= \Delta(\theta | \theta^{(n)}) \,.
  \end{align*}
$$

The inequality step follows by Jensen's inequality and the fact that $\ln(\cdot)$ is concave on $[0, \infty]$. The second last step follows since $p_{\theta^{(n)}}(\mathbf{y})$ does not depend on $\mathbf{X}$. Therefore, we have 
$$
\ell(\theta) \ge \ell(\theta^{(n)}) + \Delta(\theta|\theta^{(n)}) \,.
$$
Define 

$$
G(\theta | \theta^{(n)}) := \ell(\theta^{(n)}) + \Delta(\theta|\theta^{(n)}) \,,
$$
then 
$\ell(\theta) \ge G(\theta|\theta^{(n)})$. 
That is, 
$G(\theta|\theta^{(n)})$ 
is upper-bounded by $\ell(\theta)$ for all $\theta \in \Theta$. The equality holds when $\theta = \theta^{(n)}$ since
$$
\begin{align*}
G(\theta^{(n)}|\theta^{(n)}) &amp;amp;= \ell(\theta^{(n)}) + \Delta(\theta^{(n)}|\theta^{(n)}) \\
&amp;amp;= \ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y} ) p_{\theta^{(n)}}(\mathbf{y})}\right] \\
&amp;amp;= \ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}\left[\ln \frac{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x}, \mathbf{y})}\right] \\
&amp;amp;= \ell(\theta^{(n)}) \,.
\end{align*}
$$
Therefore, when computing an updated $\theta$, any increase in 
$G(\theta|\theta^{(n)})$ leads to an increase in $\ell(\theta)$ by at least 
$\Delta(\theta|\theta^{(n)})$. The observation is that, by selecting the $\theta$ that maximizes 
$\Delta(\theta|\theta^{(n)})$, we can achieve the largest increase in $\ell(\theta)$. Formally, we have 
$$
\begin{align*}
\theta^{(n+1)} &amp;amp;= \arg\max_{\theta\in\Theta} G(\theta | \theta^{(n)}) \\
&amp;amp; = \arg\max_{\theta\in\Theta} 
\left\lbrace
\ell(\theta^{(n)}) + \mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}} 
\left[
\ln \frac{p_\theta(\mathbf{x}, \mathbf{y})}{p_{\theta^{(n)}}(\mathbf{x} | \mathbf{y}) p_{\theta^{(n)}}(\mathbf{y})}
\right]
\right\rbrace\\
&amp;amp; = \underbrace{\arg\max_{\theta\in\Theta}}_{\text{Maximization}} \underbrace{\mathbb{E}_{\mathbf{X} | \mathbf{y} , \theta^{(n)}}}_{\text{Expectation}}[\ln p_\theta(\mathbf{x}, \mathbf{y})] \\
&amp;amp; = \arg\max_{\theta\in\Theta} Q(\theta | \theta^{(n)}) \,,
\end{align*}
$$

where the second last step follows by dropping terms constant with respect to $\theta$. Thus, the E-step and M-step are made apparent in the formulation. Also, by maximizing 
$G(\theta | \theta^{(n)})$ instead of $\ell(\theta)$, we have made use of the information of hidden variables $\mathbf{X}$ in the complete-data likelihood. 

&lt;/details&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this article, we see that EM converts a difficult problem with missing information to an easy problem through the optimization transfer framework. We also see EM in action by solving step-by-step two problems with Python implementation (Gaussian mixture clustering and peppered moth population genetics). More importantly, we show that EM is not just a smart hack but has solid mathematical groundings on why it would work.&lt;/p&gt;

&lt;p&gt;I hope this introductory article has helped you a little in getting to know the EM algorithm. From here, if you are interested, consider exploring the following topics.&lt;/p&gt;

&lt;h2 id=&quot;further-topics&quot;&gt;Further topics&lt;/h2&gt;
&lt;p&gt;Digging deeper, the first question you might ask is: So, is EM perfect? Of course, it’s not. Sometimes, the Q function is difficult to obtain analytically. We could use Monte Carlo techniques to estimate the Q function, e.g., check out Monte Carlo &lt;a href=&quot;https://amstat.tandfonline.com/doi/abs/10.1198/106186001317115045&quot;&gt;EM&lt;/a&gt;. Sometimes, even with complete-data information, the Q function is still difficult to maximize. We could consider alternative maximizing techniques, e.g., see expectation conditional maximization (&lt;a href=&quot;https://academic.oup.com/biomet/article-abstract/80/2/267/251605&quot;&gt;ECM&lt;/a&gt;). Another disadvantage of EM is that it provides us with only point estimates. In case we want to know the uncertainty in these estimates, we would need to conduct variance estimation through other techniques, e.g., Louis’s method, supplemental EM, or bootstrapping.&lt;/p&gt;

&lt;p&gt;Thanks for reading! Please consider leaving feedback for me below.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 class=&quot;no_toc&quot; id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Dempster&quot;&gt;
      &lt;p&gt;Dempster, A. P., Laird, N. M., &amp;amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. &lt;em&gt;Journal of the Royal Statistical Society: Series B (Methodological)&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(1), 1-22. &lt;a href=&quot;#fnref:Dempster&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Domain Expertise: What deep learning needs for better COVID-19 detection</title>
   <link href="http://localhost:4000/data/2020/09/27/detecting-covid19-using-cnn.html"/>
   <updated>2020-09-27T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/27/detecting-covid19-using-cnn</id>
   <content type="html">&lt;p&gt;By now, you’ve probably seen a few, if not many, &lt;a href=&quot;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=detecting+covid-19+using+neural+network&amp;amp;btnG=&quot;&gt;articles&lt;/a&gt; on how deep learning could help detect COVID-19. In particular, convolutional neural networks (CNNs) have been studied as a faster and cheaper alternative to the gold-standard PCR test by just analyzing the patient’s computed tomography (CT) scan. It’s not surprising since CNN is excellent at image recognition; many places have CT scanners rather than COVID-19 testing kits (at least initially).&lt;/p&gt;

&lt;p&gt;Despite its success in image recognition tasks such as the ImageNet challenge, can CNN really help doctors detect COVID-19? If it can, how accurately can it do so? It’s well known that CT scans are sensitive but not &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7227176/&quot;&gt;specific&lt;/a&gt; to COVID-19. That is, COVID-19 almost always produces abnormal lung patterns visible from CT scans. However, other pneumonia can create the same abnormal patterns. Can the powerful and sometimes magical CNN tackle this ambiguity issue?&lt;/p&gt;

&lt;p&gt;We had a chance to answer these questions ourselves (with my colleague &lt;a href=&quot;https://www.linkedin.com/in/yuchen-shi-2830ba158/?originalSubdomain=sg&quot;&gt;Yuchen&lt;/a&gt; and advisor &lt;a href=&quot;https://www.eng.nus.edu.sg/isem/staff/chen-nan/&quot;&gt;A/P Chen&lt;/a&gt;). I’ll walk you through a COVID-19 classifier that we’ve built as an entry to 2020 INFORMS QSR Data &lt;a href=&quot;https://connect.informs.org/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=f404f7b8-fcd6-75d5-f7a7-d262eab132e7&quot;&gt;Challenge&lt;/a&gt;. If you are not familiar with CNN or would like a refresher on the key features of CNNs, I highly recommend reading &lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/09/24/intro-to-cnn.html&quot;&gt;Convolutional Neural Network: How is it different from the other networks?&lt;/a&gt; first. Also, if you’d like to get hands-on, you can get all the code and data from my Github &lt;a href=&quot;https://github.com/YangXiaozhou/CNN-COVID-19-classification-using-chest-CT-scan&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Transfer learning using pre-trained CNN can achieve a really strong baseline performance on COVID-19 classification (85% accuracy).&lt;/li&gt;
  &lt;li&gt;However, domain expertise-informed feature engineering and adaptation are required to elevate the CNN (or other ML methods) to a medically convincing level.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;whats-the-challenge&quot;&gt;What’s the challenge?&lt;/h1&gt;

&lt;p&gt;COVID-19 pandemic has changed lives around the world. This is the current situation as of 2020/09/26, according to &lt;a href=&quot;https://covid19.who.int/&quot;&gt;WHO&lt;/a&gt;. &lt;img src=&quot;/assets/cnn-covid-19/covid-19-pandemic.png&quot; alt=&quot;current situation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CT scans have been used to screen and diagnose COVID-19, especially in areas where swab test resources are severely lacking. The goal of this data challenge is to diagnose COVID-19 using chest CT scans. Therefore, we need to build a &lt;strong&gt;classification model&lt;/strong&gt; that can classify patients to COVID or NonCOVID based on their chest CT scans, &lt;strong&gt;as accurately as possible&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-provided&quot;&gt;What’s provided?&lt;/h2&gt;
&lt;p&gt;Relatively even number of COVID and NonCOVID images are provided to train the model. While meta-information of these images is also provided, they will not be provided during testing. The competition also requires that the model’s training with provided data must take less than one hour.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training data set
    &lt;ul&gt;
      &lt;li&gt;251 COVID-19 CT images&lt;/li&gt;
      &lt;li&gt;292 non-COVID-19 CT images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Meta-information
    &lt;ul&gt;
      &lt;li&gt;e.g., patient information, severity, image caption&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All challenge data are taken from a public &lt;a href=&quot;https://github.com/UCSD-AI4H/COVID-CT&quot;&gt;data set&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;model-performance&quot;&gt;Model performance&lt;/h1&gt;
&lt;p&gt;Let’s first take a look at the result, shall we?&lt;/p&gt;

&lt;p&gt;The trained model is evaluated with an independent set of test data. Here you can see the confusion matrix. The overall accuracy is about 85% with slightly better sensitivity than specificity, i.e., true positive rate &amp;gt; true negative rate. 
&lt;img src=&quot;/assets/cnn-covid-19/confusion_matrix.png&quot; alt=&quot;confusion&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Here are some of the provided NonCOVID and COVID CT scans. It’s important to note that the challenge is to distinguish between COVID and NonCOVID CT scans, rather than COVID and Normal scans. In fact, there may be some NonCOVID CT scans that belong to other pneumonia patients. 
&lt;img src=&quot;/assets/cnn-covid-19/first_look.png&quot; alt=&quot;first_look&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;train-validation-split&quot;&gt;Train-validation split&lt;/h2&gt;
&lt;p&gt;We reserve 20% of the data for validation. Since some consecutive images come from the same patient, they tend to be similar to each other.  That is, many of our data are &lt;strong&gt;not independent&lt;/strong&gt;. To prevent data leakage (information of training data spills over to validation data), we keep the original image sequence and hold out the last 20% as the validation set.&lt;/p&gt;

&lt;p&gt;After the splitting, we have two pairs of data:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;X_train&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;y_train&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;X_val&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;y_val&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;X is a list of CT scans, and y is a list of binary labels (0 for NonCOVID, 1 for COVID).&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h2&gt;
&lt;p&gt;Data augmentation is a common way to include more random variations in the training data. It helps to prevent overfitting. For image-related learning problems, augmentation typically means applying &lt;strong&gt;random&lt;/strong&gt; geometric (e.g., crop, flip, rotate, etc.) and appearance transformation (e.g., contrast, edge filter, Gaussian blur, etc.). Here we use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.Sequential&lt;/code&gt; to create a pipeline in which the input image is randomly transformed through the following operations:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Random horizontal and vertical flip&lt;/li&gt;
  &lt;li&gt;Rotation by a random degree in the range of $[-5\%, 5\%]*2\pi$&lt;/li&gt;
  &lt;li&gt;Random zoom in height by $5\%$&lt;/li&gt;
  &lt;li&gt;Random translation by $5\%$&lt;/li&gt;
  &lt;li&gt;Random contrast adjustment by $5\%$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is how they look after the augmentation. 
&lt;img src=&quot;/assets/cnn-covid-19/augmented_scans.png&quot; alt=&quot;augmented_scans&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-pre-trained-cnn-as-the-backbone&quot;&gt;Using pre-trained CNN as the backbone&lt;/h2&gt;
&lt;p&gt;We do not build a CNN from scratch. For an image-related problem with only a modest number of training images, it is recommended to use a pre-trained model as the backbone and do &lt;a href=&quot;https://cs231n.github.io/transfer-learning/&quot;&gt;transfer learning&lt;/a&gt; on that. The chosen model is &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0&quot;&gt;EfficientNetB0&lt;/a&gt;. It belongs to the family of models called &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNets&lt;/a&gt; proposed by researchers from Google. EfficientNets are among the current state-of-the-art CNNs for computer vision tasks. They&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;need a considerably lower number of parameters,&lt;/li&gt;
  &lt;li&gt;achieved very high accuracies on ImageNet,&lt;/li&gt;
  &lt;li&gt;transferred well to other image classification tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here’s a performance &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;comparison&lt;/a&gt; between EfficientNets and other well-known models:&lt;img src=&quot;/assets/cnn-covid-19/efficientnets.png&quot; alt=&quot;EfficientNet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EfficientNets, and other well-known pre-trained models, can be easily loaded from &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.applications&lt;/code&gt;. We first import the pre-trained EfficientNetB0 and use it as our model backbone. We remove the original output layer of EfficientNetB0 since it was trained for 1000-class classification. Also, we freeze the model’s weights so that they won’t be updated during the initial training.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Create a base model from the pre-trained EfficientNetB0
base_model = keras.applications.EfficientNetB0(input_shape=IMG_SHAPE, include_top=False)
base_model.trainable = False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wrap-our-model-around-it&quot;&gt;Wrap our model around it&lt;/h2&gt;

&lt;p&gt;With EfficientNet imported, we can use it to our problem by wrapping our classification model around it. You can think of the EfficientNetB0 as a trained feature extractor. The final model has:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;An input layer&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;EfficientNetB0 base model&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;An average pooling layer: Pool the information by average operation&lt;/li&gt;
  &lt;li&gt;A dropout layer: Set a percentage of inputs to zero&lt;/li&gt;
  &lt;li&gt;A classification layer: Output the probability of NonCOVID&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can also use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.utils.plot_model&lt;/code&gt; to visualize our model. &lt;img src=&quot;/assets/cnn-covid-19/model.png&quot; alt=&quot;model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;?&lt;/code&gt; in input and output shape is a reserved place for the number of samples, which the model does not know yet.&lt;/li&gt;
  &lt;li&gt;EfficientNetB0 sits right after the input layer.&lt;/li&gt;
  &lt;li&gt;The last (classification) layer has an output of dimension 1: The probability for NonCOVID.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;training-our-model&quot;&gt;Training our model&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Public data pre-training&lt;/strong&gt;: To help EfficientNetB0 adapt to COVID vs NonCOVID image classification, we’ve actually trained our model on another public CT scan data &lt;a href=&quot;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&quot;&gt;set&lt;/a&gt;. The hope is that training the model on CT scans would allow it to learn features specific to our COVID-19 classification task. We will not go into the public data training part, but the procedure is essentially the same as what I will show below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transfer learning workflow&lt;/strong&gt;: We use a typical transfer-learning workflow:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Phase 1 (Feature extraction): Fix EfficientNetB0’s weights, only update the last classification layer’s weights.&lt;/li&gt;
  &lt;li&gt;Phase 2 (Fine tuning): Allow some of EfficientNetB0’ weights to update as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can read more about the workflow &lt;a href=&quot;https://www.tensorflow.org/guide/keras/transfer_learning#the_typical_transfer-learning_workflow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key configurations&lt;/strong&gt;: We use the following metrics and loss function:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: to evaluate model performance
    &lt;ul&gt;
      &lt;li&gt;Binary accuracy&lt;/li&gt;
      &lt;li&gt;False and true positives&lt;/li&gt;
      &lt;li&gt;False and true negatives&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: to guide gradient search
    &lt;ul&gt;
      &lt;li&gt;Binary cross-entropy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We use the &lt;code class=&quot;highlighter-rouge&quot;&gt;Adam&lt;/code&gt; optimizer, the learning rates is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;[1e-3, 1e-4]&lt;/code&gt; and the number of training epochs is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;[10, 30]&lt;/code&gt; for the two phases, respectively. The two-phase training is iterated for two times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training history&lt;/strong&gt;: Let’s visualize the training history: &lt;img src=&quot;/assets/cnn-covid-19/training_history.png&quot; alt=&quot;training_history&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here you can see that after we’ve allowed some layers of EfficientNets to update (after Epoch 10), we obtain a significant improvement in classification accuracy. The final training and validation accuracy is around 98% and 82%.&lt;/p&gt;

&lt;h1 id=&quot;how-does-it-perform-on-test-data&quot;&gt;How does it perform on test data?&lt;/h1&gt;
&lt;p&gt;We can obtain a set of test data from the same data repo that contains 105 NonCOVID and 98 COVID images. Let’s see how the trained model performs on them. Here’s a result breakdown using &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.metrics.classification_report&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;              precision    recall  f1-score   support

       COVID       0.85      0.83      0.84        98
    NonCOVID       0.84      0.87      0.85       105

    accuracy                           0.85       203
   macro avg       0.85      0.85      0.85       203
weighted avg       0.85      0.85      0.85       203
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And the ROC curve: &lt;img src=&quot;/assets/cnn-covid-19/roc_curve.png&quot; alt=&quot;roc_curve&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-are-correctly-and-incorrectly-classified-ct-scans&quot;&gt;What are correctly and incorrectly classified CT scans?&lt;/h2&gt;

&lt;p&gt;We can dive into the classification result and see which ones are identified correctly and identified incorrectly. &lt;strong&gt;Potential patterns&lt;/strong&gt; found could be leveraged to help further improve the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Could you identify some patterns?&lt;/strong&gt;
&lt;img src=&quot;/assets/cnn-covid-19/true_positives.png&quot; alt=&quot;true_positives&quot; /&gt;
&lt;img src=&quot;/assets/cnn-covid-19/true_negatives.png&quot; alt=&quot;true_negatives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here are the incorrect ones:
&lt;img src=&quot;/assets/cnn-covid-19/false_positives.png&quot; alt=&quot;false_positives&quot; /&gt;
&lt;img src=&quot;/assets/cnn-covid-19/false_negatives.png&quot; alt=&quot;false_negatives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can probably make several observations here:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;True positives have obvious abnormal patterns, and the lung structures are well-preserved.&lt;/li&gt;
  &lt;li&gt;Many of the true negatives have complete black lungs (no abnormal pattern).&lt;/li&gt;
  &lt;li&gt;The lung boundaries of many false positives are not clear.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The point is, to a non-medical person like me, many of the COVID and NonCOVID images look the same. The ambiguity is even more severe when some images have unclear lung boundaries. It seems like our CNN is also having trouble distinguishing those images.&lt;/p&gt;

&lt;h1 id=&quot;where-do-we-go-from-here&quot;&gt;Where do we go from here?&lt;/h1&gt;

&lt;p&gt;From the above results, we can see that a pre-trained CNN can be adapted to achieve a really strong baseline performance. However, there are clear limitations to what a deep learning model (or any other model) alone can achieve. In this case, computer vision researchers and medical experts need to collaborate in a meaningful way so that the end model is both computationally capable and medically reliable.&lt;/p&gt;

&lt;p&gt;There are several directions for which we could make the model better:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Lung segmentation&lt;/strong&gt;: Process each image and retain only the lung area of the CT scan, for example, see &lt;a href=&quot;https://pubs.rsna.org/doi/full/10.1148/rg.2015140232&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;More sophisticated &lt;strong&gt;transfer learning&lt;/strong&gt; design: For example, see multi-task &lt;a href=&quot;https://ruder.io/multi-task/&quot;&gt;learning&lt;/a&gt; or supervised domain &lt;a href=&quot;https://en.wikipedia.org/wiki/Domain_adaptation#The_different_types_of_domain_adaptation&quot;&gt;adaptation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ensemble&lt;/strong&gt; model: This seems like a common belief, especially among Kaggle users, that building an ensemble &lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html&quot;&gt;model&lt;/a&gt; would almost always give you an extra few percent accuracy increases.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;That’s it for our CNN COVID-19 CT scan classification! Thank you!&lt;/em&gt; 👏👏👏&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Convolutional Neural Network: How is it different from the other networks?</title>
   <link href="http://localhost:4000/data/2020/09/24/intro-to-cnn.html"/>
   <updated>2020-09-24T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/24/intro-to-cnn</id>
   <content type="html">&lt;p&gt;I am not a deep learning researcher, but I’ve come to know a few things about neural networks through various exposures. I’ve always heard that CNN is a type of neural network that’s particularly good at image-related problems. But, what does that really mean? What’s with the word “convolutional”? What’s so unusual about an image-related problem that a different network is required?&lt;/p&gt;

&lt;p&gt;Recently I had the opportunity to work on a COVID-19 image classification problem and built a CNN-based classifier using tensorflow.kerasthat achieved an 87% accuracy rate. More importantly, I think that I’ve figured out the answers to those questions. In this post, I share with you those answers in an intuitive math-free way. If you are already familiar with DNNs and CNNs, this post should feel like a good refresher. If not, at the end of this post, you could gain an intuitive understanding of the motivation behind CNN and the unique features that define a CNN.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#deep-neural-network&quot; id=&quot;markdown-toc-deep-neural-network&quot;&gt;Deep neural network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#forward-propagation&quot; id=&quot;markdown-toc-forward-propagation&quot;&gt;Forward propagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backpropagation&quot; id=&quot;markdown-toc-backpropagation&quot;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convolutional-neural-network&quot; id=&quot;markdown-toc-convolutional-neural-network&quot;&gt;Convolutional neural network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-power-of-convolution&quot; id=&quot;markdown-toc-the-power-of-convolution&quot;&gt;The power of convolution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#key-features-of-a-cnn&quot; id=&quot;markdown-toc-key-features-of-a-cnn&quot;&gt;Key features of a CNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-next&quot; id=&quot;markdown-toc-whats-next&quot;&gt;What’s next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;deep-neural-network&quot;&gt;Deep neural network&lt;/h1&gt;

&lt;p&gt;If you are familiar with DNNs, feel free to skip to &lt;a href=&quot;#Convolutional neural network&quot;&gt;Convolutional neural network&lt;/a&gt;. Before diving into neural networks, let’s first see the machine learning big picture:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; (ML) is the study of computer algorithms that improve automatically through experience. – Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking at the problems that ML tries to solve, ML is often sliced into&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Supervised learning: predicting a label, e.g., classification, or a continuous variable;&lt;/li&gt;
  &lt;li&gt;Unsupervised learning: pattern recognition for unlabeled data, e.g., clustering;&lt;/li&gt;
  &lt;li&gt;Reinforcement learning: algorithms learn the best way to “behave”, e.g., AlphaGo, self-driving cars.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among others, deep learning is a powerful form of machine learning that has garnered much attention for its successes in computer vision (e.g., image recognition), natural language processing, and beyond. The neural network is inspired by information processing and communication nodes in biological systems. By design, input data is passed through layers of the network, containing several nodes, analogous to “neurons”. The system then outputs a particular representation of the information. DNN is probably the most well-known network for deep learning, and it can be trained to learn the features of the data very well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/deep-nn.jpg&quot; alt=&quot;Deep neural network&quot; /&gt;Image &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347705/&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Roughly speaking, there are two important operations that make a neural network.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Forward propagation&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward propagation&lt;/h2&gt;
&lt;p&gt;This is the &lt;strong&gt;prediction&lt;/strong&gt; step. The network reads the input data, computes its values across the network, and gives a final output value.&lt;/p&gt;

&lt;p&gt;But how does the network computes an output value? Let’s see what happens in a single layer network when it makes one prediction. It takes input as a vector of numbers. Each node in the layer has its weight. When the input value is passed through the layer, the network computes its weighted sum. This is usually followed by a (typically nonlinear) activation function, e.g., step function, where the weighted sum is “activated”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/perceptron.jpg&quot; alt=&quot;perceptron&quot; /&gt; Image &lt;a href=&quot;https://deepai.org/machine-learning-glossary-and-terms/perceptron&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you know a bit about algebra, this is what the operation is doing:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(\mathbf{w}\cdot \mathbf{x} + b)&lt;/script&gt;

&lt;p&gt;where $\mathbf{w}\cdot \mathbf{x} + b$ is the weighted sum, $f(\cdot)$ is the activation function, and $y$ is the output. Now, in a deeper neural network, the procedure is essentially the same, i.e., the &lt;strong&gt;input –&amp;gt; weighted sum –&amp;gt; activation&lt;/strong&gt; process is repeated for each layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/mlp.png&quot; alt=&quot;mlp&quot; /&gt; Image &lt;a href=&quot;https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2020/lectures/03/MLP_and_backprop.html&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;This is the &lt;strong&gt;training&lt;/strong&gt; step. By comparing the network’s predictions/outputs and the ground truth values, i.e., compute loss, the network adjusts its parameters to improve the performance.&lt;/p&gt;

&lt;p&gt;How does the network adjust the parameters (weights and biases) through training? This is done through an operation called &lt;strong&gt;backpropagation&lt;/strong&gt;, or backprop. The network takes the loss and recursively calculates the loss function’s slope with respect to each parameter. Calculating these slopes requires the usage of chain rule from calculus; you can read more about it &lt;a href=&quot;https://sebastianraschka.com/faq/docs/backprop-arbitrary.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An optimization algorithm is then used to update network parameters using the gradient information until the performance cannot be improved anymore. One commonly used optimizer is stochastic gradient descent.&lt;/p&gt;

&lt;p&gt;One analogy often used to explain gradient-based optimization is hiking. Training the network to minimize loss is like getting down to the lowest point on the ground from a mountain. Backprop operation finding the loss function gradients is like finding the path on your way down. The optimization algorithm is the step where you actually take the path and eventually reach the lowest point. &lt;img src=&quot;/assets/cnn-covid-19/gradient-descent.png&quot; alt=&quot;gradient-descent&quot; /&gt; Image &lt;a href=&quot;https://www.datasciencecentral.com/profiles/blogs/alternatives-to-the-gradient-descent-algorithm&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am glossing over many details, but I hope you now know that DNN&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is a powerful &lt;strong&gt;machine learning&lt;/strong&gt; technique;&lt;/li&gt;
  &lt;li&gt;can be used to tackle &lt;strong&gt;supervised&lt;/strong&gt;, &lt;strong&gt;unsupervised&lt;/strong&gt; and &lt;strong&gt;reinforcement learning&lt;/strong&gt; problems;&lt;/li&gt;
  &lt;li&gt;consists of forward propagation (&lt;strong&gt;input to output&lt;/strong&gt;) and backpropagation (&lt;strong&gt;error to parameter update&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are ready to talk about CNN!&lt;/p&gt;

&lt;h1 id=&quot;convolutional-neural-network&quot;&gt;Convolutional neural network&lt;/h1&gt;

&lt;p&gt;Ordinary neural networks that we’ve talked about above expect input data to be a &lt;strong&gt;vector of numbers&lt;/strong&gt;, i.e., $\mathbf{x} = [x_1, x_2, x_3, \dots]$. What if we want to train an &lt;strong&gt;image classifier&lt;/strong&gt;, i.e., use an image as the input? Let’s talk about some digital image basics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An image is a &lt;strong&gt;collection of pixels&lt;/strong&gt;. For example, a 32-by-32 image has $32 \times 32 = 1024$ pixels.&lt;/li&gt;
  &lt;li&gt;Each pixel is an &lt;strong&gt;intensity represented by a number&lt;/strong&gt; in the range $[0, 255]$, $0$ is black and $255$ is white.&lt;/li&gt;
  &lt;li&gt;Color images have three dimensions: &lt;strong&gt;[width, height, depth]&lt;/strong&gt; where depth is usually 3.&lt;/li&gt;
  &lt;li&gt;Why is depth 3? That’s because it encodes the intensity of [&lt;strong&gt;R&lt;/strong&gt;ed, &lt;strong&gt;G&lt;/strong&gt;reen, &lt;strong&gt;B&lt;/strong&gt;lue], i.e., RGB values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, this black and white Lincoln image is just a matrix of integers. 
&lt;img src=&quot;/assets/cnn-covid-19/image_pixel.png&quot; alt=&quot;image_pixel&quot; /&gt; Image &lt;a href=&quot;https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since a digital image can be represented as a 2D grid of pixel values, we could stretch out/flatten the grid, make it into a vector of numbers and feed it into a neural network. That solves the problem…right?&lt;/p&gt;

&lt;p&gt;However, there are two major limitations to this approach.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;It does not scale well to bigger images.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;While it is still manageable for an input with $32\times32 = 1024$ dimensions, most real-life images are bigger than this.&lt;/li&gt;
      &lt;li&gt;For example, a color image of size 320x320x3 would translate to an input with dimension &lt;strong&gt;307200&lt;/strong&gt;!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;It does not consider the properties of an image.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Locality&lt;/em&gt;: Nearby pixels are usually strongly correlated (e.g., see the outline of Lincoln’s face). Stretching it out breaks the pattern.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Translation invariance&lt;/em&gt;: Meaningful features could occur anywhere on an image, e.g., see the flying bird.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/flying-bird.png&quot; alt=&quot;bird&quot; /&gt; Image &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L3%20-%20UUCLxDeepMind%20DL2020.pdf&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-power-of-convolution&quot;&gt;The power of convolution&lt;/h2&gt;

&lt;p&gt;On the other hand, CNN is designed to scale well with images and take advantage of these unique properties. It does with two unique features:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Weight sharing&lt;/strong&gt;: All local parts of the image are processed with the same weights so that identical patterns could be detected at many locations, e.g., horizontal edges, curves and etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hierarchy of features&lt;/strong&gt;: Lower-level patterns learned at the start are composed to form higher-level ones across layers, e.g., edges to contours to face outline.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is done through the operation of &lt;strong&gt;convolution&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Define a filter: a 2D weight matrix of a certain size, e.g. 3-by-3 filter.&lt;/li&gt;
  &lt;li&gt;Convolve the whole image with the filter: multiply each pixel under the filter with the weight.&lt;/li&gt;
  &lt;li&gt;Convolution output forms a new image: a feature map.&lt;/li&gt;
  &lt;li&gt;Using multiple filters (each with a different weight matrix), different features can be captured.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;convolution-example-mean-filter&quot;&gt;Convolution example: mean filter&lt;/h4&gt;
&lt;p&gt;Actually, let’s see the operation in numbers and images. It will be easier to understand what’s really going on. Here we create an image of a bright square using 0s and 1s. &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; interprets values in [0,1] the same as in [0, 255].&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Original image pixel values: 
 [[0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And this is how the image looks like: &lt;img src=&quot;/assets/cnn-covid-19/bright_square.png&quot; alt=&quot;square&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that a filter is a 2D weight matrix. Let’s create an example filter, and call it the &lt;strong&gt;“mean filter”&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[[0.11 0.11 0.11]
 [0.11 0.11 0.11]
 [0.11 0.11 0.11]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In a convolution, this “mean filter” actually slides across the image, takes the values of 9 connected pixels, multiplies each with the weight (0.11), and returns the sum, i.e., the weighted average of the original 9 values, hence the name “mean filter”: 
&lt;img src=&quot;/assets/cnn-covid-19/convolution.gif&quot; alt=&quot;convolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the averaging effect from the filtered image pixel values. It blurs out any edges in the image.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Filtered image pixel values: 
 [[0.11 0.22 0.33 0.22 0.11]
 [0.22 0.44 0.67 0.44 0.22]
 [0.33 0.67 1.   0.67 0.33]
 [0.22 0.44 0.67 0.44 0.22]
 [0.11 0.22 0.33 0.22 0.11]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;What’s this to do with a convolutional neural network?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Well, CNN essentially applies the same convolution procedure, but the key difference is it &lt;strong&gt;learns the filter weights&lt;/strong&gt; through backpropagation (training).&lt;/p&gt;

&lt;p&gt;Also, there are usually many filters for each layer, each with a different weight matrix, applied to the same image. Each filter would capture a different pattern of the same image. A CNN could also have many layers of convolution. The complexity of the network allows features at different scales to be captured. This is the hierarchy of features mentioned above.&lt;/p&gt;

&lt;p&gt;For example, here’s an illustration of features learned by filters from early to the latter part of the network.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Early filters capture edges and textures. (&lt;strong&gt;General&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Latter filters form parts and objects. (&lt;strong&gt;Specific&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/feature.png&quot; alt=&quot;title&quot; /&gt; Image &lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;key-features-of-a-cnn&quot;&gt;Key features of a CNN&lt;/h2&gt;

&lt;p&gt;While DNN uses many fully-connected layers, CNN contains mostly convolutional layers. In its simplest form, CNN is a network with a set of layers that transform an image to a set of class probabilities. Some of the most popular types of layers are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Convolutional layer&lt;/strong&gt; (CONV): Image undergoes a convolution with filters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RELU layer&lt;/strong&gt; (RELU): Element-wise nonlinear activation function (same as those in DNN before).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pooling layer&lt;/strong&gt; (POOL): Image undergoes a convolution with a mean (or max) filter, so it’s down-sampled.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fully-connected layer&lt;/strong&gt; (FC): Usually used as the last layer to output a class probability prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, if you are &lt;em&gt;designing your own CNN&lt;/em&gt;, there are many elements to play with. They generally fall into two categories:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Type of convolutional layer
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Depth&lt;/strong&gt;: The number of filters to use for each layer.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Stride&lt;/strong&gt;: How big of a step to take when sliding the filter across the image, usually 1 (see the convolution GIF above) or 2.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: Size of each convolution filter, e.g., the mean filter is 3-by-3.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Padding&lt;/strong&gt;: Whether to use paddings around images when doing convolution. This determines the output image size.&lt;/li&gt;
      &lt;li&gt;And others.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How to connect each layer?
    &lt;ul&gt;
      &lt;li&gt;Besides the type of layers, you need to design an architecture for your CNN. This is an active field of research, e.g., what’s a better architecture? or can we automatically search for a better architecture? Check “neural architecture search” out if you are interested.&lt;/li&gt;
      &lt;li&gt;A commonly used architecture goes like this:
        &lt;ul&gt;
          &lt;li&gt;$\text{INPUT} \rightarrow [ [\text{CONV} \rightarrow \text{RELU}]^N \rightarrow \text{POOL}]^M \rightarrow [\text{FC} \rightarrow \text{RELU}]^K \rightarrow \text{FC}$&lt;/li&gt;
          &lt;li&gt;The power $N, M, K$ means that the operation is repeated those number of times.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h1&gt;

&lt;p&gt;Thank you for reading until the end! I hope by now you could see the difference between a CNN and a regular DNN, and also gained an intuitive understanding of what convolution operation is all about. Please let me know your thoughts or any feedback in the comment section below.&lt;/p&gt;

&lt;p&gt;In the next article, we explore how CNN can be used to build a COVID-19 CT scan image classifier. Unsurprisingly, a pre-trained CNN can achieve strong baseline performance (85% accuracy). However, it would take more than a neural network to produce reliable and convincing results. Here’s the article:&lt;br /&gt;
&lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/09/27/detecting-covid19-using-cnn.html&quot;&gt;What deep learning needs for better COVID-19 detection&lt;/a&gt;&lt;/p&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;further-resources&quot;&gt;Further resources&lt;/h1&gt;

&lt;p&gt;If you are interested in knowing more about CNNs, check out 👉:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/&quot;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=shVKhOmT0HE&amp;amp;ab_channel=DeepMind&quot;&gt;DeepMind x UCL | Convolutional Neural Networks for Image Recognition
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and how to implement them 👉:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://keras.io/getting_started/intro_to_keras_for_engineers/&quot;&gt;Introduction to Keras for Engineers
&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/images/cnn&quot;&gt;Tensorflow Keras CNN Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enjoy! 👏👏👏&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Statistical learning knowledge repository</title>
   <link href="http://localhost:4000/data/2020/09/17/knowledge-repository.html"/>
   <updated>2020-09-17T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/17/knowledge-repository</id>
   <content type="html">&lt;p&gt;This is a collection of my notes on various topics of statistical learning. It is intended as a knowledge repository for some of the unexpected discoveries, less-talked-about connections, and under-the-hood concepts for statistical learning. It’s a work in progress that I will periodically update.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ridge-regularization&quot; id=&quot;markdown-toc-ridge-regularization&quot;&gt;Ridge regularization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#relationship-between-ols-ridge-regression-and-pca&quot; id=&quot;markdown-toc-relationship-between-ols-ridge-regression-and-pca&quot;&gt;Relationship between OLS, Ridge regression, and PCA&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;ridge-regularization&quot;&gt;Ridge regularization&lt;/h1&gt;
&lt;h2 id=&quot;relationship-between-ols-ridge-regression-and-pca&quot;&gt;Relationship between OLS, Ridge regression, and PCA&lt;/h2&gt;
&lt;p&gt;Simple yet elegant relationships between OLS estimates, ridge estimates and PCA can be found through the lens of spectral decomposition. We see these relationships through Exercise 8.8.1 of MA&lt;sup id=&quot;fnref:MA&quot;&gt;&lt;a href=&quot;#fn:MA&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;set-up&quot;&gt;Set-up&lt;/h3&gt;

&lt;p&gt;Given the following regression model:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\mu \mathbf{1}+\mathbf{u}, \quad \mathbf{u} \sim N_{\mathrm{n}}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right),&lt;/script&gt;

&lt;p&gt;consider the columns of $\mathbf{X}$ have been standardized to have mean 0 and variance 1. Then the ridge estimate of $\boldsymbol{\beta}$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\beta}^* = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^{\top} \mathbf{y}&lt;/script&gt;

&lt;p&gt;where for given $\mathbf{X}$, $\lambda \ge 0$ is a small fixed ridge regularization parameter. Note that when $\lambda = 0$, it is just the OLS formulation. Also, consider the spectral decomposition of the var-cov matrix $\mathbf{X}^{\top} \mathbf{X} = \mathbf{G} \mathbf{L} \mathbf{G}^{\top}$. Let $\mathbf{W} = \mathbf{X}\mathbf{G}$ be the principal component transformation of the original data matrix.&lt;/p&gt;

&lt;h3 id=&quot;result-11&quot;&gt;Result 1.1&lt;/h3&gt;

&lt;p&gt;If $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ represents the parameter vector of principal components, then we can show that the ridge estimates $\boldsymbol{\alpha}^*$ can be obtained from OLS estimates $\hat{\boldsymbol{\alpha}}$ by simply scaling them with the ridge regularization parameter:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p.&lt;/script&gt;

&lt;p&gt;This result shows us two important insights:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For PC-transformed data, we can obtain the ridge estimates through a simple element-wise scaling of the OLS estimates.&lt;/li&gt;
  &lt;li&gt;The shrinking effect of ridge regularization depends on both $\lambda$ and eigenvalues of the corresponding PC:
    &lt;ul&gt;
      &lt;li&gt;Larger $\lambda$ corresponds to heavier shrinking for every parameter.&lt;/li&gt;
      &lt;li&gt;However, given the same $\lambda$, principal components corresponding to larger eigenvalues receive the least shrinking.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To demonstrate the two shrinking effects, I plotted the percentage shrunken ($1- l_{i}/(l_{i}+\lambda)$) as a function of the ordered principal components as well as the value of the ridge regularization parameter. The two shrinking effects are clearly visible from this figure.
&lt;img src=&quot;/assets/learning-repo/pca_ridge_lambda_effect.png&quot; alt=&quot;lambda_effect&quot; /&gt;&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.1:&lt;/summary&gt;
Since $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ and $\mathbf{W} = \mathbf{X}\mathbf{G}$, then 

$$
\begin{align*}
\boldsymbol{\alpha}^* &amp;amp;= (\mathbf{W}^{\top}\mathbf{W} + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= (\Lambda + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= \operatorname{diag}((l_{i} + \lambda)^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

Hence 

$$
\alpha^*_{i} = (l_i + \lambda)^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$ 

for $i=1, \ldots, p$, where $\mathbf{w}_{(i)}$ is the $i$th column of $\mathbf{W}$. Since 

$$
\begin{align*}
\hat{\boldsymbol{\alpha}} &amp;amp;= (\mathbf{W}^{\top}\mathbf{W})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= \operatorname{diag}(l_{i}^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

We have 

$$
\hat{\alpha}_{i} = l_i^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$

for $i=1, \ldots, p$. Therefore, by comparing the two estimate expressions, we have the result

$$
\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p. \blacksquare
$$

&lt;/details&gt;

&lt;h3 id=&quot;result-12&quot;&gt;Result 1.2&lt;/h3&gt;

&lt;p&gt;It follows from Result 1.1 that we can establish a direct link between the OLS estimate $\hat{\boldsymbol{\beta}}$ and the ridge estimate $\boldsymbol{\beta}^*$ through spectral decomposition of the var-cov matrix. Specifically, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\beta}^{*}=\mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}, \quad \text { where } \mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right),&lt;/script&gt;

&lt;p&gt;for $i=1, \ldots, p$.&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.2:&lt;/summary&gt;
Since $\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}$, for $i=1, \ldots, p$, and $\hat{\boldsymbol{\alpha}} = \mathbf{G}^{\top}\hat{\boldsymbol{\beta}}$, then

$$
\boldsymbol{\alpha}^{*} =  \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \,,
$$

by writing in matrix form. Also, since 

$$
\boldsymbol{\alpha}^* = \mathbf{G}^{\top}\boldsymbol{\beta}^*
$$

where $\boldsymbol{\beta}^*$ is the ridge estimate of $\boldsymbol{\beta}$, then we have

$$
\begin{align*}
\mathbf{G}^{\top}\boldsymbol{\beta}^* &amp;amp;= \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
\boldsymbol{\beta}^* &amp;amp;= \mathbf{G} \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
&amp;amp;= \mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}
\end{align*}
$$

where $\mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right)$, for $i=1, \ldots, p$. $\blacksquare$
&lt;/details&gt;

&lt;h3 id=&quot;result-13&quot;&gt;Result 1.3&lt;/h3&gt;

&lt;p&gt;One measure of the quality of the estimators $\boldsymbol{\beta}^*$ is the trace mean square error (MSE):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\phi(\lambda) &amp;= \operatorname{tr} E[(\boldsymbol{\beta}^* - \boldsymbol{\beta})(\boldsymbol{\beta}^* - \boldsymbol{\beta})^{\top}] \\
&amp;= \sum_{i=1}^{p} E[(\beta_{i}^* - \beta_{i})^2] \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, from the previous two results, we can show that the trace MSE of the ridge estimates can be decomposed into two parts: &lt;strong&gt;variance&lt;/strong&gt; and &lt;strong&gt;bias&lt;/strong&gt;, and obtain an explicit formula for them. The availability of the exact formula for MSE allows things like regularization path to be computed easily.&lt;/p&gt;

&lt;p&gt;Specifically, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(\lambda) = \gamma_1(\lambda) + \gamma_2(\lambda)&lt;/script&gt;

&lt;p&gt;where the first component is the sum of variances:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_1(\lambda) = \sum_{i=1}^{p} V(\beta_{i}^*) = \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,,&lt;/script&gt;

&lt;p&gt;and the second component is the sum of squared biases:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_2(\lambda) = \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 = \lambda^2 \sum_{i=1}^{p} \frac{\alpha_i^2}{(l_i + \lambda)^2} \,.&lt;/script&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.3:&lt;/summary&gt;
First let's look at the sum of variances. We start by writing out the expression for the variance of the ridge estimates:

$$
\begin{align*}
V(\boldsymbol{\beta}^*) &amp;amp;= \mathbf{GDG}^{\top} V(\hat{\boldsymbol{\beta}}) \mathbf{GDG}^{\top} \\
&amp;amp;= \mathbf{GDG}^{\top} \sigma^2 \mathbf{X^{\top}X}^{-1} \mathbf{GDG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{GDG}^{\top} (\mathbf{GLG})^{-1} \mathbf{GDG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{GD} L^{-1} \mathbf{DG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{G} \operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \,, \quad i=1, \ldots, p.
\end{align*}
$$ 

Hence, by extracting out the variances from the diagonal, we obtain the first expression:

$$
\begin{align*}
\gamma_1(\lambda) &amp;amp;= \sum_{i=1}^{p} V(\beta_{i}^*) \\
&amp;amp;= \operatorname{tr} V(\boldsymbol{\beta}^*) \\
&amp;amp;= \sigma^2 \operatorname{tr}(\operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \mathbf{G}) \\
&amp;amp;= \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,.
\end{align*}
$$

Now let's look at the bias term. We write it in matrix form to see that:

$$
\begin{align*}
\gamma_2(\lambda) &amp;amp;= \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 \\
&amp;amp;= (E[\boldsymbol{\beta}^*] - \boldsymbol{\beta})^{\top}(E[\boldsymbol{\beta}^*] - \boldsymbol{\beta}) \\
&amp;amp;= (\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta})^{\top}(\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta}) \\
&amp;amp;= \mathbf{B}^{\top}\mathbf{GD}^2\mathbf{G}^{\top}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\top}\mathbf{GDG^{\top}}\boldsymbol{\beta} + \boldsymbol{\beta}^{\top}\boldsymbol{\beta} \\
&amp;amp;= \boldsymbol{\alpha}^{\top}\mathbf{D}^2\boldsymbol{\alpha} - 2\boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha} + \boldsymbol{\alpha}^{\top}\mathbf{G^{\top}G}\boldsymbol{\alpha} \\
&amp;amp;= \boldsymbol{\alpha}^{\top} (\mathbf{D}^2 - 2\mathbf{D} + 1) \boldsymbol{\alpha} \\
&amp;amp;= \boldsymbol{\alpha}^{\top} \operatorname{diag}(\lambda^2/(l_i + \lambda)^2) \boldsymbol{\alpha} \\
&amp;amp;= \lambda^2 \sum_{i=1}^{p}(\alpha_{i}^2/(l_i + \lambda)^2) \,.
\end{align*}
$$

Combining the two gamma terms completes the proof. $\blacksquare$
&lt;/details&gt;

&lt;h3 id=&quot;result-14&quot;&gt;Result 1.4&lt;/h3&gt;

&lt;p&gt;This is a quick but revealing result that follows from Result 1.3. Taking a partial derivative of the trace MSE function with respect to $\lambda$ and take $\lambda = 0$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial{\phi(\lambda)}}{\partial{\lambda}} = -2 \sigma^2 \sum_{i=1}^{p} 1/l_i^2 \,.&lt;/script&gt;

&lt;p&gt;Notice that the gradient of the trace MSE function is negative when $\lambda$ is 0. This tells us two things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;We can reduce the trace MSE by taking a non-zero $\lambda$ value. In particular, we are trading a bit of bias for a reduction in variance as the variance function ($\gamma_1$) is monotonically decreasing in $\lambda$. However, we need to find the right balance between variance and bias so that the overall trace MSE is minimized.&lt;/li&gt;
  &lt;li&gt;The reduction in trace MSE by ridge regularization is higher when some $l_i$s are small. That is, when there is considerable collinearity among the predictors, ridge regularization can achieve much smaller trace MSE than OLS.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;visualization&quot;&gt;Visualization&lt;/h3&gt;

&lt;p&gt;Using the &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.metrics.make_regression&lt;/code&gt; function, I generated a noisy regression data set with 50 samples and 10 features. However, most of the variances (in PCA sense) are explained by 5 of those 10 features, i.e. last 5 eigenvalues are relatively small. Here are the regularization path and the coefficient error plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/learning-repo/ridge_lambda_mse.png&quot; alt=&quot;ridge_error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figure, we can clearly see that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Increasing $\lambda$ shrinks every coefficient towards 0. &lt;/li&gt;
  &lt;li&gt;OLS procedure (left-hand side of both figures) produces erroneous (and with a large variance) estimate. the estimator MSE is significantly larger than that of the ridge regression. &lt;/li&gt;
  &lt;li&gt;An optimal $\lambda$ is found at around 1 where the MSE of ridge estimated coefficients is minimized. &lt;/li&gt;
  &lt;li&gt;On the other hand, $\lambda$ values larger and smaller than 1 are suboptimal as they lead to over-regularization and under-regularization in this case.&lt;/li&gt;
&lt;/ul&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot, credit to &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coef&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;n&quot;&gt;effective_rank&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train the model with different regularisation strengths
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Display results
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xscale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'log'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'weights'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Ridge coefficients as a function of $&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tight'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xscale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'log'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Coefficient error as a function of $&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tight'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:MA&quot;&gt;
      &lt;p&gt;Mardia, K. V., Kent, J. T., &amp;amp; Bibby, J. M. &lt;em&gt;Multivariate Analysis&lt;/em&gt;. 1979. Probability and mathematical statistics. Academic Press Inc. &lt;a href=&quot;#fnref:MA&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>菊与刀：三个月的研究写出了人类学最出名的书？</title>
   <link href="http://localhost:4000/reading/2020/08/02/The-Chrysanthemum-and-the-Sword.html"/>
   <updated>2020-08-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2020/08/02/The-Chrysanthemum-and-the-Sword</id>
   <content type="html">&lt;p&gt;时间是个奇妙的东西。第一次知道《菊与刀》还是在中学的课堂上，这书可能是推荐课外阅读丛书之一，那时的我对日本没什么特别兴趣，只依稀记得徐慧老师在讲台上介绍这本书的模样。十年之后，偶然再次看到此书，对樱花之国的历史和文化有了一定了解，读罢，感触颇多。&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 50%;&quot;&gt;&lt;img src=&quot;/assets/books/C_and_S_cover&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;书名：&lt;a href=&quot;https://book.douban.com/subject/26171368/&quot;&gt;菊与刀&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;原名：The Chrysanthemum and the Sword&lt;/li&gt;
  &lt;li&gt;作者：鲁思·本尼迪克特 (Ruth Benedict)&lt;/li&gt;
  &lt;li&gt;译者：胡新梅&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-背景&quot;&gt;1. 背景&lt;/h3&gt;

&lt;p&gt;鲁思是美国的一位人类学家，她关注民族文化的产生以及差异，认为对不同民族的了解与尊重是我们能和平共存的基础：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The purpose of anthropology is to make the world safe for human differences.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;美国加入二战后，鲁思被战时情报局(Office of War Information)聘为特殊顾问，专职研究各国国民性，包括日本。基于当时的工作写出的《菊与刀》(1946) 现在通常被认为是最畅销、最经典的西方战后日本人論著作。值得一提的是，《菊与刀》在日本和中国家喻户晓，几乎是“日本人国民性研究”的代言词，而在美国，这只是鲁思职业生涯中的一个工作，读者大多局限于亚洲研究学者。&lt;/p&gt;

&lt;p&gt;同样一本书在太平洋的两岸有着完全不同的回响，这是为什么？是否跟读者的身份有关？是否跟这书出版的时间有关？自出版以来，对《菊与刀》的评价也从来没少过，有人称赞鲁思工作的开创性，也有人质疑她研究的准确性，毕竟她从未去过日本。这些问题待我们看过本书的内容之后再来聊。&lt;/p&gt;

&lt;h3 id=&quot;2-框架&quot;&gt;2. 框架&lt;/h3&gt;

&lt;p&gt;作者在第一章定义了问题和使用的分析方法，在第二章谈论了日本人的战争观，当时的美国人接触到日本大多是通过战争，这一章的内容是作为引子来带出后面的论述。第三、四章讨论了日本社会信奉的等级制以及它在明治政府所做社会改革中的体现。之后鲁思详细说明了形成日本人人生观的各种道德准则（第五到第九章）、他们面临的道德困境（第十章）、对自我修炼的执著（第十一章）以及对子女教育的观察（第十二章）。最后一章是作者对日本战后社会的预测。&lt;/p&gt;

&lt;h3 id=&quot;3-方法&quot;&gt;3. 方法&lt;/h3&gt;

&lt;p&gt;作者在第一章提出了她的研究问题，说明了这不是一本介绍日本社会和历史的书：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;因此，本书并非一本专门论述日本的宗教、经济、政治或家庭观念的书，而是考察日本人在日常生活中的
一些固有观念，从而探讨日本民族特性形成的原因。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在作者看来，固有观念是依据，民族特性是议题。所以，跟其他学术研究一样，本书有它设立的假定，有它问题的局限性，所使用的研究方法也不一定完美。我们大可不必把它看做是绝对权威的日本研究，以客观、尊重的眼光来评判它，比较合适。具体来说，作者的研究建立在这样一个假定之上：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…即认为那些最孤立的细小行为之间也一定存在着某种系统性的联系。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;带着这样的信念，作者主要从两个方向着手，一个是研究日本与亚太地区其他国家（主要是中国）之间的文化相似性和差异性，另一个是研究日本人的日常琐事。而鉴于战时无法前往日本生活，作者当时研究的材料主要分三种：与居住在美国的日本人访谈，关于日本的已有文献资料以及日本人创作的电影。我们可以记住作者的假定、方法以及材料，因为这些也是后人对本书或褒或贬的评价的焦点。&lt;/p&gt;

&lt;h3 id=&quot;4-等级制&quot;&gt;4. 等级制&lt;/h3&gt;

&lt;p&gt;作者认为，日本发起侵略战争是因为他们相信等级制，欲在大东亚建立新的秩序，让各国“各安其位”。在这里的分析中，作者准确地捕捉到了天皇在军事行动中的角色：军人的精神领袖与让行动正当化的理由。因为天皇就是日本的象征，军人们认为上战场就是“对天皇意志的效忠”。&lt;/p&gt;

&lt;p&gt;那等级制只有在军队里重要吗？不，恰恰相反，作者认为日本人信奉的等级制体现在社会中的方方面面，比如社会阶级、家庭成员、年龄与性别，在这些语境里都有明确的等级区分：皇室高于平民，父亲高于孩子，长辈高于晚辈以及男人高于女人。基于这样的观察，作者总结说&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;日本人对于等级制的信赖是他们处理人与人、人与国家关系等一切关系的基础。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里我们实际上能看到两个被后人批评的点：作者常以“日本人”开头，暗指所有人都一样，这样的论述有过于简化的嫌疑；而“一切”这类代表绝对程度的词更是在学术研究中极少见到，批评者只需找出一个反例便能让你在问答环节瞬间尴尬。&lt;/p&gt;

&lt;p&gt;不过我倒是可以想到一个说明等级制观念渗透于日本社会的例子：日语。在日语里，根据说话人双方的等级，所用的词语要相应改变，比如当我在跟上级讲话时，我需要使用所谓的尊敬型词语形式，而跟我的同辈则不用，甚至我可以使用“向下”型跟小孩儿讲话(さしあげます/あげます/やります)。上级包括老板、客户、长辈以及任何我“应该”以尊敬语气交谈的人，也就是说，小孩儿们从语言学习开始就被教导着人分等级，以及该如何去判断说话双方的等级。这并不让人惊讶，毕竟语言往往承载着这社会的历史、习惯与认知，学习语言也就成了继承这些传统的方式。&lt;/p&gt;

&lt;p&gt;有意思的是，作者观察到等级高的人并不能为所欲为，他们受到尊敬的前提是有尽其职责。在谈到家长在家庭中的地位时，她用了一个很形象的比喻：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;日本人从家庭生活经验中了解到只有符合家庭共同利益的决定才能得到认可，任何人都不能强制执行，包括一家之长，日本的家长更像是全家人的物质和精神财产管理人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为什么在很多人看来等级制无处不在、人的行为被无数框架约束着的日本社会能高效运作，在战后经济腾飞，成为为数不多的亚洲发达国家？作者知道对于她的美国读者来说，这可能很难理解，因为美国社会看上去似乎没那么多等级差异也更崇尚个人主义。她认为这是因为日本人习惯“行为准则被细化、地位被明确”的社会：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;虽然生活中要遵守各种繁琐的细则，但对日本人来说，只有这样，他们的生活才能有序进行，才会有一种安全感和归属感。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;没在日本生活过、不懂日语也并非日本研究出身的鲁思，能在短短几个月之内就抓住一个又一个日本社会的独特样貌。对这个让生活“有序进行”的“安全感和归属感”我也深有感触。在日本生活了一段时间之后我发现，不同岗位上的人们矜矜业业，进便利店总有店员对你的欢迎，火车总是准点，拉面店里空间狭小但你旁边的大叔总会整齐放好他的背包以免给你造成不便。身在其中，我时不时会有种错觉，会以为这是个精密无比的机器，而每个零件都在全力履行着他们的职责，像萨特所描述的咖啡店里那个太想把自己变成服务员的&lt;a href=&quot;https://existentialcomics.com/comic/101&quot;&gt;服务员&lt;/a&gt;。不过，我确实感觉得到安全感，万物以已知的规律在展开；而当我也开始遵守这些细则时，我感觉成了这社会的一员。&lt;/p&gt;

&lt;h3 id=&quot;5-明治政府&quot;&gt;5. 明治政府&lt;/h3&gt;

&lt;p&gt;倒幕成功之后组建起来的明治政府做出的各行各业的改革充分体现了日本人对“各安其位”的等级观念。新政府当时派出伊藤博文前往欧洲各国取经，想要从立法等层面着手建立新日本。在当时的英国博学家斯宾塞(Herbert Spencer)与伊藤博文的信件来往中，他提到&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…日本人对长者的传统义务以及对天皇的义务都是日本独特的文化。日本将在“长者”的带领下避开那些崇尚个人主义国家所面临的一些不可避免的问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对长者的义务或许不是日本独有，但对天皇的义务确实如此。Spencer是19世纪末最有影响力的英国博学家，他首先提出了“适者生存(Survival of the fittest)”的概念，后期也以支持社会达尔文主义“出名”。虽然在20世纪以来他已经失去了在西方世界曾经的学术地位，但他在日本与中国仍然有着很大的影响。明治新政府也很满意Spencer的回信，这样他们就能将天皇等反映等级制的社会元素保留下来。在新议会政治中，天皇可以任命高层人物，上议院的影响力则远高于下议院。在宗教上，神道作为国民信仰已然是超越宗教的存在，其神职人员有着“贵族”的地位，而其他宗教人员则没有。&lt;/p&gt;

&lt;p&gt;而另外一个新政府下出现的变革是军部的独立。当时的宪法虽然没有明文规定，但是军部可以在一定程度上独立于内阁做决定甚至直接影响内阁，比如他们可以“拒绝委派陆海军将领担任内阁中的职务”。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;…人们对军部的做法通常采取接受服从的态度。并不是因为他们赞同这些做法，而是他们早已习惯了这种等级关系，也不赞成逾越特权界限。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;而民众对军部也是尊敬的，这或许与早期军人许多曾是武士有关，也与他们宣称是天皇的部队有关。总之，这种人民的支持和内阁对军部有限的牵制能力为之后二战时期日本变成军国主义国家埋下了伏笔。&lt;/p&gt;

&lt;h3 id=&quot;6-受恩与还债&quot;&gt;6. 受恩与还债&lt;/h3&gt;

&lt;p&gt;日本是个非常强调个人义务和责任的社会，这也是我去到那边之后能很快感到自如的原因。而在这样的环境里，作者观察到，“爱”跟义务画上了等号：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;从某种意义上说，日本人认为“爱”其实就是一种应尽的义务，一种对恩情的回报。而美国人却认为“爱”是一种真情的流露，是自愿给予的、不求回报的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;虽然这两种理解看似处在天平的两端，但我认为“爱”是包括了责任与情感。一个更准确的说法可能是，日本社会更强调“爱”当中的责任，而美国则更强调情感。本书的研究来自于作者在八十年前对当时的资料以及更早的历史的考察，这些观察不一定也适用于现在的日本。总的来说，人会受身处大环境的影响，尤其在成长期，日本社会养育出看重亲密关系中责任的人，但同时也会让其他不适应者想要逃离吧。&lt;/p&gt;

&lt;p&gt;如果说来自于“爱”的义务沉重但没那么频繁，那“陌生人的恩惠”则相反。这样的恩惠会时常发生，而在很多日本人心中，接受了别人的恩惠就必须要回报，所以作者发现他们会常为此类恩惠感到烦恼。比如说，日语里有许多表示感谢的词语，当中很多是用来表达接受帮助时的“不安心情”。&lt;/p&gt;

&lt;p&gt;刚去日本没多久的时候，有个词让我有点摸不着头脑 - すみません，字面意思是“抱歉”，像是英文的“execus me”，而“谢谢”的日语直译是“ありがとう”。我发现大家用这两个词的习惯跟我很不一样。比如说，在路上捡了别人掉的毛巾还给他，他不会说“ありがとう（谢谢）”，而是说“すみません（抱歉）”；公交车特意多等了会儿之后上来的乘客对司机说的不是“ありがとう（谢谢）”而是“すみません（抱歉）”。后来我明白了，承受了别人恩情（尤其是陌生人）的时候，大家习惯对别人表示抱歉，来表达自己受到恩情而又可能无法偿还的不安心情。&lt;/p&gt;

&lt;h3 id=&quot;7-社会的压力&quot;&gt;7. 社会的压力&lt;/h3&gt;

&lt;p&gt;大家都知道高自杀率是日本一大社会问题，不少日本国内国外的学者、记者、公民团体等都致力于解决这个问题。在一个平均寿命最长、医疗体系发达、山清水秀的发达国家，为什么有那么多人想自杀？这本书的后几章我觉得也给出了一些答案。&lt;/p&gt;

&lt;p&gt;第一个因素就跟报答恩情有关。作者发现，日本人把恩情分为两类，第一类是无法还清的恩情：忠与孝；第二类是需要在一定时间之内偿还的：欠世人的情义与对自己名誉的维护。日本社会把人们是否尽力偿还恩情看得很重，有的人甚至把“不能报答情义的人视为是人格上破产”。虽然这类社会关系与期待带来的心理负担不是日本社会仅有，但他们的过分强调会使得普通人无时无刻不处在巨大的压力之下：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;总之，人们在履行“情义”时总是感到很为难，感到不情愿，因为“情义”成为人们日常交往中所背负的沉重的心理负担。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;另外一个因素则是日本人对维护自己名誉的追求。作者在许多历史文献与影视作品中发现，尽其所能去澄清谣言、维护名誉的事情是高尚的，是一种被推崇的美德。而同时，日本人看上去习惯把某人所从事的工作与他本人紧密联系起来，对他工作的评价既是对他本人的评价。所以可以想象，当我的工作受到批评，我会觉得这是对我这个人本身的批评，从而一个正常的失误可能就变成了我质疑自己生命价值的灵魂拷问，这样的生活能不压力大吗？&lt;/p&gt;

&lt;p&gt;而这些压力会被另外一个元素加剧：羞耻感。在日本社会长大的孩子往往羞耻心会比较重，作者发现&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;日本人之所以把谨慎与自重划等号，是因为他们总是感觉别人在注视评价自己。…因此，这种以羞耻感为基础的文化更看重别人的评价。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;随时都在想着自己在别人眼中的样子，一直做一个“被注视”的人，虽然这种来自他人的原初体验在萨特看来是我们与这个世界交流所必不可少的，但如果体验只有“被注视”，总是担心自己的行为是否会给自己带来负面评价，这样的生活肯定很累吧。一个例子就是明星所面对的24小时360度全民评价，就在今年五月，日本的摔跤手木村花在参加了一档非常出名的真人秀(Terrace House)之后，受到了连续几个月的网络暴力无法承受而&lt;a href=&quot;https://www.nytimes.com/2020/07/17/arts/television/terrace-house-suicide.html&quot;&gt;结束了自己的生命&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;8-结语&quot;&gt;8. 结语&lt;/h3&gt;

&lt;p&gt;二战战败之后，日本人开始了对自己身份的搜寻，而《菊与刀》作为一本美国学者的著作刚好能让日本人从中去搜寻自己与美国人的区别，如何在美国政府的介入下重建日本。学界对本书的批评从来没有消失过，主要批评的点有三个：书中很多被冠以日本民族性的特征实际上是基于军国主义时期的军队，而基于军人的研究并不能代表普通日本人；不过这源自于本书采用的方法的问题，鲁思从头到尾都把日本民族当做了整齐的一体来描述，然而同一民族也会有多样性，本书没有照顾到这一方面；最后一个是鲁思书中的单一不变的民族性，对时间的元素于民族特征的影响没有讨论。&lt;/p&gt;

&lt;p&gt;不过，就像开头所说，本书有设立假定，有作者作为美国人类学家的视角限制，也有她为了写成一本书而对内容所做出的取舍，《菊与刀》带起来的对日本社会、日本人的研究热潮是这本书实实在在的贡献，后来的研究中有去对她的观点做进一步阐述的，也有做有理有据反驳的，但本书不断被引用，也在中国和日本继续热销，可能有很多跟我一样的年轻朋友，读到的第一本非中国视角的关于日本的书籍就是《菊与刀》，这种经久不衰的影响力着实很让人惊叹了。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>2020年6月：像做talk show一样去做学术报告</title>
   <link href="http://localhost:4000/journal/2020/07/10/journal-2020-06.html"/>
   <updated>2020-07-10T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/07/10/journal-2020-06</id>
   <content type="html">&lt;p&gt;6月的新加坡结束了封城，而我也在这个月的最后一天收到了&lt;a href=&quot;https://www.researchgate.net/publication/342652506_A_Control_Chart_Approach_to_Power_System_Line_Outage_Detection_Under_Transient_Dynamics&quot;&gt;文章&lt;/a&gt;被接收的邮件。&lt;/p&gt;

&lt;p&gt;博一开始就在与不同老师的交流中了解到，科研与发表不是简单的因果关系：做了厉害的科研工作就能在顶级期刊上发表研究结果，这俩更像是有一定相关性但也分别独立于彼此的事件，最简单的例子就是我可能可以控制某个工作投入的时间，但无法控制谁是审稿人、审稿人的学术品味等等。在我很小的博士生圈子里，一篇文章被修改两三次、回复审稿人意见的信件比原文章还要长、修改期间所做的工作量不比原文少等这一类的经历也是没少听说，所以当我的第一篇文章在经过一轮修改之后便被接收时，心里虽然很高兴，但总觉得这次多少也走了些运吧。&lt;/p&gt;

&lt;p&gt;不过话说回来，give credit when it’s due，文章被接收是一件值得庆祝的事情，主要得益于老板和超哥在研究与写作方面给我的指导。这也是一种来自同行的认可，说明我们做的东西有价值，毕竟之前读到一篇关于researcher的心理状态的文章也说过了，researchers tend to be their own worst crtics：我们总是能在自己的研究里找到不足，或许是我对这个问题的重要性其实并没有那么自信，亦或是这个方法的效果在某些时候其实并不那么稳定等等。&lt;/p&gt;

&lt;h4 id=&quot;广告广而告之&quot;&gt;广告，广而告之&lt;/h4&gt;
&lt;p&gt;这个月的月志，主要是想记录一下给我们&lt;a href=&quot;https://frs.ethz.ch/&quot;&gt;FRS研究组&lt;/a&gt;所做的一次学术报告(presentation)的经历。&lt;/p&gt;

&lt;p&gt;上一次在FRS做报告还是一年前，那个时候其实自己并没有太多具体的东西可以讲，并且当时的形式是每个人three minutes, three slides，无法深入聊。今年研究组换了leadership，开始了半小时周会报告的分享形式，再加上新项目来了很多新的面孔，6月中旬轮到我分享时，我就想着趁这个机会把我的研究整个介绍一下。&lt;/p&gt;

&lt;p&gt;我的研究集中于统计方法在电力系统的应用，来参加分享会的人包括博士生、博后、教授等涉足不同领域也有着不同的背景知识，只有少数人在电力方面做研究，可能更少的人了解工业统计。那么问题来了，我应该如何照顾到观众去设计分享报告的内容？如果我一上来就一个劲儿地介绍电力系统和统计方法，可能不到三分钟就会失去大部分观众的兴趣。认清这次分享的&lt;strong&gt;观众群体&lt;/strong&gt;与他们已有的&lt;strong&gt;知识背景&lt;/strong&gt;之后，我就可以定一个总的目的了：让观众对我研究的问题、问题的重要性以及所涉及的方法有一个直观的理解(intuitive understanding)。&lt;/p&gt;

&lt;p&gt;下一个我会想到的问题就是，如果观众从我的分享中只记住了一件事儿，这事儿是什么？换句话说，我整个分享的&lt;strong&gt;中心思想&lt;/strong&gt;是什么。定好这样一个key takeaway之后，就可以根据它去设计分享的内容了：所有内容都要服务于这个中心思想，如果不相关，那就要果断删去。&lt;/p&gt;

&lt;p&gt;回到照顾观众的问题，我在思考要如何解决观众的知识背景与我的研究之间的差异问题时，一直想找到一个合适的类比(analogy)。介绍陌生的概念时，如果用观众熟悉的事情做类比，往往能一下子让大家有一个直观的理解。实际上，这样的“套路”在各类talk show里经常被用到，尽管大多数时候是带着戏谑成分的类比。&lt;/p&gt;

&lt;p&gt;沿着这个思路，我越发清晰地认识到，我所做的电网断线监测研究与传染病大流行有几个非常相似的地方：他们都能迅速从一个case变成一个cluster；他们出现时都会有些异常的“信号”；而且这些“信号”一开始会很微弱，以及不完整。Good!虽然不是每个人都熟悉电网，但是我相信大家应该对新冠疫情不会陌生了。所以后来在我的分享里，几乎每一个关键地方，我都会用新冠疫情作为类比来解释。从我得到的一些反馈来看，这个类比应该还是发挥了一些作用了。&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/assets/month-journal/presentation-2.png&quot; alt=&quot;presentation-2&quot; /&gt;
  &lt;figcaption&gt;这里我把研究问题中的挑战与对抗新冠疫情的挑战做了类比。&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;整个分享我花了最多的时间去聊问题的意义，目前大家面临的挑战，以及我们的研究能解决哪部分的问题，中间穿插着各种跟新冠疫情的类比，确保观众能对我说的东西有一个直观的理解。聊的时间最少的是实际工作中费时最久的部分，比如方法、仿真和代码等等，毕竟我的目的是让大家对这问题有所兴趣，而不是只把我们写的文章读出来。&lt;/p&gt;

&lt;p&gt;值得一提的是，我在写分享会的稿子时，特意在某一页的内容里设计了一个joke，想尝试一下像talk show一样营造一下轻松的气氛。然而，在实际分享过程中，不知道是因为脱稿还是因为这事儿对我来说实在太新，竟然完完全全忘记了说这个joke，最后讲完才想起来，实在有点小小的遗憾。行吧，下次再试一次。&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>一件“小事”：Discrimination其实离我们很近</title>
   <link href="http://localhost:4000/journal/2020/06/14/journal-discrimination.html"/>
   <updated>2020-06-14T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/06/14/journal-discrimination</id>
   <content type="html">&lt;p&gt;记录一件“小事”。&lt;/p&gt;

&lt;p&gt;昨天跟女朋友坐车去超市，坐在了靠近下车处的第一排两人座位。一路上跟往常一样聊天，说这说那的，坐了大概二十分钟快要到站的时候，一位带着新加坡口音的短发阿姨突然从我们后面走出来，站在打卡下车处，刚落脚就开始对我们讲话，阿姨说：“Are you from China? Don’t you watch our news? You don’t talk (Refrain from talking)  on public transport!” （中文：你们是中国来的吗？你们不看我们的新闻吗？不要在公共交通上讲话！）  因为都戴着口罩，后一句是我不大确定的部分，但是前两句我听得很清楚。这阿姨出现得突然，我们一时间没反应过来，车门打开后，她立马就打了卡下了车，走出车门的同时，还一边看着我们一边用手在自己的太阳穴旁画圈，意思是我们脑子有问题。片刻之后，等我们反应过来，下车去看，她已经走出很远。&lt;/p&gt;

&lt;p&gt;一时间我们俩都被复杂的情绪包裹着，说不出话来。我们心里感到有些抱歉，因为政府确实有宣传，结束封城之后，大家在公共交通上不要讲话，这确实是我们疏忽了；但同时，我想我们俩都感到了一种被侮辱的心情，因为她的第一句话和下车时的手势。如果她只是为了大家的安全着想，完全可以在我们说话时过来提醒我们，甚至大声斥责，也恐怕可以理解。但是，为什么她选择在下车前一秒跟我们说，然后快步走开？为什么第一句话是问出“你们是不是中国来的？”这样的问题？这背后是否带着一种对中国人的隐形的歧视或偏见呢？如果我们是日本人，或者英国人，她也会问出一样的关于国籍的问题吗？&lt;/p&gt;

&lt;p&gt;新冠疫情刚爆发时，亚裔在很多国家遭受到了非暴力和暴力的攻击；最近因为弗洛伊德的事件，反抗种族歧视、警察暴力的抗议活动席卷全美，以及其他被殖民过或深受殖民时期影响的国家。美国的种族歧视问题有长久的历史原因，是制度性的、让人呼吸不过来的，但我对这些了解最多来自一些文字、影像和媒体记录，实际上无法真正体会到作为一个非裔美国人，在那样的环境中长大、生存是怎样的经历。&lt;/p&gt;

&lt;p&gt;新加坡也曾经是英国的殖民地，从古至今的重要贸易交汇点聚集了来自不同地区的人，种族之间的对抗和摩擦也一直存在。最近的一次大规模冲突事件发生在1969年，在那之后政府和民众很努力去维持多种族、多国籍、多宗教的社会的和谐，也没再发生过跟种族或宗教相关的严重的抗议游行事件。但，这不代表隐性的歧视不存在了，昨天的事情恰恰说明这样的偏见其实离我们很近。我觉得我有义务把经历的这一件“小事”说出来，因为太多的沉默让类似的事情一再发生。另外，一个人恐怕很难改变别人的既有偏见，甚至很多时候会越描越黑，但我们可以尽量不用有色眼镜去看待其他种族、信仰、出身地、甚至是任何与我们背景不同的人。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>2020年5月：浅尝Snack Writing</title>
   <link href="http://localhost:4000/journal/2020/06/06/journal-2020-05.html"/>
   <updated>2020-06-06T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/06/06/journal-2020-05</id>
   <content type="html">&lt;p&gt;五月因为疫情的原因继续在家里工作。这段时间工作重心主要在写东西上面，不太需要跑大量的数据、也不需要太多跟同事老板的交流，虽然我蛮想念办公室的espresso咖啡和时不时不同领域的人过来做的seminar的，这两件事儿家里没法复制。&lt;/p&gt;

&lt;p&gt;提交了上一篇文章的审稿意见回复，两个审稿人提了不少问题但都是建设性或者澄清类的，我写的回复初稿被老板修改不少（First version is always subpar.)。我发现我的回复往往更婉转、迂回，用词也习惯性地用我熟悉的这个工作的语言；而对比老板修改过的版本，表达的点是一样的，但这个点在他的回复里就很明确、更assertive，同时也把那些更技术性的词语换成了更易理解的。这么一想，每次做报告的问答环节我也是这样。最后提交上去的回复，给我的感觉是：同意的点会明确说、不清楚的点会用浅显的语言解释、误解了的点会用不卑不亢的姿态澄清。最后根据审稿人意见做过修改的文章，确实是要比之前的版本更完整、可能造成误解的地方更少了。&lt;/p&gt;

&lt;p&gt;下半月把之前好不容易搞懂的方法用$\LaTeX$一一打出来写进了自己的笔记里，然后又基于这个笔记写了第二个工作的文章的初稿（主要是方法部分），包括电力系统的动态模型、用particle filter做状态估计以及如何用EM做参数估计。方法里面自己花时间最久的应该是最后那个参数估计的方法，上一篇月志也提到过，在笔记里的篇幅也是最长的，因为各种步骤。不过，因为不是这个工作的重点，最终写到文章里就只有几段话。这道理，博一的时候&lt;a href=&quot;https://www.eng.nus.edu.sg/isem/staff/ye-zhisheng/&quot;&gt;叶老师&lt;/a&gt;课上就已经跟我们分享过了，写文章的时候要&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Write around your contributions, not about what you have learned.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这一次写，尝试了一个新的写作模式：每天早上起床后写一个小时任何我现在在写的、在学的东西。缘由是我回去再读了一下上个日志说到的那本书，里面有专门写到研究生应该如何写作。这里的“如何写作”不是指的写作风格、语言组织之类，而是指什么时候写，写些什么。书中主要提到&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Don’t write when ready, write to be ready.&lt;/li&gt;
  &lt;li&gt;Don’t be binge writing, be snack writing.&lt;/li&gt;
  &lt;li&gt;Schedule regular sessions for writing.&lt;/li&gt;
  &lt;li&gt;Writing means writing, not editing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;确实是这样。我经常觉得得等我所有方法都完全搞懂了，每一个细节都走通了，才能动手开始写；而且因为经常是在短时间内写大量的内容，写完总会感觉很累；有的时候也会发现花了时间却只写了几句话，原来是时间都用去做编辑$\LaTeX$符号、插入文献这种事了。&lt;/p&gt;

&lt;p&gt;所以我给自己定了个新的写作模式：早上起床后专注写一小时的内容，内容是接着昨天写的，并且只有写作；白天的时候再来慢慢修改字符，添加文献和检查错别字之类的编辑工作。这样的模式写了大半月，发现写作进度确实很快（相对以前），一点一点把之前大多只是停留在草稿本和代码上的工作都系统地写进了笔记，然后又根据笔记写了文章的初稿，在写的过程中也把很多细节的东西捋清了不少。不过，我觉得用这样的写作模式有一个要求，那就是得提前打好草稿：具体内容、提纲、每一部分的核心内容等自己得知道，不然的话，早上一小时很难写出东西，即使写出来了，大部分内容我后来都想改掉重写。&lt;/p&gt;

&lt;p&gt;为什么要把学过的东西像书一样写进笔记里呢？因为前面写到的写作模式和需要写文章的原因，开始寻思着能不能把花时间搞懂的东西以系统的、我自己能明白的方式写进笔记？当然可以，但问题是为什么要这么做？毕竟学术文章所需的内容和行文风格跟笔记还是很不一样的，这就意味着更多的写作工作量，$\LaTeX$写起来也没那么轻松。其实这事儿前两年也想过，主要还是想建立一个属于自己的知识体系并能不断修改和扩充它吧。&lt;/p&gt;

&lt;p&gt;前段时间因为写“&lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/05/17/francis-galton.html&quot;&gt;Francis Galton: 维多利亚时代的博学家与他观察到的奇妙世界&lt;/a&gt;”，在豆瓣搜索关于Galton的文章，读到了于淼的博客，起初惊叹于他的笔触间渗透着生物、数学、统计和编程等方面多年积累的知识以及对于科研和这个世界的一些独到的见解。了解之后才发现这位来自中科院的博士在环境科学方面做研究，16年从中科院毕业的他算是超哥（我的co-author）小三届的师弟。从时间线上看得到，读博以来这么多年，一直有在博客上写讨论各种问题的博文，大多是基于观察、学习与研究的议题。同时这位哥们热衷于上网课读教材，更爱做笔记整理知识，目前他的博客上有这么多年积攒下来的知识笔记的&lt;a href=&quot;http://yufree.github.io/notes/index.html&quot;&gt;汇编&lt;/a&gt;，最后我发现，他还是统计之都的编辑部主编。厉害！&lt;/p&gt;

&lt;p&gt;大概就是想尝试做这个事儿吧，一个一个对学过的东西做系统性的整理，梳理框架然后写成相对完整的笔记，对框架里的每个部分逐渐形成自己的理解；在之后的扩充过程中既能找到新知识在框架中的位置，也能不断更新自己的框架和理解。&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Francis Galton: 维多利亚时代的博学家与他观察到的奇妙世界</title>
   <link href="http://localhost:4000/data/2020/05/17/francis-galton.html"/>
   <updated>2020-05-17T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/05/17/francis-galton</id>
   <content type="html">&lt;p&gt;周末读Aeon的一篇文章：&lt;a href=&quot;https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;amp;utm_medium=email&amp;amp;utm_term=0_411a82e59d-f7c118f081-69607277&quot;&gt;Algorithms associating appearance and criminality have a dark past&lt;/a&gt;，讲现在有研究人员用机器学习算法通过人脸来判断某人犯罪的几率。文中讲到这种从人外表提取预见性特征的尝试，在犯罪学历史上并不新奇，19世纪的意大利犯罪学家Cesare Lombroso认为罪犯的脸部有独特的样貌：突出的前额、鹰型鼻梁；而18世纪的Francis Galton则尝试回答一个更广泛的问题：人的外表跟他或她的健康状况、犯罪倾向、智力等等有关系吗？或者说，人的基因是否决定了健康、行为、智力和竞争力？&lt;/p&gt;

&lt;h3 id=&quot;francis-galton是谁&quot;&gt;Francis Galton是谁？&lt;/h3&gt;
&lt;p&gt;这名字看起来有点眼熟，我隐约记得在老板的一门Forecasting统计课上听到过。仔细一想，对，在线性回归的部分，老板上课专门介绍了他。Sir Francis Galton，姓Galton，名Francis，但当提到他时，出于礼仪，你得加个Sir，因为他在1909年被英国女王授予了骑士爵位。为什么在讲线性回归的时候要介绍他呢？因为他作为第一个人，观察并记录了这样一种现象&lt;sup id=&quot;fnref:Galton_heights&quot;&gt;&lt;a href=&quot;#fn:Galton_heights&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;：平均身高很高的父母，往往会有身高更接近普通的孩子；而平均身高偏低的父母的孩子，成年后通常有着更接近普通人的身高。
下图是&lt;a href=&quot;https://www.ams.org/journals/bull/2013-50-01/S0273-0979-2012-01374-5/S0273-0979-2012-01374-5.pdf&quot;&gt;Bradley Efron&lt;/a&gt;根据Galton当时收集到的父母和孩子的身高数据重新制的图，完美地展现了我们现在所知道的Bivariate normal distribution。
&lt;img src=&quot;/assets/francis-galton/regression_to_mean.png&quot; alt=&quot;regression_to_mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;他把这种现象称为&lt;a href=&quot;https://www.jstor.org/stable/2841583&quot;&gt;regression towards mediocrity&lt;/a&gt;，现在通常叫做regression toward the mean，中文貌似叫“向均数回归”。同样的现象，我们在生活中很多地方都能观察到：因为运气而押中题目的学生考出了高分，下一次考试的成绩却没那么突出；连续投中三个三分球的朋友，下个球往往“容易”失手；我上周做&lt;a href=&quot;https://yangxiaozhou.github.io/learning/2019/01/01/recipe.html#%E6%B2%B9%E6%B3%BC%E7%8C%AA%E6%89%8B&quot;&gt;油泼猪手&lt;/a&gt;时各种调料拿捏得很好，味道超棒，这周再做一次，大概率味道会比较普通🤷‍♂️。&lt;/p&gt;

&lt;p&gt;符合这原则的现象，他们有一个共通点：他们的结果往往完全或部分由随机因素决定，而随机因素的影响往往符合以0为中心的正态分布（时好时坏）。比如说，三分球进或不进，有投手能力的影响但也有运气的成分；我做的某道菜的味道，取决于下厨能力，但我的专心程度、手抖程度以及心情等几乎随机的因素也会有所影响。也就是说，假设某一天我超级走运，做出了迄今为止最好吃的一道菜，这种事件发生的概率是很小的（得到正态分布上的极大值或极小值的概率）。下一次做，大概率我会正常发挥，菜的味道也没上次好（取到了正态分布上0周围的某个值）。&lt;/p&gt;

&lt;p&gt;想象这样一种情况：朋友在我搬新家的时候来家里吃饭，刚好碰到我前面说的超常发挥，都说做的猪手好吃！过了几个月，家里聚会，应朋友强烈要求，再次做出一盘猪手，不过这次是正常发挥。朋友吃后回忆起之前，评论到：“水平下降了呀！” 我冤不冤？ 这样的冤枉我们生活中还真不少，以至于它有个专门的称呼：Regression fallacy，中文叫“回归谬误”。Daniel Kahneman讲过亲身经历的这样&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292229/&quot;&gt;一个例子&lt;/a&gt;：他有一次给飞行员学校做培训，提到了表扬能使学员变得更优秀。下面的一个教官不同意了，说他每次一夸完降落做得简直完美的学员，下一次一定做得没那么好，而刚被他骂过的学员，马上就能看到提升。听了教官的抗议，Kahneman当下有了一个eureka moment，他说道：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;回归谬误可以用数学证明，假设两个变量以bivariate normal distribution分布，只要他们的correlation小于1，就会有回归谬误出现，对证明感兴趣的朋友可以看&lt;a href=&quot;https://en.wikipedia.org/wiki/Regression_toward_the_mean&quot;&gt;维基&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;年度公牛体重竞猜&quot;&gt;年度公牛体重竞猜&lt;/h3&gt;
&lt;p&gt;让我们回到观察小天才Galton。1907年三月的自然杂志上刊登了他一篇篇幅只有一页的&lt;a href=&quot;https://www.nature.com/articles/075450a0&quot;&gt;来信&lt;/a&gt;，名为：Vox Populi，直译为“民众的声音”，现在指大多数人的意见。住在英国Plymouth的他，注意到了家附近的镇子上每年都有举办这样一种家禽体重竞猜活动：主办方拉一头牛出来，参与竞猜的本地农夫、屠夫等感兴趣且有经验者对牛进行评估，并将他认为这头牛被宰杀洗净之后的体重提交上去。本着对大众智慧的科学研究态度，他通过某种方式获得了一次竞猜比赛中的数据：牛的真实体重以及787个竞猜者的估计。&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 100%;&quot;&gt;&lt;img src=&quot;/assets/francis-galton/stock_show.jpg&quot; alt=&quot;stock_show&quot; /&gt;&lt;/p&gt;

&lt;p&gt;他把提交的所有竞猜体重从小到大排列开，发现中位数（一半的数比它低，一半比它高，”median”一词就是他给取的）是1207磅，而那头牛的真实净体重是1198磅，也就是说，民众的判断在这里跟真实值只差了0.8%！&lt;sup id=&quot;fnref:correction&quot;&gt;&lt;a href=&quot;#fn:correction&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
在那个线性回归还不是所有数据分析课程的第一节课，数据科学也还不是一种职业的时候，Galton从787个竞猜体重中通过简单的手算看到了以平均值或中位数对真实值进行估计的准确性。现在我们知道了，sample mean is an unbiased estimator of the true population mean。&lt;/p&gt;

&lt;p&gt;Galton的观察没有停在估计的准确性上，他还想知道，每个人估计的误差有多大。他随即把所有估计与中位数的偏差画了出来，他发现，每一个有经验的“肉眼测体重者”所做的估计，从低估的到高估的，一系列的偏差与正态分布极为相似。也就是说，如果把真实净体重看做是这个采样分布的mean，那任意一个参赛者（有经验的）来估计，他的估计值将是以真实净体重为中心的正态分布而分布着的（绕口！）。Let’s try again. The estimate by any pair of trained eyes is distributed normally around the true dressed weight of the ox. 这里我们得提一个无数现代科学依赖的理论：Central limit theorem (CLT)。对，就是那个可以解释为什么正态分布在现实生活中如此普遍的理论。因为CLT，我们现在确切地知道，当样本量足够大时，样本平均值呈以真实值为中心的正态分布。所以，从年度公牛体重竞猜的真实数据上，他，Sir Francis Galton，看到了central limit theorem。&lt;/p&gt;

&lt;p&gt;附上他原稿里的跟理论正态分布做对比的图，横轴是百分位，纵轴是偏差：
&lt;img src=&quot;/assets/francis-galton/francis-galton-the-wisdom-of-crowds.jpg&quot; alt=&quot;francis-galton-the-wisdom-of-crowds&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;galton-board与柏青哥&quot;&gt;Galton Board与柏青哥&lt;/h3&gt;
&lt;p&gt;Galton对于这种没有征兆但又近乎定律般呈正态分布的偏差非常着迷，他把偏差呈现出来的图称为:The Curve of Frenquency，也就是我们现在熟知的样本偏差的正态分布图。为了展现这种偏差的正态分布（即，CLT)以及前面提到的回归谬误，Galton设计了一个令人拍案叫绝的装置：Galton Board，现在也叫bean machine，如下图：&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 75%;&quot;&gt;&lt;img src=&quot;/assets/francis-galton/GaltonBoard.png&quot; alt=&quot;GaltonBoard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;像不像我们小时候在街巷的小卖部里玩过的弹珠机？没错，他们的背后是同样的原理。实际上，风靡全日本的柏青哥也是用的这样的设计原理：弹珠从顶部落下，经过跟若干层的撞针的撞击，最终掉进最下面从左到右N个桶当中的某个桶里。因为我们并不知道弹珠在跟每一根撞针撞击之后是走左还是走右，所以某个弹珠的最终位置并不能提前知道（i.e. 随机事件，随机漫步，随机过程）。&lt;/p&gt;

&lt;p&gt;但！虽然单独一个弹珠的去向无法提前获知，但我们却有办法知道某个弹珠落入某个区间的概率。粗略来说，弹珠到达某一个桶的路线数量除以所有它可能走的路线，就是它进入某个桶的概率。比如，一颗弹珠想要到达最左边的区间，它只有一条路可以走：从第一层开始一直往左弹。算出其他区间的路线数和概率可以有很多方法，比如枚举（费劲）或用斐波那契数列（你也很能观察！），也可以根据Binomial distribution的probability mass function (pmf)得到（$n$是撞针的层数，$k$是桶的编号，$p$是弹珠撞击后弹左的概率）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{Pr}(X=k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}&lt;/script&gt;

&lt;p&gt;for $k = 0, \dots, n$.&lt;/p&gt;

&lt;p&gt;读到这里，了解CLT的朋友或许已经明白为什么这个Galton board可以展示呈正态分布的偏差了。CLT的一个特殊应用是证明当试验的次数($n$)足够大的时候，binomial distribution的pmf会跟正态分布十分相似。换句话说，当我们的Galton board足够大，同时扔下的弹珠足够多的时候，我们应该就能看到经典的正态分布Bell curve！Genius!&lt;/p&gt;

&lt;center&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/jiWt77xme64&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;hr /&gt;
&lt;p&gt;为什么我们说这个弹珠机也能展示之前提到的回归谬误呢？首先，让我们把刚才那几百个弹珠落下来之后呈现出来的分布记在脑海中。让我再次使用做菜的例子，假设落到最右端的弹珠代表着我做出了迄今为止最好吃的一道菜，因为不寻常地走运（弹珠掉入最右边的几率非常小）。现在，我把这颗弹珠拿出来，让它从顶部再一次下落（再做一次同样的菜），你觉得大概率会是掉在哪片地方？有多大概率再次到达最右端（做出同样高水平的菜）？&lt;/p&gt;

&lt;p&gt;呵，Life!&lt;/p&gt;

&lt;p&gt;对于众多弹珠看似随机、无法预测地落下，最后被某种魔力聚拢，一个挨着一个，逐渐呈现出美丽的正态分布的现象，Galton自己是这样描述的：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Order in Apparent Chaos: I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Law of Frequency of Error. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;结语&quot;&gt;结语&lt;/h3&gt;
&lt;p&gt;Francis Galton作为英国维多利亚时期的一位博学家，经历实在是太过丰富。自幼出生在富足精英的家庭，他是达尔文的表弟，年轻时继承了父亲的大笔遗产之后去非洲大陆探险，回国之后写成的游记成了畅销书。用他敏锐的观察力和好奇心，Galton研究了很多问题，有些没啥实际影响（最佳切蛋糕法、最佳沏茶法），有些却改变了众多领域接下来一百多年的发展。他做了早期的回归分析、提出了correlation的概念、将统计应用到遗传学、心理学，数理统计最重要的学者之一Karl Pearson是他的学生。同时，他为了得到数据，发明了问卷调查；研究天气，发明了第一张天气地图、开启了对气候的科学研究；提出了一种有效识别指纹的方法，对当时的法医学做了推动。哦，对了，正如我们开头所说，他也提出了一种根据不同人脸图像提取“平均特征”的方法。&lt;/p&gt;

&lt;p&gt;Galton所观察到的世界，让他有了很多疑问，他尝试用各种方法去丈量这个世界，并从看似混沌无序的现象中找到秩序和规律。我惊叹于Galton的观察力、跟随自己好奇心不断的探索与尝试以及对自己专业不设限的态度。文艺复兴人的精神劲儿可见一斑。好了，不多说了，我要去入手一个Galton board了。&lt;/p&gt;

&lt;p&gt;最后附上一个把Galton board解释得比我清楚得多、诙谐又幽默的哥们的&lt;a href=&quot;https://www.youtube.com/embed/UCmPmkHqHXk&quot;&gt;视频&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;注释&quot;&gt;注释&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Galton_heights&quot;&gt;
      &lt;p&gt;这里放上Galton自己制作的父母孩子身高回归图：&lt;img src=&quot;/assets/francis-galton/Galton's_correlation_diagram_1875.jpg&quot; alt=&quot;Galton_heights&quot; /&gt; &lt;a href=&quot;#fnref:Galton_heights&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:correction&quot;&gt;
      &lt;p&gt;后来的研究修正了Galton原稿的数据错误，当时那头牛的真实净体重应该是1197，而中位数估计应该是1208。在原稿中，Galton用中位数进行了真实值估计。不过，当时787个估计的平均数是1197。也就是说，平均数其实以零误差的表现估计到了真实值！ &lt;a href=&quot;#fnref:correction&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>2020年4月：更难的方法与有套路的交流</title>
   <link href="http://localhost:4000/journal/2020/04/30/journal-2020-04.html"/>
   <updated>2020-04-30T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/04/30/journal-2020-04</id>
   <content type="html">&lt;p&gt;这个月收到了稿子的消息，在尝试拼上最后一块问题的“拼图”，以及思考如何坐上司机的位置。&lt;/p&gt;

&lt;p&gt;一转眼又两个星期没跟老板（博士导师）讨论研究上的问题了。&lt;/p&gt;

&lt;p&gt;上周聊过关于回复审稿人意见的事，第一次投稿和收到修改意见，多少有点忐忑。学术文章发表的过程大致是投稿、编辑抄送给匿名审稿人、审稿人回复（推荐与否加上具体意见）、编辑回复（接受、修改、拒绝）。如果回复是修改，那可能会重复几轮这个步骤。可能是因为疫情的关系，去年十一月就投出去的文章，历时五个月，上上周才收到回复，并且只有两个审稿人。我打心里感谢这两位审稿人在这特殊时期还审我的稿，并且提的大多是寻求进一步解释或者论述性质的意见，而不是“质疑”型的问题。&lt;/p&gt;

&lt;p&gt;每周周二，是我们往常组会和见面讨论的日子。自从疫情在新加坡扩散以来，学校先是规定会议要测体温登记时间等等，不久就要求全员在家工作了。全校人员撤离校园的同时，所有跟新冠病毒相关的研究活动得到特殊允许，可以继续开展。我们这一类不用做实验有电脑就能工作的研究，在家工作没太大影响；Science、土木一类需要学校资源做实验的朋友们就没那么幸运了，不知道他们现在在家里如何继续科研，刚好集中精力整理数据写写文章？&lt;/p&gt;

&lt;p&gt;不知道是否是出于习惯，周二一上班就把最近困扰我的问题好好组织打稿一番发给了老板。这问题算是我目前工作的最后一块“拼图”吧。粗略来说，我模型里有个未知的参数要估计，但问题是线下（offline)还是线上(online)估计。线下估计需要一定数量的训练数据，也很难保证普遍性，万一有个没见过的情况，效果可能就不好；线上估计的适应性就强很多，也不那么依赖训练样本质量。但是，线上估计的方法我前段时间看了一下文献，没怎么看懂。当时急着想要看看整体方法的表现，姑且用线下学习估计了，初步结果看上去也不错。然而，这参数普遍性的问题在我尝试做大量仿真检验的时候不出意外地出现了！虽然心里知道这问题得解决，但真处理起这最后一块“拼图”时，我还是犹豫的。&lt;/p&gt;

&lt;p&gt;问题发给了老板，很快回复了我：可以线上估计，你看看xxx（几个思想的关键词，方便我去查阅）。得到回复的时候没有失望，反而是安心了一些，跟自己说，踏踏实实做吧。&lt;/p&gt;

&lt;h3 id=&quot;一个学期只联系一次研一都是放养的吗&quot;&gt;一个学期只联系一次，研一都是放养的吗？&lt;/h3&gt;

&lt;p&gt;前两天偶然看到豆瓣有人问了如上的问题，他/她看上去很困惑，说希望能跟导师有更多的交流。蛮多人回复说这事儿得看老师。确实，每个老板指导风格不同会有不少区别，不过这只是公式的一半吧，自己作为另一半能做的事儿也很多。&lt;/p&gt;

&lt;p&gt;我的老板也是中间偏“放养”型的，比较少会主动过问我进展如何，时不时会发点文章给我看，不过我想讨论的时候，也会认真指导。记得博一刚开始那会儿跟老板聊过关于学生跟博导的关系，他觉得读博做研究这事儿归根结底是我们自己的，学生应该把自己放到驱动者的角色；而导师的角色应该是一个advisor/mentor，会负责给评价、提建议、提供我需要的帮助，但并不负责帮我做事儿、监督我做事儿。对这，我是完全同意的，并且庆幸老板在一开始就跟我们说清楚、让我们有了合理的期待(manage expectations)。&lt;/p&gt;

&lt;p&gt;我后来意识到，既然要坐到司机的位置上，那就意味着得学习怎么开车。不是说的学习如何做数学推导，而是学习如何做研究生，其中很重要的一项就是如何跟老板保持沟通（尤其是“放养”型）。有些学校会给新入学的研究生提供类似培训的课程，但不是所有学校都这样做。在摸索的过程中，同研究中心的德国博后同事推荐了本小册子给我：“&lt;a href=&quot;/assets/month-journal/The 7 Secrets.pdf&quot; target=&quot;_blank&quot;&gt;The Seven Secrets of Highly Successful Research Students&lt;/a&gt;”。我们研究中心有点特殊，中心很多博士都是在瑞士招了然后来新加坡读的，没办法用到瑞士那边大学里的众多资源，他们的博导们也大多base在瑞士。这也是大家比较关心“如何跟老板保持沟通”，“如何有效管理自己的研究进度”等问题的原因吧。小册子里面给出了蛮多诚恳的建议。其中关于如何跟老板保持沟通，交流工作的部分让我也受益匪浅。小册子推荐跟老板交流工作时用这样一个模板：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;自从上次会议我做了：1..2..3..&lt;/li&gt;
  &lt;li&gt;新遇到的问题：1..2..3..&lt;/li&gt;
  &lt;li&gt;（老板的）评价与建议：1..2..3..&lt;/li&gt;
  &lt;li&gt;接下来要做：1..2..3..&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这模板用起来特别容易上手。我是按这样的顺序使用的：根据上次会议老板提出的评价与建议去解决我新遇到的问题，然后去做“接下来要做”的事儿。在这过程中，开启新的一页，并记录新的第一和第二点，然后重复。用这模板跟老板做每周的讨论快一年了，跟以前开会前一天匆忙准备讨论的点相比，能明显感觉到现在自己在讨论时思路清晰很多，比较少因为老板的发散性提问而忘记原本想要讨论的点。另外一个改变是，当我能参照自己的记录，比较准确地讲出这问题的来龙去脉时，往往能得到比较清晰的建议与评价。这样记录下来的笔记，日后往回查看也方便很多，尤其是想要查找老板曾经给出的“自相矛盾”的建议的时候😃。&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Tracking the COVID-19 outbreak and signals of containment</title>
   <link href="http://localhost:4000/data/2020/03/24/COVID-19.html"/>
   <updated>2020-03-24T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/03/24/COVID-19</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.&lt;/strong&gt; 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are &lt;strong&gt;interactive&lt;/strong&gt; and best viewed on a desktop browser.&lt;/p&gt;

&lt;h2 id=&quot;signals-of-containment&quot;&gt;Signals of Containment&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;Confirmed and Death Cases&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-interplay-of-confirmed-and-death-cases&quot;&gt;The Interplay of Confirmed and Death Cases&lt;/h3&gt;
&lt;p&gt;When should the economy reopen? To try to answer this question, we could look at the interplay of new confirmed cases and death cases.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1739652220&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;More cases means more healthcare resource demand, and doctors and nurses have to make tough decisions. Unfortunately, more patients who need intensive care might not get it, leading to higher fatalities. We are probably going to see a peak in daily cases, and after some time, a peak in daily fatalities. This phenomenon is visible in the graph below. Passing the first peak means measures are taking effect; passing the second means our healthcare system is now able to cope. So, where do countries stand as of now?&lt;/p&gt;

&lt;p&gt;Of course, the decision has to also depend on other factors such as the ability of testing and tracking down close contacts of those infected.&lt;/p&gt;

&lt;p&gt;There are actually many questions that we could ask from this graph. For example:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Why does Germany has much higher daily confirms than Switzerland, and yet manages a much flatter death curve?&lt;/li&gt;
  &lt;li&gt;Why do the two peaks for the UK seem to occur at the same time while that’s not the case for the rest?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Countries with hopes of relaxing some of the lockdown measures: Germany and Switzerland. Both of them have passed the peaks, have low daily cases (&amp;lt;20), and relatively flat and low death case curve (&amp;lt;5).&lt;/li&gt;
  &lt;li&gt;Countries that probably need more time: They are at the edge of passing the first peak and record about 80 daily cases. What’s more worrying, though, is the evident pressure on the healthcare system. UK sees a drop in daily death cases, but that number is still high at 11; the US’s death case curve seems not at its peak yet. They probably need more time. - April 23, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Percentage Change&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;daily-case-percentage-change&quot;&gt;Daily Case Percentage Change&lt;/h3&gt;
&lt;p&gt;Look out for the 7-day moving average of the day-on-day percentage change in confirmed cases. It is important to see both the current percentage change and its trend. To easily classify the situation, we can use the following scale&lt;sup id=&quot;fnref:percentage&quot;&gt;&lt;a href=&quot;#fn:percentage&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r &gt; 10\%&lt;/script&gt;: &lt;strong&gt;Rapidly increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 10\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 5\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Slowly increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 1\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Under control&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=565833280&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Japan had a turning point on 23rd March where the increase of cases started accelerating. Coincidently (or maybe not), Japan and I.O.C. officially anounced the &lt;a href=&quot;https://www.nytimes.com/2020/03/24/sports/olympics/coronavirus-summer-olympics-postponed.html&quot;&gt;postponement of Tokyo 2020&lt;/a&gt; on the next day.&lt;/li&gt;
  &lt;li&gt;The cases in Japan have been rising at an increasing rate, now at a 10% &lt;a href=&quot;#Percentage Change&quot;&gt;day-on-day growth rate&lt;/a&gt;. Considering the exponential growth of infections, Abe, Japanese prime minister, is declaring emergency state for seven prefectures. - April 7, 2020&lt;/li&gt;
  &lt;li&gt;Japan sees a slowdown of daily new cases. It’s been two weeks since the first declaration of “Emergency Situation” by the prime minister. On average, a 50% reduction in the number of people going out in monitored areas &lt;a href=&quot;https://www3.nhk.or.jp/news/special/coronavirus/#infection-status&quot;&gt;are observed&lt;/a&gt;. Meanwhile, mask sales have skyrocketed in Japan. - April 22, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Google Search Interest&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;google-search-interest&quot;&gt;Google Search Interest&lt;/h3&gt;
&lt;p&gt;This figure tells us how many people in the US are searching for keywords such as “hand sanitizer” or “symptom”. I suspect that as the community spread of the virus is being contained, we can expect to see a drop in searches for words like “symptom” and “influenza”, similar to the trends shown in Singapore.&lt;/p&gt;

&lt;p&gt;There are drastic differences in terms of the US and Singapore google search interests during this pandemic. When signs of community infection emerged in early March, people in the US were searching for “symptom” at a record-high frequency, similarly for “influenza” and “hand sanitizer”. Searches for “mask”, however, were not so heightened. The picture in Singapore looks very different. When more infections emerged inside the border in late January and early February, the search for “mask” shoot up rapidly, and masks went out of stock everywhere in Singapore. There are probably two main reasons for this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A high percentage of Chinese living in Singapore;&lt;/li&gt;
  &lt;li&gt;As a nation that went through SARS, it feels natural for most people to wear masks when a contagion is spreading in the community.&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=783455223&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=196247116&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;US Testing Numbers&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;us-testing-numbers&quot;&gt;US Testing Numbers&lt;/h3&gt;
&lt;p&gt;As the containment takes effect, we expect to see the number of positive and negative tests stabilize, and the number of tests pending result drops. As you can see, we are not there yet.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=481777218&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;cumulative-case-progression&quot;&gt;Cumulative Case Progression&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;Case progression&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;696.0000000000001&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=967719983&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Japan has a relatively flat curve. However, there are legitimate concerns that Japan has been under-testing its population to know what is really going on. Assuming the true CFR is 1.2%&lt;sup id=&quot;fnref:diamond_princess&quot;&gt;&lt;a href=&quot;#fn:diamond_princess&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, Japan’s current fatality number, 77, indicates that at least 6,417 people have been infected. However, only 3,139 cases are officially confirmed as of now. Also, Japan has conducted 486 tests &lt;a href=&quot;#https://www.worldometers.info/coronavirus/&quot;&gt;per one million population&lt;/a&gt;. In Singapore, that number is 11,110. - April 5, 2020.&lt;/li&gt;
  &lt;li&gt;For the first time, Singapore is going into a national “Shelter in Place” mode. The timeing is not surprising as some degree of wide-spread community infection is going on. The number of unlinked cases, those yet to find the source of infection, spiked over the last few days; Singapore also recorded 12 new clusters of infection just over the past five days (One of them is right across the river from my house). - April 5, 2020.&lt;/li&gt;
  &lt;li&gt;Singapore sees a steady increase in confirmed cases, mainly in foreign worker dormitory clusters. However, if we look at the &lt;a href=&quot;#Case progression&quot;&gt;progression of confirmed cases&lt;/a&gt; in Singapore, it’s an almost perfect example of what “flatten the curve” looks like. For the most part, the cases double every ten days, whereas cases in some of the worst-hit countries double every one to three days.  - April 8, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;death-cases&quot;&gt;Death Cases&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;case fatality rate&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Germany and Switzerland fare well in this regard and manage to record comparatively low CFRs. Austria, too, has managed one of the lowerest CFRs among European nations. Austria, Germany, and a large part of Switzerland are German-speaking.🤔&lt;/li&gt;
  &lt;li&gt;While the CFRs in Switzerland and Germany have been comparatively low, they are steadily climbing. Switzerland is probably the first country in Europe to flatten the curve, which conducted one of the highest number of tests &lt;a href=&quot;#https://www.worldometers.info/coronavirus/&quot;&gt;per one million population&lt;/a&gt;. - April 10, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;cfr bias&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How does selection bias affect CFR?
    &lt;blockquote&gt;
      &lt;p&gt;[In Italy], a change in strategy on Feb 25 limited testing to patients who had severe signs and symptoms also resulted in a 19% positive rate (21,157 of 109,170 tested as of Mar 14) and an apparent increase in the death rate—from 3.1% on Feb 24 to 7.2% on Mar 17—patients with milder illness were no longer tested.  In the UK, only patients deemed ill enough to require at least one night in hospital met the criteria for a Covid-19 test.&lt;/p&gt;

      &lt;p&gt;CFR rates are subject to selection bias as more severe cases are tested, generally those in the hospital settings or those with more severe symptoms. The number of currently infected asymptomatics is uncertain: estimates put it at least a half are asymptomatic; the proportion not coming forward for testing is also highly doubtful (i.e. you are symptomatic, but you do not present for testing). Therefore we can assume the IFR is significantly lower than the CFR.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;When is CFR accurate?
    &lt;blockquote&gt;
      &lt;p&gt;Iceland’s higher rates of testing, the smaller population, and their ability to ascertain all those with Sars-CoV-2  means they can obtain. an accurate estimate of the CFR and the infection fatality rate (IFR) during the pandemic (most countries will only be able to do this after the pandemic). Current data from Iceland suggests their IFR is somewhere between 0.01% and 0.19%.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The bottom line is, CFR is probably &lt;strong&gt;inflated&lt;/strong&gt; in many countries and IFR is &lt;strong&gt;much lower&lt;/strong&gt; than CFR.&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;h4 id=&quot;websites&quot;&gt;Websites&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Bloomberg&lt;/strong&gt;: &lt;a href=&quot;https://www.bloomberg.com/graphics/2020-coronavirus-cases-world-map/?srnd=premium-asia&quot;&gt;Mapping the Coronavirus Outbreak Across the World&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Johns Hopkins University&lt;/strong&gt;: &lt;a href=&quot;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Coronavirus COVID-19 Global Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Global MediXchange&lt;/strong&gt;: &lt;a href=&quot;https://www.alibabacloud.com/universal-service/pdf_reader?spm=a3c0i.14138300.8102420620.dreadnow.646d647fDWbsii&amp;amp;cdnorigin=video-intl&amp;amp;pdf=Read%20Online-Handbook%20of%20COVID-19%20Prevention%20and%20Treatment.pdf&quot;&gt;Handbook of COVID-19 Prevention and Treatment&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;articles&quot;&gt;Articles&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nytimes.com/2020/03/19/us/politics/trump-coronavirus-outbreak.html&quot;&gt;Before Virus Outbreak, a Cascade of Warnings Went Unheeded&lt;/a&gt;, March 19, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.citylab.com/life/2020/03/coronavirus-cases-france-train-hospital-tgv-covid-19-patient/608833/&quot;&gt;To Fight a Fast-Moving Pandemic, Get a Faster Hospital&lt;/a&gt;, March 26, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.businessinsider.sg/coronavirus-spain-says-rapid-tests-sent-from-china-missing-cases-2020-3?_ga=2.212074516.1285585527.1585620210-963085568.1583747541&amp;amp;r=US&amp;amp;IR=T&quot;&gt;Spain, Europe’s worst-hit country after Italy, says coronavirus tests it bought from China are failing to detect positive cases&lt;/a&gt;, March 26, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://time.com/5812555/germany-coronavirus-deaths/&quot;&gt;Why Is Germany’s Coronavirus Death Rate So Low?&lt;/a&gt;, March 30, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nytimes.com/interactive/2020/04/14/science/coronavirus-transmission-cough-6-feet-ar-ul.html&quot;&gt;This 3-D Simulation Shows Why Social Distancing Is So Important&lt;/a&gt;, April 14, 2020&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-sources&quot;&gt;Data sources&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Japan: &lt;a href=&quot;https://www3.nhk.or.jp/news/special/coronavirus/#infection-status&quot;&gt;NHK&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Singapore: &lt;a href=&quot;https://www.moh.gov.sg/covid-19&quot;&gt;Ministry of Health&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Other countries: JHU &lt;a href=&quot;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Coronavirus COVID-19 Global Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;US testing numbers: &lt;a href=&quot;https://covidtracking.com/&quot;&gt;The COIVD Tracking Project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Search interests: &lt;a href=&quot;https://trends.google.com/trends/explore?date=today%205-y&amp;amp;geo=US&amp;amp;q=%2Fm%2F0b23px,%2Fm%2F01kr41,%2Fm%2F0cycc,%2Fm%2F01b_06&quot;&gt;Google Trends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:percentage&quot;&gt;
      &lt;p&gt;The percentage only indicates a relative change. The actual number of new cases reported in each country may be very different, as it depends on the absolute number of cumulative cases in that country. &lt;a href=&quot;#fnref:percentage&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:diamond_princess&quot;&gt;
      &lt;p&gt;Russell, Timothy W., et al. “&lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.03.05.20031773v2&quot;&gt;Estimating the infection and case fatality ratio for COVID-19 using age-adjusted data from the outbreak on the Diamond Princess cruise ship.&lt;/a&gt;” medRxiv (2020). &lt;a href=&quot;#fnref:diamond_princess&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>读书笔记：读书体验是什么</title>
   <link href="http://localhost:4000/reading/2019/11/12/how-to-read.html"/>
   <updated>2019-11-12T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2019/11/12/how-to-read</id>
   <content type="html">&lt;p&gt;我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 50%;&quot;&gt;&lt;img src=&quot;/assets/2019-11-12/book_cover.jpg&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;书名：&lt;a href=&quot;https://book.douban.com/subject/26789567/&quot;&gt;如何有效阅读一本书&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;原名：読書は1冊のノートにまとめなさい&lt;/li&gt;
  &lt;li&gt;作者：&lt;a href=&quot;http://okuno0904.com/about/index.html&quot;&gt;奥野宣之&lt;/a&gt;（おくの・のぶゆき）&lt;/li&gt;
  &lt;li&gt;购入日期：2019.11.11（打折！）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;读书体验&quot;&gt;读书体验&lt;/h3&gt;
&lt;p&gt;这本149页的小书，我觉得可以看做是“如何读书”的一本参考书；里面介绍的观点和方法，不同级别的读者都可以在适当的时候读一读，并学到些东西。这本小书旨在回答这样一个问题：如何才能不忘记读过的书中的内容，并使之融入身心，真正使书籍影响自己？&lt;/p&gt;

&lt;p&gt;而他的回答是，我们应该重新思考读书这件事儿。读书不应始于翻开书籍，也不终于最后一页。每一次读书，我们应该去创造属于自己的“读书体验”。要如何理解这个“体验”呢？我们可以想想生活中的其他体验，尤其是使用体验。我不经常买东西，但如果要买什么，会尽量去想清楚为什么要买，会提前去了解这个东西的背景，功能，评价，使用过程中会不断更新最初的设想，并持续影响我下一次购物的判断。这也就是一次有意识有目的能影响未来的购物体验。也就是说，作者推崇的是一种有目的能重温、甚至历久弥新的读书体验。实际上，作者也认为，这样的读书体验比书本身重要多了。&lt;/p&gt;

&lt;p&gt;作者总结了下面五个具体可行的步骤来创造所谓的读书体验：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;选书&lt;/strong&gt;：收集随想，建立目的&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;购书&lt;/strong&gt;：冷静评估，书籍确认&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;读书&lt;/strong&gt;：适当标记，提炼重点&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;记录&lt;/strong&gt;：原文摘抄，原创思考&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;活用&lt;/strong&gt;：重读笔记，思想输出&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而在作者的心中，创造这个读书体验必不可少的伙伴是一个普通的笔记本。因为它在上面五个步骤中所发挥的重要作用，这笔记本应时常伴我们左右，并且，如果直译本书的原书名，你会发现，它其实意思是“请用一个笔记本整理你的读书”。&lt;/p&gt;

&lt;h3 id=&quot;用购书清单指名购书&quot;&gt;用购书清单指名购书&lt;/h3&gt;
&lt;p&gt;创造读书体验的第一步，是给自己开出一个购书清单。开清单不是为了逛书店的时候容易找（也有这好处），更重要的是，让我们能清楚认识到读每一本书的目的是什么。作者是这么说的：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;那么，为什么要把列清单的过程也作为读书方法的一部分来说明呢？理由之一，就是要培养带着目的去读书的目的意识。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是作者推荐的选书购书的具体操作步骤：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;好奇心激发&lt;/strong&gt; → &lt;strong&gt;随想笔记&lt;/strong&gt; → &lt;strong&gt;购书清单&lt;/strong&gt; → &lt;strong&gt;购书&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一步是将激发好奇心的源头记进笔记本里，可以叫做随想笔记。这源头的可能性就很多了：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报刊上读到有意思的书评&lt;/li&gt;
  &lt;li&gt;听到感兴趣的政治时事评论&lt;/li&gt;
  &lt;li&gt;来自朋友的书籍推荐&lt;/li&gt;
  &lt;li&gt;等等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;只要是激起了我们好奇心的东西，都应该记进随想笔记里去。之后便能根据随想笔记，建立起读书的目的，并去寻找相关书籍。在经过冷静的评估之后，将想要购买的书籍列进清单。这流程的好处是，我们相对能够准确地选出自己真正想读并且明白为什么想读的书，这样买回来感兴趣、读下去的几率都比较高（这个很重要😂）。并且，在读书过程中，可以带着最初被激发的好奇心去读，读完之后也能回顾一开始好奇心被激发的契机，因为这些都被一一记录在笔记本里。&lt;/p&gt;

&lt;h3 id=&quot;用笔记把读过的书变为精神财富&quot;&gt;用笔记把读过的书变为精神财富&lt;/h3&gt;
&lt;p&gt;这里讲的包含了步骤三和四：读书，记录。&lt;/p&gt;

&lt;p&gt;读书的时候，我们遵循这样一个流程：通读 → 片段重读 → 标记。意思就是，通读之后，去重读你感到有共鸣、疑惑、感兴趣等等的片段，有必要就用统一符号标记下来，比如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;下划直线 ＿＿：客观重要&lt;/li&gt;
  &lt;li&gt;下划波浪线 ˷˷˷˷˷˷：我觉得重要，非常重要&lt;/li&gt;
  &lt;li&gt;圆圈 ◯：关键词、专业名称&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样读过一个章节、一本书，就能进入到下一个步骤：记录。这个时候就可以一一回顾上一步所标记出来的部分，参照下面的格式，在笔记本上写下读过这本书后的读书笔记：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;⚬ …接原文摘抄、要点概括&lt;/li&gt;
  &lt;li&gt;⭑ …接评论、感想&lt;/li&gt;
  &lt;li&gt;重复上面&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体来说，我们摘抄的时候，可以摘抄些什么呢？&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;能让我主观产生共鸣的&lt;/li&gt;
  &lt;li&gt;不是读后觉得”理应如此“，而是“这么一说确实如此”的&lt;/li&gt;
  &lt;li&gt;能颠覆我已有的想法、动摇我认识的&lt;/li&gt;
  &lt;li&gt;等等&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;摘抄或要点概括很大程度上是原作者的思想，而在评论里，我们应该尽量去挖掘些原创的思考。这会是个耗时间费精力的过程，不过只有试过的人才能知道到底值不值得。另外值得一提的是，作者还提倡将跟这本书有关的时间、空间印记也一起贴进笔记本里，比如说新书买来时的书腰、看那本书时所坐火车的票根等等。以后的某个时间，再次读起笔记本的这一页，看那本书时所经过的风景闻过的花香也会跃然纸上吧。&lt;/p&gt;

&lt;p&gt;当然了，上面讲的记录的形式都是作者的推荐，我们大可不必居于某种形式，只需按照自己舒服的方式坚持记下属于自己的读书笔记就好了：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;说句老生常谈的话，只有把读书笔记控制在自己能力允许的范围内，才能长久地坚持下去。所以，要选择对自己来说比较方便的笔记方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想这道理几乎适用于所有需要长久坚持的事，探究一门深奥的学问、学习一门外语、亦或是健身减肥等等。我发现人往往能轻松看到漫长过程之后的一种状态，这或许是我们这样高等生物的特殊能力；但人又往往忍不住会对期待的状态过于着急。这的确与我们身处的社会环境有所关系，但在我们的基因当中是否也有着这种既有远见又企求触手可得的回报的种子呢？&lt;/p&gt;

&lt;p&gt;我在读这本书的时候所做的读书笔记就没有按照作者推荐的格式，而是采用了自己习惯的类似于PPT设计的风格：
&lt;img src=&quot;/assets/2019-11-12/how_to_read.jpg&quot; alt=&quot;how_to_read&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不难看出，笔记里的结构几乎原封不动地变成了我这篇文章的结构，再加上在书里相关标记写下的评论，这篇文章的主要内容在我做完读书笔记的同时也就完成了。而这也是作者推崇的，以自己的读书笔记为基础，进一步写出原创文章，做属于自己的思想的输出。&lt;/p&gt;

&lt;h3 id=&quot;通过重读笔记提高自我&quot;&gt;通过重读笔记提高自我&lt;/h3&gt;
&lt;p&gt;读书体验的最后一步，也是我从没做过的一步：重读笔记。就像有人会偶尔重读日记一样，时常重读读书笔记能让自己读过的书好像一直存在自己生活中，不断酝酿，不断跟自己的经历、知识发生新的碰撞：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;如果把一本书比作一个“场所”，那么读书笔记就是在这个场所拍摄的照片。在不同时间去同一个场所拍照，拍出来的照片都会有所不同，而过一段时间再去看这些照片，对那个场所的印象也会发生变化。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于如何重读，作者推荐：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;简单回顾：读笔记&lt;/li&gt;
  &lt;li&gt;细致回顾：读笔记 + 原书标记&lt;/li&gt;
  &lt;li&gt;经典重温：读笔记 + 原书&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这么看，我确实一个回顾都没做过😂。一开始读这本书是因为买了新的笔记本，我老是买新笔记本，想着要写点什么文字，最后都沦为平时工作用的草稿纸（也是很重要啦）。看了这本书的评论觉得可能会帮我结束这个循环，目前看来很有希望。读之前，如何写读书笔记是我最感兴趣的部分，不过读完发现，以随想笔记到笔记回顾的一整个读书体验来理解读书这事儿，才是奥野宣之这本书给我最大的收获吧。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linear discriminant analysis, explained</title>
   <link href="http://localhost:4000/data/2019/10/02/linear-discriminant-analysis.html"/>
   <updated>2019-10-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2019/10/02/linear-discriminant-analysis</id>
   <content type="html">&lt;p&gt;&lt;em&gt;Intuitions, illustrations, and maths: How it’s more than a dimension reduction tool and why it’s robust for real-world applications.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-02/mda.png&quot; alt=&quot;mda&quot; /&gt; This graph shows that boundaries (blue lines) learned by mixture discriminant analysis (MDA) successfully separate three mingled classes. MDA is one of the powerful extensions of LDA.&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Linear discriminant analysis (LDA) is not just a dimension reduction tool, but also a robust classification method.&lt;/li&gt;
  &lt;li&gt;With or without data normality assumption, we can arrive at the same LDA features, which explains its robustness.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;LDA is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed.&lt;/p&gt;

&lt;p&gt;Two prominent examples of using LDA (and it’s variants) include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Bankruptcy prediction&lt;/em&gt;: Edward Altman’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Altman_Z-score&quot;&gt;1968 model&lt;/a&gt; predicts the probability of company bankruptcy using trained LDA coefficients. The accuracy is said to be between 80% and 90%, evaluated over 31 years of data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Facial recognition&lt;/em&gt;: While features learned from Principal Component Analysis (PCA) are called Eigenfaces, those learned from LDA are called &lt;a href=&quot;http://www.scholarpedia.org/article/Fisherfaces&quot;&gt;Fisherfaces&lt;/a&gt;, named after the statistician, Sir Ronald Fisher. We explain this connection later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article starts by introducing the classic LDA and why it’s deeply rooted as a classification method. Next, we see the inherent dimension reduction in this method and how it leads to the reduced-rank LDA. After that, we see how Fisher masterfully arrived at the same algorithm, without assuming anything on the data. A hand-written digits classification problem is used to illustrate the performance of the LDA. The merits and disadvantages of the method are summarized in the end.&lt;/p&gt;

&lt;p&gt;The second article following this generalizes LDA to handle more complex problems. By the way, you can find a set of &lt;a href=&quot;/assets/2019-10-02/Discriminant_Analysis.pdf&quot; target=&quot;_blank&quot;&gt;corresponding slides&lt;/a&gt; where I present roughly the same materials written in this article.&lt;/p&gt;

&lt;h3 id=&quot;classification-by-discriminant-analysis&quot;&gt;Classification by discriminant analysis&lt;/h3&gt;
&lt;p&gt;Let’s see how LDA can be derived as a supervised classification method. Consider a generic classification problem: A random variable $X$ comes from one of $K$ classes, with density $f_k(\mathbf{x})$ on $\mathbb{R}^p$. A discriminant rule tries to divide the data space into $K$ disjoint regions $\mathbb{R}_1, \dots, \mathbb{R}_K$ that represent all classes (imagine the boxes on a chessboard). With these regions, classification by discriminant analysis simply means that we allocate $\mathbf{x }$ to class $j$ if $\mathbf{x}$ is in region $j$. The question is then, how do we know which region the data $\mathbf{x }$ falls in? Naturally, We can follow two allocation rules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Maximum likelihood rule&lt;/em&gt;: If we assume that each class could occur with equal probability, then allocate $\mathbf{x }$ to class $j$ if $j = \arg\max_i f_i(\mathbf{x})$.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bayesian rule&lt;/em&gt;: If we know the class prior probabilities, $\pi_1, \dots, \pi_K$, then allocate $\mathbf{x }$ to class $j$ if $j = \arg\max_i \pi_i f_i(\mathbf{x}) $.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-and-quadratic-discriminant-analysis&quot;&gt;Linear and quadratic discriminant analysis&lt;/h4&gt;
&lt;p&gt;If we assume data comes from multivariate Gaussian distribution, i.e. $X \sim N(\mathbf{\mu}, \mathbf{\Sigma})$, explicit forms of the above allocation rules can be obtained. Following the Bayesian rule, we classify $\mathbf{x}$ to class $j$ if $j = \arg\max_i \delta_i(\mathbf{x})$ where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \delta_i(\mathbf{x}) = \log f_i(\mathbf{x}) + \log \pi_i
\end{align}&lt;/script&gt;

&lt;p&gt;is called the discriminant function. Note the use of log-likelihood here.  The decision boundary separating any two classes, $k$ and $\ell$, is the set of $\mathbf{x}$ where two discriminant functions have the same value, i.e. &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{x}: \delta_k(\mathbf{x}) = \delta_{\ell}(\mathbf{x})\}&lt;/script&gt;. Therefore, any data that falls on the decision boundary is equally likely from the two classes.&lt;/p&gt;

&lt;p&gt;LDA arises in the case where we assume equal covariance among $K$ classes, i.e. $\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \dots = \mathbf{\Sigma}_K$. Then we can obtain the following discriminant function:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = \mathbf{x}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}-\frac{1}{2} \mathbf{\mu}_{k}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}+\log \pi_{k} \,,
    \label{eqn_lda}
\end{align}&lt;/script&gt; using the Gaussian distribution likelihood function.&lt;/p&gt;

&lt;p&gt;This is a linear function in $\mathbf{x}$. Thus, the decision boundary between any pair of classes is also a linear function in $\mathbf{x}$, the reason for its name: linear discriminant analysis. Without the equal covariance assumption, the quadratic term in the likelihood does not cancel out, hence the resulting discriminant function is a quadratic function in $\mathbf{x}$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = 
    - \frac{1}{2} \log|\mathbf{\Sigma}_k| 
    - \frac{1}{2} (\mathbf{x} - \mathbf{\mu}_{k})^{T} \mathbf{\Sigma}_k^{-1} (\mathbf{x} - \mathbf{\mu}_{k}) + \log \pi_{k} \,.
    \label{eqn_qda}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the decision boundary is quadratic in $\mathbf{x}$. This is known as quadratic discriminant analysis (QDA).&lt;/p&gt;

&lt;h4 id=&quot;which-is-better-lda-or-qda&quot;&gt;Which is better? LDA or QDA?&lt;/h4&gt;
&lt;p&gt;In real problems, population parameters are usually unknown and estimated from training data as $\hat{\pi}_k, \hat{\mathbf{\mu}}_k, \hat{\mathbf{\Sigma}}_k$. While QDA accommodates more flexible decision boundaries compared to LDA, the number of parameters needed to be estimated also increases faster than that of LDA. From (\ref{eqn_lda}), $p+1$ parameters (nonlinear transformation of the original distribution parameters) are needed to construct the discriminant function. For a problem with $K$ classes, we would only need $K-1$ such discriminant functions by arbitrarily choosing one class to be the base class, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{k}'(\mathbf{x}) = \delta_{k}(\mathbf{x}) - \delta_{K}(\mathbf{x})\,,&lt;/script&gt;

&lt;p&gt;for $k = 1, \dots, K-1$. Hence, the total number of estimated parameters for LDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)(p+1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand, for each QDA discriminant function (\ref{eqn_qda}), mean vector, covariance matrix, and class prior need to be estimated:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mean: $p$&lt;/li&gt;
  &lt;li&gt;Covariance: $p(p+1)/2$&lt;/li&gt;
  &lt;li&gt;Class prior: 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The total number of estimated parameters for QDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)\{p(p+3)/2+1\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Therefore, the number of parameters estimated in LDA increases linearly with $p$ while that of QDA increases quadratically with $p$.&lt;/em&gt; We would expect QDA to have worse performance than LDA when the dimension $p$ is large.&lt;/p&gt;

&lt;h4 id=&quot;best-of-two-worlds-compromise-between-lda--qda&quot;&gt;Best of two worlds? Compromise between LDA &amp;amp; QDA&lt;/h4&gt;
&lt;p&gt;We can find a compromise between LDA and QDA by regularizing the individual class covariance matrices. Regularization means that we put a certain restriction on the estimated parameters. In this case, we require that individual covariance matrix shrinks toward a common pooled covariance matrix through a penalty parameter $\alpha$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.&lt;/script&gt;

&lt;p&gt;The pooled covariance matrix can also be regularized toward an identity matrix through a penalty parameter $\beta$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}} (\beta) = \beta \hat{\mathbf{\Sigma}} + (1-\beta) \mathbf{I} \,.&lt;/script&gt;

&lt;p&gt;In situations where the number of input variables greatly exceeds the number of samples, the covariance matrix can be poorly estimated. Shrinkage can hopefully improve estimation and classification accuracy. This is illustrated by the figure below.
&lt;img src=&quot;/assets/2019-10-02/lda_shrinkage.png&quot; alt=&quot;lda_shrinkage&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot, credit to &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for testing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# how often to repeat classification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# maximum number of features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# step size for the calculation
&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Generate random blob-ish data with noisy features.

    This returns an array of input data with shape `(n_samples, n_features)`
    and an array of `n_samples` target labels.

    Only one feature contains discriminative information, the other features
    contain only noise.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add non-discriminative features
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA with shrinkage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_features / n_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Classification accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;computation-for-lda&quot;&gt;Computation for LDA&lt;/h4&gt;
&lt;p&gt;We can see from (\ref{eqn_lda}) and (\ref{eqn_qda}) that computations of discriminant functions can be simplified if we diagonalize the covariance matrices first. That is, data are transformed to have an identity covariance matrix (no correlation, variance of 1). In the case of LDA, here’s how we proceed with the computation:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Perform eigen-decompostion on the pooled covariance matrix: &lt;script type=&quot;math/tex&quot;&gt;\hat{\mathbf{\Sigma}} = \mathbf{U}\mathbf{D}\mathbf{U}^{T} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Sphere the data:
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}^{*} \leftarrow \mathbf{D}^{-\frac{1}{2}} \mathbf{U}^{T} \mathbf{X} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Obtain class means in the transformed space: &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_1, \dots, \hat{\mu}_{K}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Classify $\mathbf{x}$ according to $\delta_{k}(\mathbf{x}^{*})$:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\delta_{k}(\mathbf{x}^{*})=\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k} \,.
\label{eqn_lda_sphered}
\end{align}&lt;/script&gt;

&lt;p&gt;Step 2 spheres the data to produce an identity covariance matrix in the transformed space. Step 4 is obtained by following (\ref{eqn_lda}). Let’s take a two-class example to see what LDA is doing. Suppose there are two classes, $k$ and $\ell$. We classify $\mathbf{x}$ to class $k$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &gt; 0.
\end{align}&lt;/script&gt;

&lt;p&gt;Following the four steps outlined above, we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &amp;= 
\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k}
- \mathbf{x^{*}}^{T} \hat{\mu}_{\ell} + \frac{1}{2} \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell} - \log \hat{\pi}_{k} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k}^{T}\hat{\mu}_{k} - \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;&gt; 0 \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That is, we classify $\mathbf{x}$ to class $k$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) &gt; \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \,.&lt;/script&gt;

&lt;p&gt;The derived allocation rule reveals the working of LDA. The left-hand side of the equation is the length of the orthogonal projection of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x^{*}}&lt;/script&gt; onto the line segment joining the two class means. The right-hand side is the location of the center of the segment corrected by class prior probabilities. &lt;em&gt;Essentially, LDA classifies the data to the closest class mean.&lt;/em&gt; We make two observations here.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The decision point deviates from the middle point when the class prior probabilities are not the same, i.e., the boundary is pushed toward the class with a smaller prior probability.&lt;/li&gt;
  &lt;li&gt;Data are projected onto the space spanned by class means, e.g. &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_{k} - \hat{\mu}_{\ell}&lt;/script&gt;. Distance comparisons are then done in that space.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reduced-rank-lda&quot;&gt;Reduced-rank LDA&lt;/h3&gt;
&lt;p&gt;What I’ve just described is LDA for classification. LDA is also famous for its ability to find a small number of meaningful dimensions, allowing us to visualize and tackle high-dimensional problems. What do we mean by meaningful, and how does LDA find these dimensions? We will answer these questions shortly. First, take a look at the below plot. For a &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine&quot;&gt;wine classification&lt;/a&gt; problem with three different types of wines and 13 input variables, the plot visualizes the data in two discriminant coordinates found by LDA. In this two-dimensional space, the classes can be well-separated. In comparison, the classes are not as clearly separated using the first two principal components found by PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-02/lda_vs_pca.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'turquoise'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'darkorange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LDA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PCA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;dimension-reduction-is-inherent-in-lda&quot;&gt;Dimension reduction is inherent in LDA&lt;/h4&gt;
&lt;p&gt;In the above wine example, a 13-dimensional problem is visualized in a 2d space. Why is this possible? This is possible because there’s an inherent dimension reduction in LDA. We have observed from the previous section that LDA makes distance comparison in the space spanned by different class means. Two distinct points lie on a 1d line; three distinct points lie on a 2d plane. Similarly, $K$ class means lie on a hyperplane with dimension at most $(K-1)$. In particular, the subspace spanned by the means is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{K-1}=\mu_{1} \oplus \operatorname{span}\left\{\mu_{i}-\mu_{1}, 2 \leq i \leq K\right\} \,.&lt;/script&gt;

&lt;p&gt;When making distance comparison in this space, distances orthogonal to this subspace would add no information since they contribute equally for each class. Hence, by restricting distance comparisons to this subspace only would not lose any information useful for LDA classification. That means, we can safely transform our task from a $p$-dimensional problem to a $(K-1)$-dimensional problem by an orthogonal projection of the data onto this subspace. When $p \gg K$, this is a considerable drop in the number of dimensions. What if we want to reduce the dimension further from $p$ to $L$ where $K \gg L$, e.g. two dimensional with $L = 2$? We can construct an $L$-dimensional subspace, $H_L$, from $H_{K-1}$, and this subspace is optimal, in some sense, for LDA classification.&lt;/p&gt;

&lt;h4 id=&quot;what-would-be-the-optimal-subspace&quot;&gt;What would be the optimal subspace?&lt;/h4&gt;
&lt;p&gt;Fisher proposes that the subspace $H_L$ is optimal when the class means of sphered data have maximum separation in this subspace in terms of variance. Following this definition, optimal subspace coordinates are simply found by doing PCA on sphered class means, since PCA finds the direction of maximal variance. The computation steps are summarized below:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Find class mean matrix, $\mathbf{M}_{(K\times p)}$, and pooled var-cov, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_{(p\times p)}&lt;/script&gt;, where&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
 \mathbf{W} = \sum_{k=1}^{K} \sum_{g_i = k} (\mathbf{x}_i - \hat{\mu}_k)(\mathbf{x}_i - \hat{\mu}_k)^T \,.
 \label{within_w}
 \end{align}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Sphere the means: $\mathbf{M}^* = \mathbf{M} \mathbf{W}^{-\frac{1}{2}}$, using eigen-decomposition of $\mathbf{W}$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \operatorname{cov}(\mathbf{M}^*)&lt;/script&gt;, the between-class covariance of sphered class means by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{B}^* = \sum_{k=1}^{K} (\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)(\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)^T \,.&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;PCA: Obtain $L$ eigenvectors &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{v}^*_\ell)&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\mathbf{V}^*&lt;/script&gt; of 
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \mathbf{V}^* \mathbf{D_B} \mathbf{V^*}^T&lt;/script&gt; cooresponding to the $L$ largest eigenvalues. These define the coordinates of the optimal subspace.&lt;/li&gt;
  &lt;li&gt;Obtain $L$ new (discriminant) variables $Z_\ell = (\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell)^T X$, for $\ell = 1, \dots, L$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through this procedure, we reduce our data from &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_{(N \times p)}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Z}_{(N \times L)}&lt;/script&gt; and dimension from $p$ to $L$. Discriminant coordinate 1 and 2 in the previous wine plot are found by setting $L = 2$. Repeating the previous LDA procedures for classification using the new data, $\mathbf{Z}$, is called the reduced-rank LDA.&lt;/p&gt;

&lt;h3 id=&quot;fishers-lda&quot;&gt;Fisher’s LDA&lt;/h3&gt;
&lt;p&gt;If the derivation of the previous reduced-rank LDA looks very different to what you’ve known before, you are not alone! Here comes the revelation. Fisher derived the computation steps according to his optimality definition in a different way&lt;sup id=&quot;fnref:Fisher&quot;&gt;&lt;a href=&quot;#fn:Fisher&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. His steps of performing the reduced-rank LDA would later be known as the Fisher’s discriminant analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fisher does not make any assumptions about the distribution of the data. Instead, he tries to find a “sensible” rule so that the classification task becomes easier.&lt;/strong&gt; In particular, Fisher finds a linear combination of the original data, &lt;script type=&quot;math/tex&quot;&gt;Z = \mathbf{a}^T X&lt;/script&gt;, where the between-class variance, $\mathbf{B} = \operatorname{cov}(\mathbf{M})$, is maximized relative to the within-class variance, $\mathbf{W}$, as defined in (\ref{within_w}).&lt;/p&gt;

&lt;p&gt;The below plot, taken from ESL&lt;sup id=&quot;fnref:ESL&quot;&gt;&lt;a href=&quot;#fn:ESL&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, shows why this rule makes intuitive sense. The rule sets out to find a direction, $\mathbf{a}$, where, after projecting the data onto that direction, class means have maximum separation between them, and each class has minimum variance within them. The projection direction found under this rule, shown in the right plot, makes classification much easier.
&lt;img src=&quot;/assets/2019-10-02/sensible_rule.png&quot; alt=&quot;sensible_rule&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;finding-the-direction-fishers-way&quot;&gt;Finding the direction: Fisher’s way&lt;/h4&gt;
&lt;p&gt;Using Fisher’s sensible rule, finding the optimal projection direction(s) amounts to solving an optimization problem:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\max_{\mathbf{a}} \frac{\mathbf{a}^{T} \mathbf{B} \mathbf{a}}{\mathbf{a}^{T} \mathbf{W} \mathbf{a}} \,.
\end{align}&lt;/script&gt;
Recall that we want to find a direction where the between-class variance is maximized (the numerator) and the within-class variance is minimized (the denominator). This can be recasted as a generalized eigenvalue problem.&lt;/p&gt;

&lt;p&gt;The problem is equivalent to 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen}
\max_{\mathbf{a}} {}&amp;{} \mathbf{a}^{T} \mathbf{B} \mathbf{a} \,,\\ 
\text{s.t. } &amp;{} \mathbf{a}^{T} \mathbf{W} \mathbf{a} = 1 \,, \nonumber
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;since the scaling of $\mathbf{a}$ does not affect the solution.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{W}^{\frac12}$ be the symmetric square root of $\mathbf{W}$, and $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$. We can rewrite the problem (\ref{eqn_g_eigen}) as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_1}
\max_{\mathbf{y}} {}&amp;{} \mathbf{y}^{T} \mathbf{W}^{-\frac12} \mathbf{B} \mathbf{W}^{-\frac12} \mathbf{y} \,,\\ 
\text{s.t } &amp;{} \mathbf{y}^{T} \mathbf{y} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $\mathbf{W}^{-\frac12} \mathbf{B} \mathbf{W}^{-\frac12}$ is symmetric, we can find the spectral decomposition of it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\mathbf{W}^{-\frac12} \mathbf{B} \mathbf{W}^{-\frac12} = \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \,.
\label{eqn_fisher_eigen}
\end{align}&lt;/script&gt;

&lt;p&gt;Let $\mathbf{z} = \mathbf{\Gamma}^T \mathbf{y}$. So $\mathbf{z}^T \mathbf{z} = \mathbf{y}^T \mathbf{\Gamma} \mathbf{\Gamma}^T \mathbf{y} = \mathbf{y}^T \mathbf{y}$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{y}^{T} \mathbf{W}^{-\frac12} \mathbf{B} \mathbf{W}^{-\frac12} \mathbf{y} &amp;= \mathbf{y}^{T} \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \mathbf{y} \\
&amp;= \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Problem (\ref{eqn_g_eigen_1}) can then be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_2}
\max_{\mathbf{z}} {}&amp;{} \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} = \sum_i \lambda_i z_i^2 \,,\\ 
\text{s.t } &amp;{} \mathbf{z}^{T} \mathbf{z} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the eigenvalues are written in descending order, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\mathbf{z}} \sum_i \lambda_i z_i^2 &amp;\le \lambda_1 \sum_i z_i^2 \,,\\
&amp;= \lambda_1 \,,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the upper bound is attained at $\mathbf{z} = (1,0,0,\dots,0)^T$. Since $\mathbf{y} = \mathbf{\Gamma} \mathbf{z}$, the solution is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \pmb \gamma_{(1)}&lt;/script&gt;, the eigenvector corresponding to the largest eigenvalue in (\ref{eqn_fisher_eigen}). Since $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$, the optimal projection direction is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a} = \mathbf{W}^{-\frac12} \pmb \gamma_{(1)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem A.6.2&lt;/strong&gt; from MA&lt;sup id=&quot;fnref:MA&quot;&gt;&lt;a href=&quot;#fn:MA&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;: For &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}_(n \times p)&lt;/script&gt; and $\mathbf{B}_(p \times n)$, the non-zero eigenvalues of
$\mathbf{AB}$ and $\mathbf{BA}$ are the same and have the same multiplicity. If $\mathbf{x}$ is a non-trivial eigenvector of $\mathbf{AB}$ for an eigenvalue $\lambda \neq 0$, then $\mathbf{y}=\mathbf{Bx}$ is a non-trivial eigenvector of $\mathbf{BA}$.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\pmb \gamma_{(1)}&lt;/script&gt; is an eigenvector of $\mathbf{W}^{-\frac12} \mathbf{B} \mathbf{W}^{-\frac12}$, then, $\mathbf{W}^{-\frac12} \pmb \gamma_{(1)}$ is also the eigenvector of $\mathbf{W}^{-\frac12} \mathbf{W}^{-\frac12} \mathbf{B} = \mathbf{W}^{-1} \mathbf{B}$, using &lt;strong&gt;Theorem A.6.2&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In summary, optimal subspace coordinates, also known as discriminant coordinates, are obtained from the eigenvectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-1}\mathbf{B}&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;\ell = 1, ... , \min\{p,K-1\}&lt;/script&gt;.&lt;/em&gt; It can be shown that the &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt;s obtained are the same as &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell&lt;/script&gt;s obtained in the reduced-rank LDA formulation.&lt;/p&gt;

&lt;p&gt;Surprisingly, Fisher arrives at this formulation without any Gaussian assumption on the population, unlike the reduced-rank LDA formulation. The hope is that, with this sensible rule, LDA would perform well even when the data do not follow exactly the Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;handwritten-digits-problem&quot;&gt;Handwritten digits problem&lt;/h2&gt;
&lt;p&gt;Here’s an example to show the visualization and classification ability of Fisher’s LDA, or simply LDA. We need to recognize ten different digits, i.e., 0 to 9, using 64 variables (pixel values from images). The dataset is taken from &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, we can visualize the training images and they look like these: 
&lt;img src=&quot;/assets/2019-10-02/digits.png&quot; alt=&quot;digits&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_digits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;i'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Next, we train an LDA classifier on the first half of the data. Solving the generalized eigenvalue problem mentioned previously gives us a list of optimal projection directions. In this problem, we keep the top 4 coordinates, and the transformed data are shown below. 
&lt;img src=&quot;/assets/2019-10-02/reduced_lda_digits.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a classifier: a Fisher's LDA classifier
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'eigen'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train lda on the first half of the digits
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Visualize transformed data on learnt discriminant coordinates
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The above plot allows us to interpret the trained LDA classifier. For example, coordinate 1 helps to contrast 4’s and 2/3’s while coordinate 2 contrasts 0’s and 1’s. Subsequently, coordinate 3 and 4 help to discriminate digits not well-separated in coordinate 1 and 2. We test the trained classifier using the other half of the dataset. The report below summarizes the result.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;precision&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;recall&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;f1-score&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.90&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.77&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;avg / total&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Predict the value of the digit on the second half:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Classification report:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The highest precision is 99%, and the lowest is 77%, a decent result knowing that the method was proposed some 70 years ago. Besides, we have not done anything to make the procedure better for this specific problem. For example, there is collinearity in the input variables, and the shrinkage parameter might not be optimal.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-lda&quot;&gt;Summary of LDA&lt;/h2&gt;
&lt;p&gt;Here I summarize the virtues and shortcomings of LDA.&lt;/p&gt;

&lt;p&gt;Virtues of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Simple prototype classifier: Distance to the class mean is used, it’s simple to interpret.&lt;/li&gt;
  &lt;li&gt;Decision boundary is linear: It’s simple to implement and the classification is robust.&lt;/li&gt;
  &lt;li&gt;Dimension reduction: It provides informative low-dimensional view on
the data, which is both useful for visualization and feature engineering.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Shortcomings of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired.&lt;/li&gt;
  &lt;li&gt;In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired.&lt;/li&gt;
  &lt;li&gt;Support for more complex prototype classification is desired.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the next article, flexible, penalized, and mixture discriminant analysis will be introduced to address each of the three shortcomings of LDA. With these generalizations, LDA can take on much more difficult and complex problems.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Fisher&quot;&gt;
      &lt;p&gt;Fisher, R. A. (1936). &lt;em&gt;The Use Of Multiple Measurements In Taxonomic Problems. Annals of eugenics&lt;/em&gt;, 7(2), 179-188. &lt;a href=&quot;#fnref:Fisher&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ESL&quot;&gt;
      &lt;p&gt;Friedman, J., Hastie, T., &amp;amp; Tibshirani, R. (2001). &lt;em&gt;The Elements Of Statistical Learning&lt;/em&gt; (Vol. 1, No. 10). New York: Springer series in statistics. &lt;a href=&quot;#fnref:ESL&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:MA&quot;&gt;
      &lt;p&gt;Mardia, K. V., Kent, J. T., &amp;amp; Bibby, J. M. &lt;em&gt;Multivariate Analysis&lt;/em&gt;. 1979. Probability and mathematical statistics. Academic Press Inc. &lt;a href=&quot;#fnref:MA&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>How did I set up my blog using Jekyll, Hyde and GitHub</title>
   <link href="http://localhost:4000/learning/2019/09/25/set-up-blog.html"/>
   <updated>2019-09-25T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2019/09/25/set-up-blog</id>
   <content type="html">&lt;p&gt;What would be a better way to start this blog than writing a post about how I set it up? Because after three days of sifting through all the documents, blogs, StackOverflow answers and GitHub issues, I finally realized that the process is not as straightforward as I thought it would be. Anyway, I got it to work (for now).&lt;/p&gt;

&lt;p&gt;My aim is simple, to set up a blog for myself where I can post stuff about my life. The blog needs to be free, elegant, intuitive and supports math. My current set up, Jekyll + Hyde + Github + MathJax, matches with that. Since there are many resources online about setting up a blog using Jekyll and serve it with GitHub, I am going to skip all the standard procedures by referring to the official documents. Instead, this post specifically documents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the sequence of setting up different parts,&lt;/li&gt;
  &lt;li&gt;adding support for Tags, Categories and their corresponding pages,&lt;/li&gt;
  &lt;li&gt;adding MathJax to support $\LaTeX$-like math.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am using a macbook, so the steps will be described assuming the system is macOS. When in doubt, just google the relevant steps for other OSs.&lt;/p&gt;

&lt;h2 id=&quot;1-set-up-jekyll&quot;&gt;1. Set up Jekyll&lt;/h2&gt;
&lt;p&gt;Jekyll is the package that is generating all your website pages. First thing you want to do is to make sure that &lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is installed and ready to run.&lt;/p&gt;

&lt;p&gt;Follow the official &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;instructions&lt;/a&gt;. If you successfully made a new site, good!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;But if you ran into a &lt;strong&gt;failed to build native extension error&lt;/strong&gt;, install macOS SDK headers with the following line.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;If you ran into a &lt;strong&gt;file permission error&lt;/strong&gt;, run the following lines to set GEM_HOME to your user directory.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'# Install Ruby Gems to ~/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export GEM_HOME=$HOME/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export PATH=$HOME/gems/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now you can proceed with the following line and the rest of the steps.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gem &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;bundler jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is caused by the macOS Mojave update. The above solution is provided by &lt;a href=&quot;https://talk.jekyllrb.com/t/issues-installing-jekyll-on-macos-mojave/2400/3&quot;&gt;desiredpersona and Frank&lt;/a&gt;. Make sure Jekyll can run normally before proceeding to step 2.&lt;/p&gt;

&lt;h2 id=&quot;2-set-up-github-repo&quot;&gt;2. Set up GitHub repo&lt;/h2&gt;
&lt;p&gt;Jekyll generates web pages locally; we need a GitHub repository to host our pages so that they can be accessed on the internet. For this part, setup can be done by following GitHub’s official &lt;a href=&quot;https://pages.github.com&quot;&gt;instructions&lt;/a&gt;. In the end, you should have a repo on GitHub called &lt;em&gt;username&lt;/em&gt;.github.io, and the corresponding local folder on your computer. In my case, the name of my repo is yangxiaozhou.github.io.&lt;/p&gt;

&lt;p&gt;By the end of Step 1 and 2, we have set up the local engine for generating web pages and the GitHub repo for hosting and publishing your pages. Now we proceed to the actual website construction.&lt;/p&gt;

&lt;h2 id=&quot;3-use-a-website-template&quot;&gt;3. Use a website template&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://hyde.getpoole.com&quot;&gt;Hyde&lt;/a&gt; is a Jekyll website theme built on &lt;a href=&quot;https://github.com/poole/poole&quot;&gt;Poole&lt;/a&gt;. They provide the template and the theme for the website. There are many themes for Jekyll, but I decided to use Hyde because I like the elegant design and it’s easy to customize.&lt;/p&gt;

&lt;p&gt;To get Hyde, just download &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;the repo&lt;/a&gt; and move all the files into the local folder that you have just created in Step 2. Remember to clear any existing file in that folder before moving in Hyde files. From here, you just have to edit parts of those files to make the website yours (or use it as it is). I changed the following two lines in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; since redcarpet and pygments are not supported anymore. Other variables can also be changed such as name, GitHub account, etc.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kramdown&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;highlighter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rouge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At this point, it would be a good idea to learn some basics of &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;Jekyll&lt;/a&gt;, e.g. what is a front matter, what is a page, how to create a layout, etc. After learning these, you can go ahead and customize the website as you’d like.&lt;/p&gt;

&lt;p&gt;One problem that I ran into is that pages look fine in local serve, but when I publish them to the web, all pages other than the home page have suddenly lost all their style elements. After searching through the internet, I realize that this has to do with the &lt;code class=&quot;highlighter-rouge&quot;&gt;url&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;baseurl&lt;/code&gt; usage. If you also have this problem, consider doing the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;change all the &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ site.baseurl }}&lt;/code&gt;
instances in &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sidebar.html&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ '/' | relative_url }}&lt;/code&gt; so that the correct files can be located.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-add-tags--categories&quot;&gt;4. Add tags &amp;amp; categories&lt;/h2&gt;
&lt;p&gt;I want to add tags and categories to my posts and create a dedicated page where posts can be arranged according to &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;tags&lt;/a&gt;/&lt;a href=&quot;https://yangxiaozhou.github.io/categories/&quot;&gt;categories&lt;/a&gt;. This should be easy since tags and categories are default front matter variables that you can define in Jekyll. For example, tags and categories of my LDA post are defined like this:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;post&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;discriminant&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;analysis,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;explained&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;s&quot;&gt;2019-10-2 08:00:00 +0800&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DATA&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LDA supervised-learning classification&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;categories&lt;/strong&gt;, I created one page where posts of different categories are collected and the page is accessible through the sidebar link. To do this, just create a &lt;code class=&quot;highlighter-rouge&quot;&gt;category.html&lt;/code&gt; in the root folder:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: page
permalink: /categories/
title: Categories
---

&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archives&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
{% for category in site.categories %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-group&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% capture category_name %}{{ category | first }}{% endcapture %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h3&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;category-head&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ category_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h3&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    {% for post in site.categories[category_name] %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;article&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-item&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;h4&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.baseurl }}{{ post.url }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{post.title}}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/article&amp;gt;&lt;/span&gt;
    {% endfor %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;tags&lt;/strong&gt;, I did two things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Show the tags of a post at the end of the content.&lt;/li&gt;
  &lt;li&gt;For every tag, create a page where posts are collected, i.e. &lt;a href=&quot;https://yangxiaozhou.github.io/tag/classification&quot;&gt;classification&lt;/a&gt;, &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;supervised-learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To do 1, include the following lines after the &lt;code class=&quot;highlighter-rouge&quot;&gt;content&lt;/code&gt; section in your &lt;code class=&quot;highlighter-rouge&quot;&gt;post.html&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-tags&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% for tag in page.tags %}
      {% capture tag_name %}{{ tag }}{% endcapture %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;no-underline&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tag/{{ tag_name }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;code&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;highligher-rouge&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;nobr&amp;gt;&lt;/span&gt;{{ tag_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/nobr&amp;gt;&amp;lt;/code&amp;gt;&lt;/span&gt;&lt;span class=&quot;ni&quot;&gt;&amp;amp;nbsp;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;    
    {% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To do 2, Long Qian has written a very clear &lt;a href=&quot;https://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;post&lt;/a&gt; about it.&lt;/p&gt;

&lt;h2 id=&quot;5-add-mathjax&quot;&gt;5. Add MathJax&lt;/h2&gt;
&lt;p&gt;The last piece to my website is to add the support of $\LaTeX$-like math. This is done through MathJax. There are two steps to achieve it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a &lt;code class=&quot;highlighter-rouge&quot;&gt;mathjax.html&lt;/code&gt; file and put it in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_includes&lt;/code&gt; folder. Download the file &lt;a href=&quot;https://github.com/YangXiaozhou/yangxiaozhou.github.io/blob/master/_includes/mathjax.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Put the following line before &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/head&amp;gt;&lt;/code&gt; in your &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt;:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; {% include mathjax.html %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;to enbale MathJax on the page.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tips&quot;&gt;Tips&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;To use the normal dollar sign instead of the MathJax command (escape), put &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;span class=&quot;tex2jax_ignore&quot;&amp;gt;...&amp;lt;/span&amp;gt;&lt;/code&gt; around the text you don’t want MathJax to process.&lt;/li&gt;
  &lt;li&gt;Check out currently supported Tex/LaTeX commands by MathJax &lt;a href=&quot;https://docs.mathjax.org/en/latest/input/tex/macros/index.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;math-rendering-showcase&quot;&gt;Math Rendering Showcase&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Inline math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$...\$&lt;/code&gt;: $\mathbf{x}+\mathbf{y}$.&lt;/li&gt;
  &lt;li&gt;Displayed math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$\$...\$\$&lt;/code&gt; on a new paragraph:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic numbering and referencing using &lt;span class=&quot;tex2jax_ignore&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\ref{label}&lt;/code&gt;&lt;/span&gt;:
In (\ref{eq:sample}), we find the value of an interesting integral:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
  \int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15} \, .
  \label{eq:sample}
\end{align}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Multiline equations using &lt;code class=&quot;highlighter-rouge&quot;&gt;\begin{align*}&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \nabla \times \vec{\mathbf{B}} -\, \frac1c\, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \,,\newline
  \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That’s it for now. Happy blogging.&lt;/p&gt;

&lt;p&gt;Additional resources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set up &lt;a href=&quot;https://blog.webjeda.com/jekyll-categories/&quot;&gt;categories &amp;amp; tags&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Set up &lt;a href=&quot;http://joshualande.com/jekyll-github-pages-poole&quot;&gt;Disqus comments &amp;amp; Google Analytics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Add in social media &lt;a href=&quot;https://jreel.github.io/social-media-icons-on-jekyll/&quot;&gt;icons&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;kramdown &lt;a href=&quot;https://kramdown.gettalong.org/quickref.html&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MathJax &lt;a href=&quot;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>菜谱</title>
   <link href="http://localhost:4000/learning/2019/01/01/recipe.html"/>
   <updated>2019-01-01T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2019/01/01/recipe</id>
   <content type="html">&lt;p&gt;做过的好吃的东西，为了不忘记怎么做，把菜谱都收集起来放在这儿。有很多是跟老饭骨们学的，有些是跟我老妈学的，有视频教程的，我都收录在&lt;a href=&quot;https://www.youtube.com/playlist?list=PL9_EOuhN2bBxtcNacsP8Qtw_zv0oN2lJu&quot;&gt;下厨时间&lt;/a&gt;播放列表里。只要没有单独说的，一律按适量原则。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#宫保鸡丁&quot;&gt;宫保鸡丁&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#油泼猪手&quot;&gt;油泼猪手&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#腊肉糯米圆子&quot;&gt;腊肉糯米圆子&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;宫保鸡丁&quot;&gt;宫保鸡丁&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/kong_pao_chicken.jpg&quot; alt=&quot;宫保鸡丁&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;准备材料：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;肉：鸡腿肉去骨，切丁。&lt;/li&gt;
  &lt;li&gt;上浆：上一次葱姜水，抓匀，上蛋清，抓匀，再上一次姜葱水，抓匀。放入生抽、老抽、花雕酒，抓匀，再放入胡椒粉、盐、淀粉，抓匀。最后用&lt;strong&gt;少量油封住&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;小料：大葱花，蒜片，姜片。&lt;/li&gt;
  &lt;li&gt;料汁儿：少量盐，多点糖，胡椒粉，料酒，酱油，老抽，适量水淀粉，最后加上点&lt;strong&gt;葱姜蒜片尝味道&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;花生：热水泡了去皮。&lt;/li&gt;
  &lt;li&gt;干辣椒：切段。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;开始做：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;小火炸花生，慢慢炸到香脆。&lt;/li&gt;
  &lt;li&gt;另起锅，热锅冷油，小火滑鸡丁，滑好倒出。&lt;/li&gt;
  &lt;li&gt;小火煸炒花椒，捞出花椒，煸干辣椒。&lt;/li&gt;
  &lt;li&gt;煸好的辣椒花椒油中加入鸡丁，开大火，翻炒几下下葱姜蒜，炒出香味，下料汁儿，看颜色适量放老抽上色。&lt;/li&gt;
  &lt;li&gt;出锅前倒入少量花椒油和花生。&lt;/li&gt;
  &lt;li&gt;上菜！&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;上浆一定要耐心，每放一次调料就要抓匀。&lt;/li&gt;
  &lt;li&gt;调好的料汁儿需要尝一尝，根据味道再做调整，缺啥补啥。&lt;/li&gt;
  &lt;li&gt;花生需要小火慢炸，炸到香脆，微黄。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;油泼猪手&quot;&gt;油泼猪手&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/splash_oil_pig_trotters.jpg&quot; alt=&quot;油泼猪手&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;准备材料：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;洗净小猪手。&lt;/li&gt;
  &lt;li&gt;小葱切段，姜切片，蒜切片。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;开始做：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;焯水：小猪手冷水下锅焯水，&lt;strong&gt;水中放醋&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;备汤：高压锅内放水，盐，花雕酒，葱叶，姜片。&lt;/li&gt;
  &lt;li&gt;炖肉：焯好水的猪脚拿出用热水清洗干净，放入高压锅，炖四十分钟左右。&lt;/li&gt;
  &lt;li&gt;小火炸花生，把花生炸至香脆。&lt;/li&gt;
  &lt;li&gt;猪手炖好拿出来泡在冰水里降温，用手尽量掰成小块儿状，方便入味。&lt;/li&gt;
  &lt;li&gt;沥干水分的猪手中放入花生米，小葱，蒜片，香油，盐，醋，搅拌均匀。&lt;/li&gt;
  &lt;li&gt;另起锅，放入油和花椒，&lt;strong&gt;小火煸出花椒油&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;拌好的猪手中放入辣椒面（量依照个人口味），泼上热的花椒油（油泼猪手）。&lt;/li&gt;
  &lt;li&gt;上菜！&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;高压锅炖久一点能使猪手更软糯，根据喜好的口感可以自己选择时长。&lt;/li&gt;
  &lt;li&gt;提前备好足够的冰块，猪手出锅后让它尽量冷却，不然手掰起来非常烫。&lt;/li&gt;
  &lt;li&gt;辣椒面根据自己喜好放入，不能吃辣的注意别放太多。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;腊肉糯米圆子&quot;&gt;腊肉糯米圆子&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/sticky_rice_ball.jpg&quot; alt=&quot;腊肉糯米圆子&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>日本語の練習</title>
   <link href="http://localhost:4000/learning/2018/11/21/japanese-essay.html"/>
   <updated>2018-11-21T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2018/11/21/japanese-essay</id>
   <content type="html">&lt;p&gt;在学习日语过程中零零散散写过一些小短文，想着不如全部放到这儿来，当做自己的一个记录，也希望收到同在日语学习路上的朋友们的反馈。
A collection of Japanese practice writings. All comments and corrections are welcomed.&lt;/p&gt;

&lt;h3 id=&quot;2019年4月5日--私の夢&quot;&gt;2019年4月5日 ｜ 私の夢&lt;/h3&gt;

&lt;p&gt;　私の夢は好きな仕事を自由にしながら、社会の人たちに何か役に立つことができるようになることです。自由は一番大切なことだと若いから決めました。やっている仕事は自分が好きなのなら、いくら難しければ、続けられますから。それに、卒業の時、父に「好きなことをやろう」と言われて、安心しました。また、人たちの生活が良くなるために、大学で勉強しています。大学院の生活はいつも忙しいですが、日本語が習えて、好きな仕事ができて毎日とても嬉しいです。夢を持っている人生は幸せだと思っています。&lt;/p&gt;

&lt;h3 id=&quot;2019年4月3日&quot;&gt;2019年4月3日&lt;/h3&gt;
&lt;p&gt;｜ 祖母について&lt;/p&gt;

&lt;p&gt;　若い時から、祖母は娘の息子の私にいつも親切にしてくれています。家で宿題をする時、よく中国語の文法や数え方などの問題があったら、一生懸命に教えてくれていました。「自分で間違いを直せるよいに、頑張って。」と言われた、今でも、覚えて仕事をしています。旅行する時、色々な所のお土産を探して持って家に帰って、お祝いを祖母にあげれば、祖母は嬉しくなるので、私も嬉しいです。&lt;/p&gt;

&lt;p&gt;｜ 新聞の読み方について&lt;/p&gt;

&lt;p&gt;　先ず、見出しは何か読んでみます。これは表であるページです。このページを読めば、世界中で一番大きいニュースがわかります。よく政治や事故なニュースがこのページでありますが、時々社会と文化のニュースもあります。見出しのニュースは私の生活から遠いですが、知るのは必要だと思います。次に、スポーツが好きで、スポーツニュースへ行きます。全部読まなくて、バスケットボールの試合が気に入っています。両親は町の新聞を一番に読んでいます。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月28日特別な思い出が作るのはどうすればいいですか&quot;&gt;2019年3月28日　｜　特別な思い出が作るのは、どうすればいいですか。&lt;/h3&gt;

&lt;p&gt;AさんとBさんはハジレーンへ行ったことがないから、初めてハジレーンをゆっくり体験できるように、ツアーを作りました。ツアーの日は天気が良くなくて、シンガポールの普通の蒸し暑い天気でした。それに、私たちの四人ガイドは時々複雑な考えが話せなかったので、英語と日本語も上手なBさんに翻訳してもらいました。でも、ツアーの終わりにAさんとBさんは「今日は、ハジチャレンジに参加してよかったね。」と思ったと思います。&lt;/p&gt;

&lt;p&gt;いい写真が撮れるだけじゃなくて、何が特別な思い出が作れるように、ツアーの形を考えました。ハジーレンにはたくさん綺麗な落書きがあって、あの落書きを詳しく見れば、町のことがよく分かるし、特別な思い出もできると思いました。それで、ハジチャレンジが生まれました。でも、このツアーが成功した一番大切な理由はニコルさんが自分で作ったパンフレットでした。AさんとBさんはその美しいパンフレットを準備してもらって、びっくりしました。二人達は探している落書きが見つかった時、きっとパンフレットにはんこを押されるのは嬉しかったでしょう。手で作ったパンフレットを使うことや、紙で書いたものに、はんこを押すのは、何が特別な感じがします。今、本やパンフレットよりもっと情報が多くて、速くて、便利な携帯電話は大変人気がありますが、地図やパンフレットなどをまだ使っている人もいます。情報は変わらないので、紙のパンフレットを使えば周りの物にもっと気がつくかもしれません。それで、その時、そこで、人と自然の、また、人と人の意味がある関係ができます。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月20日&quot;&gt;2019年3月20日&lt;/h3&gt;

&lt;p&gt;　クレメンティは私の大好きな町です。ここは人が大勢いて、いつもにぎやかなんですが、困りません。それは、色々な店が品物を売って、買い物の途中でお腹がへったら、狭い道で値段がそんなに高くない食べ物や飲み物が買えます。恥ずかしくなくて、飲み物を飲みながら、買い物を選びましょう。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月4日&quot;&gt;2019年3月4日&lt;/h3&gt;

&lt;p&gt;　昔から、中国は世界中の色々な国の人を招待しています。他の国と仲良くすれば、商品を輸入できます。たとれば、よく石油と他の原料を輸入して、美しい池の絵とか、米とか、お茶を輸出しています。今も、たくさんの国に頼っています。&lt;/p&gt;

&lt;p&gt;　去年11月に行ったフィリピンにあるマラパスクア島はとても綺麗な島です。毎日海岸で散歩する前に卵入れて朝ごはんを食べました。この島は小さいですから、工場は全然建ててありません。村の中で、豚と鳥をたくさん育てています。しかし、飲まれる水はあまり足らなくて、人たちの生活は難しいです。&lt;/p&gt;

&lt;h3 id=&quot;2019年2月18日&quot;&gt;2019年2月18日&lt;/h3&gt;

&lt;p&gt;　シンガポール島はマレーシアの南向こうの島です。この島は小さいですが、木や花が多くて、そして、木の葉も道でよく見えます。マレーシアまで二つ橋を建てています。この島で、人たちの生活は昔から大変じゃなくて、早く結婚すれば、住む家が買えます。&lt;/p&gt;

&lt;p&gt;　今日は土曜日です。初めて船で乗って、海を渡って，ウビン島に行った。一時間ぐらい、海で泳いだ。島でたくさん野菜と魚を食べられた。味はシンガポールのと違っだ。&lt;/p&gt;

&lt;h3 id=&quot;2019年1月28日&quot;&gt;2019年1月28日&lt;/h3&gt;

&lt;p&gt;　私の家の近くに空港がありませんから、飛行機を見ていませんでした。でも、家の近くにたくさん綺麗な山や川があります。春になれば、山は緑の衣をつけています。私の子供の時、よく父は山登りに連れて行ってくれました。公園へ行くのより面白いと思っていました。それで、地質学者になろうと決めました。&lt;/p&gt;

&lt;p&gt;　自動車を運転している時、とても気を付けなければなりませんでした。特に、曲がる時は、よく交通事故があるから、本当に危険だと思います。曲がる前には、車の席から左と右を見たらいいと思います。&lt;/p&gt;

&lt;h3 id=&quot;2018年11月21日順天堂大学のみなさんとの交流から学んだこと&quot;&gt;2018年11月21日　｜　順天堂大学のみなさんとの交流から学んだこと&lt;/h3&gt;

&lt;p&gt;　年を取る人が多くて、若い人があまり多くないので、介護離職はたしかに日本にとって大きい社会的問題です。そして、この問題の答えは、早く考えなければなりません。日本では介護サービスにかかる費用がとても高いので、普通の家族は、あまり払う事ができないですから、主に家族が介護をしています。さらに、介護者の人数は今よりもっとたくさん必要になります。もしそうでなければ、コストは絶対より高くなると思います。順天堂大学の学生が発表した事について、第一と第二とあんの会社員が介護休暇や介護休業を取ったり、フレックスタイム制が有ったりする会社は大切だと言いました。それは私もそう思いますが、日本の会社は残業が多くて、競争がとても厳しいですから、難しいと思います。三つ目のあんは、いいと思います。外国人の労働者は、もうシンガポールや香港では長い間います。確かに、おじいさんやおばあさんは、外国人と一緒に暮らして、世話をしてもらう事が嫌いかもしれません。でも、私は半年ぐらい京都で住んでいましたから、外国人は日本人と一緒に暮らすことができると思います。外国人のヘルパーさんは日本語や日本の文化を勉強してから、働くことが解決策の一つだと思います。&lt;/p&gt;
</content>
 </entry>
 

</feed>
