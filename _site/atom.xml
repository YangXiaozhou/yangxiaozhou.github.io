<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xiaozhou's Notes</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-03-28T23:22:26+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Yang Xiaozhou</name>
   <email></email>
 </author>

 
 <entry>
   <title>Tracking the COVID-19 Disease Outbreak and Signals of Containment</title>
   <link href="http://localhost:4000/2020/03/24/COVID-19.html"/>
   <updated>2020-03-24T08:00:00+08:00</updated>
   <id>http://localhost:4000/2020/03/24/COVID-19</id>
   <content type="html">&lt;p&gt;I update the current COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and any emerging signals of containment. Latest update: March 28, 2020&lt;/p&gt;

&lt;h2 id=&quot;1-prognosis&quot;&gt;1. Prognosis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;United States&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The number of confirmed infections has been increasing by 30% daily for the past few days. This is expected to continue, given the large number of testings being carried out right now (&lt;a href=&quot;#US Testing Numbers&quot;&gt;60K test results&lt;/a&gt; are pending).&lt;/li&gt;
      &lt;li&gt;There is an upward trend in the case fatality rate, likely due to an overwhelmed healthcare system.&lt;/li&gt;
      &lt;li&gt;On the upside, though, it seems like people are actively protecting themselves now as the &lt;a href=&quot;#Google Search Interest&quot;&gt;google search&lt;/a&gt; for “mask” is at an all-time high.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Europe&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Europe is probably reaching the end of the tunnel as the number of daily confirmed cases is increasing at a &lt;a href=&quot;#Percentage Change&quot;&gt;slower rate every day&lt;/a&gt;, with Italy, the slowest, at 10%, and Spain, the fastest, at 18%.&lt;/li&gt;
      &lt;li&gt;The Italian healthcare system faced a grave challenge, with the case fatality rate currently at 10.6% and still increasing. Spain seems to be heading &lt;a href=&quot;#case fatality rate&quot;&gt;in the same direction&lt;/a&gt; with a 7.8% fatality rate. Germany, though, fares well in this regard and manages to record only a 0.7% fatality rate.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Asia&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Most of Asia still seems to have the outbreak under control, especially in places like China, Singapore, Hong Kong, Taiwan, and South Korea.&lt;/li&gt;
      &lt;li&gt;There’s been a surge in the number of imported cases due to mainly home-coming citizens to places like China and Singapore. Governments have imposed stricter quarantine procedures since then.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-signals-of-containment&quot;&gt;2. Signals of Containment&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;Percentage Change&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;percentage-change&quot;&gt;Percentage Change&lt;/h3&gt;
&lt;p&gt;7-day moving average (MA) of the percentage change in confirmed cases reported in selected countries. It is important to see both the percentage change and the trend of the percentage. To easily classify the situation, we can use the following scale&lt;sup id=&quot;fnref:percentage&quot;&gt;&lt;a href=&quot;#fn:percentage&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r &gt; 100\%&lt;/script&gt;: &lt;strong&gt;Rapidly increasing&lt;/strong&gt;. The total number of cases is doubling everyday.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
10\% &lt; r &lt; 100\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
1\% &lt; r &lt; 10\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Slowly increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 1\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Under control&lt;/strong&gt;. There is almost no new case reported.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;696&quot; height=&quot;431.99999999999994&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1897242208&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;Google Search Interest&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;google-search-interest&quot;&gt;Google Search Interest&lt;/h3&gt;
&lt;p&gt;This figure tells us how many people in the US are searching for keywords such as “hand sanitizer” or “symptom”. I suspect that as the community spread of the virus is being contained, we can expect to see a drop in searches for words like “symptom” and “influenza”, similar to the trends shown in Singapore.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=783455223&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=196247116&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;US Testing Numbers&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;us-testing-numbers&quot;&gt;US Testing Numbers&lt;/h3&gt;
&lt;p&gt;As the containment takes effect, we expect to see the number of positive and negative tests stabilize, and the number of tests pending result drops.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=481777218&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;confirmed-cases&quot;&gt;Confirmed Cases&lt;/h2&gt;
&lt;iframe width=&quot;696.0000000000001&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=967719983&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1124767386&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;death-cases&quot;&gt;Death Cases&lt;/h2&gt;
&lt;p&gt;&lt;a name=&quot;case fatality rate&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;other-tracking-websites-worth-visting&quot;&gt;Other tracking websites worth visting:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Bloomberg&lt;/strong&gt;: &lt;a href=&quot;https://www.bloomberg.com/graphics/2020-coronavirus-cases-world-map/?srnd=premium-asia&quot;&gt;Mapping the Coronavirus Outbreak Across the World&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;JHU Center for Systems Science and Engineering&lt;/strong&gt;: &lt;a href=&quot;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Coronavirus COVID-19 Global Cases&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:percentage&quot;&gt;
      &lt;p&gt;The percentage only indicates a relative change. The actual number of new cases reported in each country may be very different, as it depends on the absolute number of cumulative cases in that country. &lt;a href=&quot;#fnref:percentage&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Roadmap of Statistical Learning</title>
   <link href="http://localhost:4000/statistics/2020/01/10/statistical_learning_map.html"/>
   <updated>2020-01-10T08:00:00+08:00</updated>
   <id>http://localhost:4000/statistics/2020/01/10/statistical_learning_map</id>
   <content type="html">&lt;p&gt;A (work-in-progress) roadmap for statistical learning concepts and tools.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Regularised Linear Regression
    &lt;ul&gt;
      &lt;li&gt;Ridge regression&lt;/li&gt;
      &lt;li&gt;Lasso regression&lt;/li&gt;
      &lt;li&gt;Logistic (ridge/lasso) regression&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Basis Expansion Method
    &lt;ul&gt;
      &lt;li&gt;Approximate mean function in a piecewise way
        &lt;ul&gt;
          &lt;li&gt;kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions&lt;/li&gt;
          &lt;li&gt;Positive part remain operator (x - r)+ to ensure continuity at knots&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Additive Model (semi-parametric method, building on top of basis expansion method)
    &lt;ul&gt;
      &lt;li&gt;More flexible than linear but retains the interpretability
        &lt;ul&gt;
          &lt;li&gt;Partially linear additive model: summation of linear variables and functions of variable
            &lt;ul&gt;
              &lt;li&gt;Nonlinear variables can be assumed to be represented by spline basis&lt;/li&gt;
              &lt;li&gt;What if a variable influences the relationship between Y and X?
                &lt;ul&gt;
                  &lt;li&gt;Varying coefficient regression model: variable coefficients are functions of a variable(Z)&lt;/li&gt;
                  &lt;li&gt;Such function can be assumed to have good estimation by spline method&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Generalised: extend the method beyond linear link function, e.g. logistic&lt;/li&gt;
      &lt;li&gt;If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Local Averaging Method
    &lt;ul&gt;
      &lt;li&gt;To estimate the regression surface, we can use local averaging method by defining two things
        &lt;ol&gt;
          &lt;li&gt;What is “local”? i.e. how do we partition the space/find out the regression surface?&lt;/li&gt;
          &lt;li&gt;How to compute “average”? i.e. simple average, weighted average?&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Point 1 above
        &lt;ul&gt;
          &lt;li&gt;Binary recursive method: Regression and classification tree (CART)&lt;/li&gt;
          &lt;li&gt;Neighbourhood method: identifies a neighbour through some metric (kNN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Point 2 above
        &lt;ul&gt;
          &lt;li&gt;Simple average: CART, kNN&lt;/li&gt;
          &lt;li&gt;Weighted average: weighted kNN, see point 7 below&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Kernel Smoothing
    &lt;ul&gt;
      &lt;li&gt;Replace the indicator function by a kernel function (symmetric pdf)&lt;/li&gt;
      &lt;li&gt;Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function&lt;/li&gt;
      &lt;li&gt;The value of h defines the size of the neighbourhood&lt;/li&gt;
      &lt;li&gt;Hence h determines the trade-off between model complexity and stability&lt;/li&gt;
      &lt;li&gt;But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        &lt;ul&gt;
          &lt;li&gt;LLKS&lt;/li&gt;
          &lt;li&gt;LPKS&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).&lt;/li&gt;
  &lt;li&gt;Dimension-reduction based method
    &lt;ol&gt;
      &lt;li&gt;Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        &lt;ul&gt;
          &lt;li&gt;More flexible than linear regression, but also more interpretable than PPR&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Ridge function &amp;gt; 1: Projection pursuit regression: one projection direction for each ridge function
        &lt;ul&gt;
          &lt;li&gt;Approximate target by non-linear function of the linear combination of input&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Machine Learning
    &lt;ul&gt;
      &lt;li&gt;Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).&lt;/li&gt;
      &lt;li&gt;Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.&lt;/li&gt;
      &lt;li&gt;Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.&lt;/li&gt;
      &lt;li&gt;How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?&lt;/li&gt;
      &lt;li&gt;Classification
        &lt;ul&gt;
          &lt;li&gt;Fisher’s linear discriminant: linear combination of dimensions of X -&amp;gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Linear methods
    &lt;ul&gt;
      &lt;li&gt;Linear regression&lt;/li&gt;
      &lt;li&gt;Linear discriminant analysis&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-linear methods
    &lt;ul&gt;
      &lt;li&gt;SVM&lt;/li&gt;
      &lt;li&gt;Discriminant analysis
        &lt;ol&gt;
          &lt;li&gt;QDA ( assume unequal variance)&lt;/li&gt;
          &lt;li&gt;Flexible discriminant analysis&lt;/li&gt;
          &lt;li&gt;Mixture discriminant analysis&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Inference for dynamic systems/state-space models(SSMs)
    &lt;ul&gt;
      &lt;li&gt;Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.&lt;/li&gt;
      &lt;li&gt;Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.&lt;/li&gt;
      &lt;li&gt;Non-linear dynamics and/or non-Gaussian noise:
        &lt;ol&gt;
          &lt;li&gt;Extended Kalman filter&lt;/li&gt;
          &lt;li&gt;Unscented Kalman filter&lt;/li&gt;
          &lt;li&gt;Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            &lt;ol&gt;
              &lt;li&gt;Importance sampling to tackle difficult-to-sample posterior distribution problem&lt;/li&gt;
              &lt;li&gt;Recursive formulation to tackle online inference complexity problem&lt;/li&gt;
              &lt;li&gt;Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;Particle filtering
            &lt;ol&gt;
              &lt;li&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;Particle smoothing&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Particle filters: vallina and advanced</title>
   <link href="http://localhost:4000/statistics/2020/01/10/particle-filter.html"/>
   <updated>2020-01-10T08:00:00+08:00</updated>
   <id>http://localhost:4000/statistics/2020/01/10/particle-filter</id>
   <content type="html">&lt;p&gt;The problem - Non-linear dynamic state estimation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;state-space model for dynamic systems&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other methods - Kalman-based fiter?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;limitations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The method - Bayesian estimation through Monte Carlo method&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bayesian estimation&lt;/li&gt;
  &lt;li&gt;Monte Carlo method&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>N1-キクタン日本語</title>
   <link href="http://localhost:4000/learning/2020/01/06/Japanese-n1-words.html"/>
   <updated>2020-01-06T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2020/01/06/Japanese-n1-words</id>
   <content type="html">&lt;h3 id=&quot;day-1&quot;&gt;Day 1&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;アルバイトで学業（がくぎょう）がおろそかにになる大学生もいる。&lt;/li&gt;
  &lt;li&gt;来週の金曜日までに、この課題（かだい）を提出してください。&lt;/li&gt;
  &lt;li&gt;田中さんの学位は修士（しゅうし）です。&lt;/li&gt;
  &lt;li&gt;中学受験のため、子どもを塾（じゅく）に通わせることにした。&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>读书笔记：读书体验是什么</title>
   <link href="http://localhost:4000/reading/2019/11/12/how-to-read.html"/>
   <updated>2019-11-12T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2019/11/12/how-to-read</id>
   <content type="html">&lt;p&gt;我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;书名：&lt;a href=&quot;https://book.douban.com/subject/26789567/&quot;&gt;如何有效阅读一本书&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;原名：読書は1冊のノートにまとめなさい&lt;/li&gt;
  &lt;li&gt;作者：&lt;a href=&quot;http://okuno0904.com/about/index.html&quot;&gt;奥野宣之&lt;/a&gt;（おくの・のぶゆき）&lt;/li&gt;
  &lt;li&gt;购入日期：2019.11.11（打折！）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;读书体验&quot;&gt;读书体验&lt;/h3&gt;
&lt;p&gt;这本149页的小书，我觉得可以看做是“如何读书”的一本参考书；里面介绍的观点和方法，不同级别的读者都可以在适当的时候读一读，并学到些东西。这本小书旨在回答这样一个问题：如何才能不忘记读过的书中的内容，并使之融入身心，真正使书籍影响自己？&lt;/p&gt;

&lt;p&gt;而他的回答是，我们应该重新思考读书这件事儿。读书不应始于翻开书籍，也不终于最后一页。每一次读书，我们应该去创造属于自己的“读书体验”。要如何理解这个“体验”呢？我们可以想想生活中的其他体验，尤其是使用体验。我不经常买东西，但如果要买什么，会尽量去想清楚为什么要买，会提前去了解这个东西的背景，功能，评价，使用过程中会不断更新最初的设想，并持续影响我下一次购物的判断。这也就是一次有意识有目的能影响未来的购物体验。也就是说，作者推崇的是一种有目的能重温、甚至历久弥新的读书体验。实际上，作者也认为，这样的读书体验比书本身重要多了。&lt;/p&gt;

&lt;p&gt;作者总结了下面五个具体可行的步骤来创造所谓的读书体验：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;选书&lt;/strong&gt;：收集随想，建立目的&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;购书&lt;/strong&gt;：冷静评估，书籍确认&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;读书&lt;/strong&gt;：适当标记，提炼重点&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;记录&lt;/strong&gt;：原文摘抄，原创思考&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;活用&lt;/strong&gt;：重读笔记，思想输出&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而在作者的心中，创造这个读书体验必不可少的伙伴是一个普通的笔记本。因为它在上面五个步骤中所发挥的重要作用，这笔记本应时常伴我们左右，并且，如果直译本书的原书名，你会发现，它其实意思是“请用一个笔记本整理你的读书”。&lt;/p&gt;

&lt;h3 id=&quot;用购书清单指名购书&quot;&gt;用购书清单指名购书&lt;/h3&gt;
&lt;p&gt;创造读书体验的第一步，是给自己开出一个购书清单。开清单不是为了逛书店的时候容易找（也有这好处），更重要的是，让我们能清楚认识到读每一本书的目的是什么。作者是这么说的：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;那么，为什么要把列清单的过程也作为读书方法的一部分来说明呢？理由之一，就是要培养带着目的去读书的目的意识。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;下面是作者推荐的选书购书的具体操作步骤：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;好奇心激发&lt;/strong&gt; → &lt;strong&gt;随想笔记&lt;/strong&gt; → &lt;strong&gt;购书清单&lt;/strong&gt; → &lt;strong&gt;购书&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一步是将激发好奇心的源头记进笔记本里，可以叫做随想笔记。这源头的可能性就很多了：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报刊上读到有意思的书评&lt;/li&gt;
  &lt;li&gt;听到感兴趣的政治时事评论&lt;/li&gt;
  &lt;li&gt;来自朋友的书籍推荐&lt;/li&gt;
  &lt;li&gt;等等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;只要是激起了我们好奇心的东西，都应该记进随想笔记里去。之后便能根据随想笔记，建立起读书的目的，并去寻找相关书籍。在经过冷静的评估之后，将想要购买的书籍列进清单。这流程的好处是，我们相对能够准确地选出自己真正想读并且明白为什么想读的书，这样买回来感兴趣、读下去的几率都比较高（这个很重要😂）。并且，在读书过程中，可以带着最初被激发的好奇心去读，读完之后也能回顾一开始好奇心被激发的契机，因为这些都被一一记录在笔记本里。&lt;/p&gt;

&lt;h3 id=&quot;用笔记把读过的书变为精神财富&quot;&gt;用笔记把读过的书变为精神财富&lt;/h3&gt;
&lt;p&gt;这里讲的包含了步骤三和四：读书，记录。&lt;/p&gt;

&lt;p&gt;读书的时候，我们遵循这样一个流程：通读 → 片段重读 → 标记。意思就是，通读之后，去重读你感到有共鸣、疑惑、感兴趣等等的片段，有必要就用统一符号标记下来，比如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;下划直线 ＿＿：客观重要&lt;/li&gt;
  &lt;li&gt;下划波浪线 ˷˷˷˷˷˷：我觉得重要，非常重要&lt;/li&gt;
  &lt;li&gt;圆圈 ◯：关键词、专业名称&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样读过一个章节、一本书，就能进入到下一个步骤：记录。这个时候就可以一一回顾上一步所标记出来的部分，参照下面的格式，在笔记本上写下读过这本书后的读书笔记：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;⚬ …接原文摘抄、要点概括&lt;/li&gt;
  &lt;li&gt;⭑ …接评论、感想&lt;/li&gt;
  &lt;li&gt;重复上面&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体来说，我们摘抄的时候，可以摘抄些什么呢？&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;能让我主观产生共鸣的&lt;/li&gt;
  &lt;li&gt;不是读后觉得”理应如此“，而是“这么一说确实如此”的&lt;/li&gt;
  &lt;li&gt;能颠覆我已有的想法、动摇我认识的&lt;/li&gt;
  &lt;li&gt;等等&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;摘抄或要点概括很大程度上是原作者的思想，而在评论里，我们应该尽量去挖掘些原创的思考。这会是个耗时间费精力的过程，不过只有试过的人才能知道到底值不值得。另外值得一提的是，作者还提倡将跟这本书有关的时间、空间印记也一起贴进笔记本里，比如说新书买来时的书腰、看那本书时所坐火车的票根等等。以后的某个时间，再次读起笔记本的这一页，看那本书时所经过的风景闻过的花香也会跃然纸上吧。&lt;/p&gt;

&lt;p&gt;当然了，上面讲的记录的形式都是作者的推荐，我们大可不必居于某种形式，只需按照自己舒服的方式坚持记下属于自己的读书笔记就好了：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;说句老生常谈的话，只有把读书笔记控制在自己能力允许的范围内，才能长久地坚持下去。所以，要选择对自己来说比较方便的笔记方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我想这道理几乎适用于所有需要长久坚持的事，探究一门深奥的学问、学习一门外语、亦或是健身减肥等等。我发现人往往能轻松看到漫长过程之后的一种状态，这或许是我们这样高等生物的特殊能力；但人又往往忍不住会对期待的状态过于着急。这的确与我们身处的社会环境有所关系，但在我们的基因当中是否也有着这种既有远见又企求触手可得的回报的种子呢？&lt;/p&gt;

&lt;p&gt;我在读这本书的时候所做的读书笔记就没有按照作者推荐的格式，而是采用了自己习惯的类似于PPT设计的风格：
&lt;img src=&quot;/assets/2019-11-12/how_to_read.jpg&quot; alt=&quot;how_to_read&quot; /&gt;
不难看出，笔记里的结构几乎原封不动地变成了我这篇文章的结构，再加上在书里相关标记写下的评论，这篇文章的主要内容在我做完读书笔记的同时也就完成了。而这也是作者推崇的，以自己的读书笔记为基础，进一步写出原创文章，做属于自己的思想的输出。&lt;/p&gt;

&lt;h3 id=&quot;通过重读笔记提高自我&quot;&gt;通过重读笔记提高自我&lt;/h3&gt;
&lt;p&gt;读书体验的最后一步，也是我从没做过的一步：重读笔记。就像有人会偶尔重读日记一样，时常重读读书笔记能让自己读过的书好像一直存在自己生活中，不断酝酿，不断跟自己的经历、知识发生新的碰撞：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;如果把一本书比作一个“场所”，那么读书笔记就是在这个场所拍摄的照片。在不同时间去同一个场所拍照，拍出来的照片都会有所不同，而过一段时间再去看这些照片，对那个场所的印象也会发生变化。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于如何重读，作者推荐：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;简单回顾：读笔记&lt;/li&gt;
  &lt;li&gt;细致回顾：读笔记 + 原书标记&lt;/li&gt;
  &lt;li&gt;经典重温：读笔记 + 原书&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这么看，我确实一个回顾都没做过😂。一开始读这本书是因为买了新的笔记本，我老是买新笔记本，想着要写点什么文字，最后都沦为平时工作用的草稿纸（也是很重要啦）。看了这本书的评论觉得可能会帮我结束这个循环，目前看来很有希望。读之前，如何写读书笔记是我最感兴趣的部分，不过读完发现，以随想笔记到笔记回顾的一整个读书体验来理解读书这事儿，才是奥野宣之这本书给我最大的收获吧。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>深度工作：一个研究者（我自己）应该做的事</title>
   <link href="http://localhost:4000/reading/2019/11/11/academic-deep-work.html"/>
   <updated>2019-11-11T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2019/11/11/academic-deep-work</id>
   <content type="html">&lt;p&gt;什么是深度工作？为什么即时讯息不会提高而是降低我的工作效率？为什么忙绿工作了十个小时却并没完成什么重要的事情？为什么我要选择鼓励深度工作的公司？对于这些问题，Newport在他的深度工作里做出了回答。我想利用这篇文章来总结一下此书，读后的感想以及能为我所用的知识。后半段将结合马华灵先生建议的学术江湖的七种武器，提炼出对于我这般刚上路的研究者，应该花足够精力和时间去做的事儿，换言之：一个研究者应该做的深度工作。&lt;/p&gt;

&lt;h2 id=&quot;深度工作&quot;&gt;深度工作&lt;/h2&gt;
&lt;p&gt;Cal Newport的《深度工作》&lt;a href=&quot;https://www.calnewport.com/books/deep-work/&quot;&gt;(Deep Work)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;ritualize-the-deep-work&quot;&gt;Ritualize the deep work&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Where you’ll work and for how long?
    &lt;ul&gt;
      &lt;li&gt;制定好深度工作的地点与时间，尽最大可能保持这个地点和时间。如果中间有会议或者seminar之类的，提前调整好时间，算作是浅型工作，总的来说要保持弹性。
        &lt;ul&gt;
          &lt;li&gt;Place of work: CREATE office; Duration: 9am - 5pm:
            &lt;ul&gt;
              &lt;li&gt;Morning deep work: 9am - 11am&lt;/li&gt;
              &lt;li&gt;Lunch break: 11am - 1230pm&lt;/li&gt;
              &lt;li&gt;Afternoon shallow work: 1230pm - 0130pm&lt;/li&gt;
              &lt;li&gt;Nap: 0130pm - 2pm&lt;/li&gt;
              &lt;li&gt;Afternoon deep work: 2pm - 5pm&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Optional
            &lt;ul&gt;
              &lt;li&gt;Dinner break: 5pm - 6pm&lt;/li&gt;
              &lt;li&gt;Night deep work: 6pm - 7pm&lt;/li&gt;
              &lt;li&gt;Night shallow work/shutdown: 7pm - 8pm&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How you’ll work once you start to work?
    &lt;ul&gt;
      &lt;li&gt;制定好深度工作时的规矩，必须遵循这个规矩。这个规矩应该可以尽可能实际，也能保证深度工作的效果。&lt;/li&gt;
      &lt;li&gt;深度工作时：
        &lt;ul&gt;
          &lt;li&gt;坚决不使用手机；&lt;/li&gt;
          &lt;li&gt;电脑上不查email，不浏览跟工作无关的网页，不看跟工作无关的文件。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How you’ll support your work?
    &lt;ul&gt;
      &lt;li&gt;尽可能从环境、吃喝等等方面帮助完成上面的ritual。&lt;/li&gt;
      &lt;li&gt;准备：
        &lt;ul&gt;
          &lt;li&gt;早晨深度工作开始之前喝一杯咖啡，并将水壶接满水；&lt;/li&gt;
          &lt;li&gt;9点开启潮汐专注模式，并将手机放进抽屉；&lt;/li&gt;
          &lt;li&gt;下午2点开始潮汐专注模式，并将手机放进抽屉。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;embrace-boredom&quot;&gt;Embrace boredom&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;This is to prevent our brains from being addicted to distractions. Resist the temptation to use handphone or surf the internet at the slightest hint of boredom.&lt;/li&gt;
  &lt;li&gt;Memory training helps one’s ability to concentrate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;quit-social-media&quot;&gt;Quit social media&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Put conscience effort in scheduling off-work hour activities. Our brain relaxes when we swtich focus, not by induldging in semi-conscious web surfing.
    &lt;ul&gt;
      &lt;li&gt;晚上的时候不固定具体时间和时长，但是尽量做到三件事：
        &lt;ul&gt;
          &lt;li&gt;背N1单词&lt;/li&gt;
          &lt;li&gt;读正在读的书&lt;/li&gt;
          &lt;li&gt;11点睡床上&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;学术武器&quot;&gt;学术武器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.douban.com/note/671893735/&quot;&gt;马华灵：我的思想历程（附：学术江湖的七种武器）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;马华灵根据自己的经历总结出了做学术需要修炼的七大“武器”，这七个方向包括了对学术人的追求的拷问，也有对做学术的具体操作的指导，下面列出他的七大“武器”以及我的一些总结和思考。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;学术源泉
    &lt;ul&gt;
      &lt;li&gt;我为什么做学术？&lt;/li&gt;
      &lt;li&gt;什么是学术？&lt;/li&gt;
      &lt;li&gt;学术的发展历史是怎样的？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术问题
    &lt;ul&gt;
      &lt;li&gt;我的学术问题是什么？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术概念
    &lt;ul&gt;
      &lt;li&gt;我对我使用的概念真的了解了吗？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术阅读
    &lt;ul&gt;
      &lt;li&gt;梳理逻辑框架&lt;/li&gt;
      &lt;li&gt;把握文章论述重点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术笔记
    &lt;ul&gt;
      &lt;li&gt;记录并明白核心框架&lt;/li&gt;
      &lt;li&gt;建立一个有层次的文章思路地图（i.e. Sections, subsections, sub-subsections）&lt;/li&gt;
      &lt;li&gt;注明文章的出版信息和页码&lt;/li&gt;
      &lt;li&gt;尝试对文章的每一步进行质问&lt;/li&gt;
      &lt;li&gt;总结文章的优点和缺点，思考优点如何借鉴，缺点如何避免与解决&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术批评
    &lt;ul&gt;
      &lt;li&gt;建设性批评&lt;/li&gt;
      &lt;li&gt;目的是能完善文章&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学术写作
    &lt;ul&gt;
      &lt;li&gt;逻辑清晰&lt;/li&gt;
      &lt;li&gt;观点明确&lt;/li&gt;
      &lt;li&gt;化整为零，然后化零为整（i.e. paragraph to subsection, subsection to section, section to paper）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Beyond LDA: Flexible, penalized, and mixture discriminant analysis</title>
   <link href="http://localhost:4000/statistics/2019/10/03/beyond-lda.html"/>
   <updated>2019-10-03T08:00:00+08:00</updated>
   <id>http://localhost:4000/statistics/2019/10/03/beyond-lda</id>
   <content type="html">&lt;p&gt;This is the second article that talks about the classification and dimension reduction tool LDA.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;flexible-discriminant-analysis-fda&quot;&gt;Flexible discriminant analysis (FDA)&lt;/h3&gt;
&lt;h3 id=&quot;penalized-discriminant-analysis-pda&quot;&gt;Penalized discriminant analysis (PDA)&lt;/h3&gt;
&lt;h3 id=&quot;mixture-discriminant-analysis-mda&quot;&gt;Mixture discriminant analysis (MDA)&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>LDA: Linear discriminant analysis in vanilla form</title>
   <link href="http://localhost:4000/statistics/2019/10/02/linear-discriminant-analysis.html"/>
   <updated>2019-10-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/statistics/2019/10/02/linear-discriminant-analysis</id>
   <content type="html">&lt;p&gt;Linear discriminant analysis (LDA) is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces stable, effective, and interpretable classification results. Therefore, when tackling a classification problem, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed.&lt;/p&gt;

&lt;p&gt;Two prominent examples of using LDA (and it’s variants) include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Bankruptcy prediction&lt;/em&gt;: Edward Altman’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Altman_Z-score&quot;&gt;1968 model&lt;/a&gt; predicts the probability of company bankruptcy using trained LDA coefficients. The accuracy is said to be between 80% and 90%, evaluated over 31 years of data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Facial recognition&lt;/em&gt;: While features learned from Principal Component Analysis (PCA) are called Eigenfaces, those learned from LDA are called &lt;a href=&quot;http://www.scholarpedia.org/article/Fisherfaces&quot;&gt;Fisherfaces&lt;/a&gt;, named after the statistician, Sir Ronald Fisher. We explain this connection later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article starts by introducing the classic LDA and its reduced-rank version. Then we summarize the merits and disadvantages of LDA. The second article following this generalizes LDA to handle more complex problems. By the way, you can find a set of &lt;a href=&quot;/assets/2019-10-02/Discriminant_Analysis.pdf&quot; target=&quot;_blank&quot;&gt;corresponding slides&lt;/a&gt; where I present roughly the same materials written in this article.&lt;/p&gt;

&lt;h3 id=&quot;classification-by-discriminant-analysis&quot;&gt;Classification by discriminant analysis&lt;/h3&gt;
&lt;p&gt;Consider a generic classification problem: A random variable $X$ comes from one of $K$ classes, $G = 1, \dots, K$, with density $f_k(\mathbf{x})$ on $\mathbb{R}^p$. A discriminant rule divides the space into $K$ disjoint regions $\mathbb{R}_1, \dots, \mathbb{R}_K$. Classification by discriminant analysis simply means that we allocate $\mathbf{x }$ to $\Pi_{j}$ if $\mathbf{x} \in \mathbb{R}_j$. We can follow two allocation rules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Maximum likelihood rule&lt;/em&gt;: If we assume that each class could occur with equal probability, then allocate $\mathbf{x }$ to $\Pi_{j}$ if $j = \arg\max_i f_i(\mathbf{x})$ .&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bayesian rule&lt;/em&gt;: If we know the class prior probabilities, $\pi_1, \dots, \pi_K$, then allocate $\mathbf{x }$ to $\Pi_{j}$ if $j = \arg\max_i \pi_i f_i(\mathbf{x}) $.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-and-quadratic-discriminant-analysis&quot;&gt;Linear and quadratic discriminant analysis&lt;/h4&gt;
&lt;p&gt;If we assume data comes from multivariate Gaussian distribution, i.e. $X \sim N(\mathbf{\mu}, \mathbf{\Sigma})$, explicit forms of the above allocation rules can be obtained. Following the Bayesian rule, we classify $\mathbf{x}$ to $\Pi_{j}$ if $j = \arg\max_i \delta_i(\mathbf{x})$ where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \delta_i(\mathbf{x}) = \log f_i(\mathbf{x}) + \log \pi_i
\end{align}&lt;/script&gt;

&lt;p&gt;is called the discriminant function. Note the use of log-likelihood here.  The decision boundary separating any two classes, $k$ and $\ell$, is the set of $\mathbf{x}$ where two discriminant functions have the same value, i.e. &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{x}: \delta_k(\mathbf{x}) = \delta_{\ell}(\mathbf{x})\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;LDA arises in the case where we assume equal covariance among $K$ classes, i.e. $\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \dots = \mathbf{\Sigma}_K$. Then we can obtain the following discriminant function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = \mathbf{x}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}-\frac{1}{2} \mathbf{\mu}_{k}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}+\log \pi_{k} \,.
    \label{eqn_lda}
\end{align}&lt;/script&gt;

&lt;p&gt;This is a linear function in $\mathbf{x}$. Thus, the decision boundary between any pair of classes is also a linear function in $\mathbf{x}$, the reason for its name: linear discriminant analysis. Without the equal covariance assumption, the quadratic term in the likelihood does not cancel out, hence the resulting discriminant function is a quadratic function in $\mathbf{x}$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = 
    - \frac{1}{2} \log|\mathbf{\Sigma}_k| 
    - \frac{1}{2} (\mathbf{x} - \mathbf{\mu}_{k})^{T} \mathbf{\Sigma}_k^{-1} (\mathbf{x} - \mathbf{\mu}_{k}) + \log \pi_{k} \,.
    \label{eqn_qda}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the decision boundary is quadratic in $\mathbf{x}$. This is known as quadratic discriminant analysis (QDA).&lt;/p&gt;

&lt;h4 id=&quot;number-of-parameters&quot;&gt;Number of parameters&lt;/h4&gt;
&lt;p&gt;In real problems, population parameters are usually unknown and estimated from training data as $\hat{\pi}_k, \hat{\mathbf{\mu}}_k, \hat{\mathbf{\Sigma}}_k$. While QDA accommodates more flexible decision boundaries compared to LDA, the number of parameters needed to be estimated also increases faster than that of LDA. From (\ref{eqn_lda}), $p+1$ parameters (nonlinear transformation of the original distribution parameters) are needed to construct the discriminant function. For a problem with $K$ classes, we would only need $K-1$ such discriminant functions by arbitrarily choosing one class to be the base class, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{k}'(\mathbf{x}) = \delta_{k}(\mathbf{x}) - \delta_{K}(\mathbf{x})\,,&lt;/script&gt;

&lt;p&gt;$k = 1, \dots, K-1$. Hence, the total number of estimated parameters for LDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)(p+1)&lt;/script&gt;. On the other hand, for each QDA discriminant function (\ref{eqn_qda}), mean vector, covariance matrix, and class prior need to be estimated:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mean: $p$&lt;/li&gt;
  &lt;li&gt;Covariance: $p(p+1)/2$&lt;/li&gt;
  &lt;li&gt;Class prior: 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The total number of estimated parameters for QDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)\{p(p+3)/2+1\}&lt;/script&gt;. &lt;em&gt;Therefore, the number of parameters estimated in LDA increases linearly with $p$ while that of QDA increases quadratically with $p$.&lt;/em&gt; We would expect QDA to have worse performance than LDA when the dimension $p$ is large.&lt;/p&gt;

&lt;h4 id=&quot;compromise-between-lda--qda&quot;&gt;Compromise between LDA &amp;amp; QDA&lt;/h4&gt;
&lt;p&gt;We can find a compromise between LDA and QDA by regularizing the individual class covariance matrices. That is, individual covariance matrix shrinks toward a common pooled covariance matrix through a penalty parameter $\alpha$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.&lt;/script&gt;

&lt;p&gt;The pooled covariance matrix can also be regularized toward an identity matrix through a penalty parameter $\beta$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}} (\beta) = \beta \hat{\mathbf{\Sigma}} + (1-\beta) \mathbf{I} \,.&lt;/script&gt;

&lt;p&gt;In situations where the number of input variables greatly exceed the number of samples, covariance matrix can be poorly estimated. Shrinkage can hopefully improve the estimation and classification accuracy.&lt;br /&gt;
&lt;img src=&quot;/assets/2019-10-02/lda_shrinkage.png&quot; alt=&quot;lda_shrinkage&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot, credit to &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for testing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# how often to repeat classification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# maximum number of features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# step size for the calculation
&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Generate random blob-ish data with noisy features.

    This returns an array of input data with shape `(n_samples, n_features)`
    and an array of `n_samples` target labels.

    Only one feature contains discriminative information, the other features
    contain only noise.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add non-discriminative features
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA with shrinkage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_features / n_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Classification accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;computation-for-lda&quot;&gt;Computation for LDA&lt;/h4&gt;
&lt;p&gt;We can see from (\ref{eqn_lda}) and (\ref{eqn_qda}) that computations of discriminant functions can be simplified if we diagonalize the covariance matrices first. That is, data are transformed to have an identity covariance matrices. In the case of LDA, here’s how we proceed with the computation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Perform eigen-decompostion on the pooled covariance matrix: 
&lt;script type=&quot;math/tex&quot;&gt;\hat{\mathbf{\Sigma}} = \mathbf{U}\mathbf{D}\mathbf{U}^{T} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Sphere the data:
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}^{*} \leftarrow \mathbf{D}^{-\frac{1}{2}} \mathbf{U}^{T} \mathbf{X} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Obtain class centroids in the transformed space: &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_1, \dots, \hat{\mu}_{K}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Classify $\mathbf{x}$ according to $\delta_{k}(\mathbf{x}^{*})$:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\delta_{k}(\mathbf{x}^{*})=\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k} \,.
\label{eqn_lda_sphered}
\end{align}&lt;/script&gt;

&lt;p&gt;Step 2 spheres the data to produce an identity covariance matrix in the transformed space. Step 4 is obtained by following (\ref{eqn_lda}). Let’s take a two-class example to see what LDA is doing. Suppose there are two classes, $k$ and $\ell$. We classify $\mathbf{x}$ to class $k$ if &lt;script type=&quot;math/tex&quot;&gt;\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &gt; 0&lt;/script&gt;. Following the four steps outlined above, we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &amp;= 
\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k}
- \mathbf{x^{*}}^{T} \hat{\mu}_{\ell} + \frac{1}{2} \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell} - \log \hat{\pi}_{k} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k}^{T}\hat{\mu}_{k} - \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;&gt; 0 \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That is, we classify $\mathbf{x}$ to class $k$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) &gt; \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \,.&lt;/script&gt;

&lt;p&gt;The derived allocation rule reveals the working of LDA. The left-hand side of the equation is the length of the orthogonal projection of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x^{*}}&lt;/script&gt; onto the line segment joining the two class centroids. The right-hand side is the location of the center of the segment corrected by class prior probabilities. &lt;em&gt;Essentially, LDA classifies the data to the closest class centroid.&lt;/em&gt; We make two observations here.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The decision point deviates from the middle point when the class prior probabilities are not the same, i.e., the boundary is pushed toward the class with a smaller prior probability.&lt;/li&gt;
  &lt;li&gt;Data are projected onto the space spanned by class centroids, e.g. &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_{k} - \hat{\mu}_{\ell}&lt;/script&gt;. Distance comparisons are then done in that space.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reduced-rank-lda&quot;&gt;Reduced-rank LDA&lt;/h3&gt;
&lt;p&gt;What I’ve just described is the classification by LDA. LDA is also famous for its ability to find a small number of meaningful dimensions, allowing us to visualize high-dimensional problems. What do we mean by meaningful, and how does LDA find these dimensions? We will answer these questions shortly. First, take a look at the below plot. For a &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine&quot;&gt;wine classification&lt;/a&gt; problem with three different types of wines and 13 input variables, the plot visualizes the data in two discriminant coordinates found by LDA. In this two-dimensional space, the classes can be well-separated. In comparison, the classes are not as clearly separated using the first two principal components found by PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-02/lda_vs_pca.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'turquoise'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'darkorange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LDA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PCA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;inherent-dimension-reduction&quot;&gt;Inherent dimension reduction&lt;/h4&gt;
&lt;p&gt;In the above wine example, a 13-dimensional problem is visualized in a 2d space. Why is this possible? This is possible because there’s an inherent dimension reduction in LDA. We have observed from the previous section that LDA makes distance comparison in the space spanned by different class centroids. Two distinct points lie on a 1d line; three distinct points lie on a 2d plane. Similarly, $K$ class centroids lie on a hyperplane with dimension at most $(K-1)$. In particular, the subspace spanned by the centroids is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{K-1}=\mu_{1} \oplus \operatorname{span}\left\{\mu_{i}-\mu_{1}, 2 \leq i \leq K\right\} \,.&lt;/script&gt;

&lt;p&gt;When making distance comparisons, distances orthogonal to this subspace would add no information since they contribute equally for each class. Hence, by restricting distance comparisons to this subspace only would not lose any information useful for LDA classification. That means, we can safely transform our task from a $p$-dimensional problem to a $(K-1)$-dimensional problem by an orthogonal projection of the data onto this subspace. When $p \gg K$, this is a considerable drop in the number of dimensions. What if we want to reduce the dimension further from $p$ to $L$ where $K \gg L$? We can construct an $L$-dimensional subspace, $H_L$, from $H_{K-1}$, and this subspace is optimal, in some sense, to LDA classification.&lt;/p&gt;

&lt;h4 id=&quot;optimal-subspace-and-computation&quot;&gt;Optimal subspace and computation&lt;/h4&gt;
&lt;p&gt;Fisher proposes that the subspace $H_L$ is optimal when the class centroids of sphered data have maximum separation in this subspace in terms of variance. Following this definition, optimal subspace coordinates are simply found by doing PCA on sphered class centroids. The computation steps are summarized below:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Find class centroid matrix, $\mathbf{M}_{(K\times p)}$, and pooled var-cov, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_{(p\times p)}&lt;/script&gt;, where&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
 \mathbf{W} = \sum_{k=1}^{K} \sum_{g_i = k} (\mathbf{x}_i - \hat{\mu}_k)(\mathbf{x}_i - \hat{\mu}_k)^T \,.
 \label{within_w}
 \end{align}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Sphere the centroids: $\mathbf{M}^* = \mathbf{M} \mathbf{W}^{-\frac{1}{2}}$, using eigen-decomposition of $\mathbf{W}$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \operatorname{cov}(\mathbf{M}^*)&lt;/script&gt;, the between-class covariance of sphered class centroids by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{B}^* = \sum_{k=1}^{K} (\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)(\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)^T \,.&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Obtain $L$ eigenvectors &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{v}^*_\ell)&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\mathbf{V}^*&lt;/script&gt; of 
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \mathbf{V}^* \mathbf{D_B} \mathbf{V^*}^T&lt;/script&gt; cooresponding to the $L$ largest eigenvalues. These define the coordinates of the optimal subspace.&lt;/li&gt;
  &lt;li&gt;Obtain $L$ new (discriminant) variables $Z_\ell = (\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell)^T X$, for $\ell = 1, \dots, L$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through this procedure, we reduce our data dimension from &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_{(N \times p)}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Z}_{(N \times L)}&lt;/script&gt;. Discriminant coordinate 1 and 2 in the previous wine plot are found by setting $L = 2$. Repeating LDA procedures for classification using the new data, $\mathbf{Z}$, is called the reduced-rank LDA.&lt;/p&gt;

&lt;h3 id=&quot;fishers-lda&quot;&gt;Fisher’s LDA&lt;/h3&gt;
&lt;p&gt;Fisher derived the computation steps according to his optimality definition in a different way&lt;sup id=&quot;fnref:Fisher&quot;&gt;&lt;a href=&quot;#fn:Fisher&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. His steps of performing the reduced-rank LDA would later be known as the Fisher’s LDA. Fisher does not make any assumption about the distribution of the populations, $\Pi_1, \dots, \Pi_K$. Instead, he tries to find a “sensible” rule so that the classification task becomes easier. In particular, Fisher finds a linear combination &lt;script type=&quot;math/tex&quot;&gt;Z = \mathbf{a}^T X&lt;/script&gt; where the between-class variance, $\mathbf{B} = \operatorname{cov}(\mathbf{M})$, is maximized relative to the within-class variance, $\mathbf{W}$, as defined in (\ref{within_w}).&lt;/p&gt;

&lt;p&gt;The below plot, taken from ESL&lt;sup id=&quot;fnref:ESL&quot;&gt;&lt;a href=&quot;#fn:ESL&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, shows why this rule makes intuitive sense. The rule sets out to find a direction, $\mathbf{a}$, where, after projecting the data onto that direction, class centroids have maximum separation between them, and each class has minimum variance within them. The projection direction found under this rule, shown in the plot on the right, is much better.
&lt;img src=&quot;/assets/2019-10-02/sensible_rule.png&quot; alt=&quot;sensible_rule&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;generalized-eigenvalue-problem&quot;&gt;Generalized eigenvalue problem&lt;/h4&gt;
&lt;p&gt;Finding the optimal direction(s) above amounts to solving an optimization problem:
&lt;script type=&quot;math/tex&quot;&gt;\max_{\mathbf{a}} (\mathbf{a}^{T} \mathbf{B} \mathbf{a})/(\mathbf{a}^{T} \mathbf{W} \mathbf{a}) \,,&lt;/script&gt;
which is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen}
\max_{\mathbf{a}} {}&amp;{} \mathbf{a}^{T} \mathbf{B} \mathbf{a} \,,\\ 
\text{s.t. } &amp;{} \mathbf{a}^{T} \mathbf{W} \mathbf{a} = 1 \,, \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;since the scaling of $\mathbf{a}$ does not affect the solution. Let $\mathbf{W}^{\frac12}$ be the symmetric square root of $\mathbf{W}$, and $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$. We can rewrite the problem (\ref{eqn_g_eigen}) as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_1}
\max_{\mathbf{y}} {}&amp;{} \mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} \,,\\ 
\text{s.t } &amp;{} \mathbf{y}^{T} \mathbf{y} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$ is symmetric, we can find the spectral decomposition of it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} = \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \,.
\label{eqn_fisher_eigen}
\end{align}&lt;/script&gt;

&lt;p&gt;Let $\mathbf{z} = \mathbf{\Gamma}^T \mathbf{y}$. So $\mathbf{z}^T \mathbf{z} = \mathbf{y}^T \mathbf{\Gamma} \mathbf{\Gamma}^T \mathbf{y} = \mathbf{y}^T \mathbf{y}$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} &amp;= \mathbf{y}^{T} \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \mathbf{y} \\
&amp;= \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Problem (\ref{eqn_g_eigen_1}) can then be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_2}
\max_{\mathbf{z}} {}&amp;{} \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} = \sum_i \lambda_i z_i^2 \,,\\ 
\text{s.t } &amp;{} \mathbf{z}^{T} \mathbf{z} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the eigenvalues are written in descending order, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\mathbf{z}} \sum_i \lambda_i z_i^2 &amp;\le \lambda_1 \sum_i z_i^2 \,,\\
&amp;= \lambda_1 \,,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the upper bound is attained at $\mathbf{z} = (1,0,0,\dots,0)^T$. Since $\mathbf{y} = \mathbf{\Gamma} \mathbf{z}$, the solution is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \pmb \gamma_{(1)}&lt;/script&gt;, the eigenvector corresponding to the largest eigenvalue in (\ref{eqn_fisher_eigen}). Since $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$, the optimal projection direction is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a} = \mathbf{W}^{-\frac12} \pmb \gamma_{(1)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem A.6.2&lt;/strong&gt; from MA&lt;sup id=&quot;fnref:MA&quot;&gt;&lt;a href=&quot;#fn:MA&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;: For &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}_(n \times p)&lt;/script&gt; and $\mathbf{B}_(p \times n)$, the non-zero eigenvalues of
$\mathbf{AB}$ and $\mathbf{BA}$ are the same and have the same multiplicity. If $\mathbf{x}$ is a non-trivial eigenvector of $\mathbf{AB}$ for an eigenvalue $\lambda \neq 0$, then $\mathbf{y}=\mathbf{Bx}$ is a non-trivial eigenvector of $\mathbf{BA}$.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\pmb \gamma_{(1)}&lt;/script&gt; is an eigenvector of $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$, then, $\mathbf{W}^{-\frac12} \pmb \gamma_{(1)}$ is also the eigenvector of $\mathbf{W}^{-\frac12} \mathbf{W}^{-\frac12} \mathbf{B} = \mathbf{W}^{-1} \mathbf{B}$, using &lt;strong&gt;Theorem A.6.2&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In summary, optimal subspace coordinates, also known as discriminant coordinates, are obtained from eigenvectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-1}\mathbf{B}&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;\ell = 1, ... , \min\{p,K-1\}&lt;/script&gt;.&lt;/em&gt; It can be shown that the &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt;s obtained are the same as &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell&lt;/script&gt;s obtained in the reduced-rank LDA formulation. Surprisingly, Fisher arrives at this formulation without any Gaussian assumption on the population, unlike the reduced-rank LDA formulation. The hope is that, with this sensible rule, LDA would perform well even when the data do not follow exactly the Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;handwritten-digits-problem&quot;&gt;Handwritten digits problem&lt;/h2&gt;
&lt;p&gt;Here’s an example to show the visualization and classification ability of Fisher’s LDA, or simply LDA. We need to recognize ten different digits, i.e., 0 to 9, using 64 variables (pixel values from images). The dataset is taken from &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, we can visualze the training images and they look like these: 
&lt;img src=&quot;/assets/2019-10-02/digits.png&quot; alt=&quot;digits&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_digits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;i'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Next, we train an LDA classifier on the first half of the data. Solving the generalized eigenvalue problem mentioned previously gives us a list of optimal projection directions. In this problem, we keep the top 4 coordinates, and the transformed data are shown below. 
&lt;img src=&quot;/assets/2019-10-02/reduced_lda_digits.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a classifier: a Fisher's LDA classifier
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'eigen'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train lda on the first half of the digits
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Visualize transformed data on learnt discriminant coordinates
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The above plot allows us to interpret the trained LDA classifier. For example, coordinate 1 helps to contrast 4’s and 2/3’s while coordinate 2 contrasts 0’s and 1’s. Subsequently, coordinate 3 and 4 help to discriminate digits not well-separated in coordinate 1 and 2. We test the trained classifier using the other half of the dataset. The report below summarizes the result.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;precision&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;recall&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;f1-score&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.90&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.77&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;avg / total&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Predict the value of the digit on the second half:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Classification report:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The highest precision is 99%, and the lowest is 77%, a decent result knowing that the method was proposed some 70 years ago. Besides, we have not done anything to make the procedure better for this specific problem. For example, there is collinearity in the input variables, and the shrinkage parameter might not be optimal.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-lda&quot;&gt;Summary of LDA&lt;/h2&gt;
&lt;p&gt;Here I summarize the virtues and shortcomings of LDA.&lt;/p&gt;

&lt;p&gt;Virtues of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Simple prototype classifier: simple to interpret.&lt;/li&gt;
  &lt;li&gt;Decision boundary is linear: simple to implement and robust.&lt;/li&gt;
  &lt;li&gt;Dimension reduction: provides informative low-dimensional view on
data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Shortcomings of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired.&lt;/li&gt;
  &lt;li&gt;In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired.&lt;/li&gt;
  &lt;li&gt;Support for more complex prototype classification is desired.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the next article, flexible, penalized, and mixture discriminant analysis will be introduced to address each of the three shortcomings of LDA. With these generalizations, LDA can take on much more difficult and complex problems.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Fisher&quot;&gt;
      &lt;p&gt;Fisher, R. A. (1936). &lt;em&gt;The use of multiple measurements in taxonomic problems. Annals of eugenics&lt;/em&gt;, 7(2), 179-188. &lt;a href=&quot;#fnref:Fisher&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ESL&quot;&gt;
      &lt;p&gt;Friedman, J., Hastie, T., &amp;amp; Tibshirani, R. (2001). &lt;em&gt;The elements of statistical learning&lt;/em&gt; (Vol. 1, No. 10). New York: Springer series in statistics. &lt;a href=&quot;#fnref:ESL&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:MA&quot;&gt;
      &lt;p&gt;Mardia, K. V., Kent, J. T., &amp;amp; Bibby, J. M. &lt;em&gt;Multivariate analysis&lt;/em&gt;. 1979. Probability and mathematical statistics. Academic Press Inc. &lt;a href=&quot;#fnref:MA&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>How did I set up my blog using Jekyll, Hyde and GitHub</title>
   <link href="http://localhost:4000/code/2019/09/25/set-up-blog.html"/>
   <updated>2019-09-25T08:00:00+08:00</updated>
   <id>http://localhost:4000/code/2019/09/25/set-up-blog</id>
   <content type="html">&lt;p&gt;What would be a better way to start this blog than writing a post about how I set it up? Because after three days of sifting through all the documents, blogs, StackOverflow answers and GitHub issues, I finally realized that the process is not as straightforward as I thought it would be. Anyway, I got it to work (for now).&lt;/p&gt;

&lt;p&gt;My aim is simple, to set up a blog for myself where I can post stuff about my life. The blog needs to be free, elegant, intuitive and support math. My current set up, Jekyll + Hyde + Github + MathJax, matches with that. Since there are many resources online about setting up a blog using Jekyll and serve it with GitHub, I am going to skip all the standard procedures by referring to the official documents. Instead, this post specifically documents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the sequence of setting up different parts,&lt;/li&gt;
  &lt;li&gt;adding support for Tags, Categories and their corresponding pages,&lt;/li&gt;
  &lt;li&gt;adding MathJax to support $\LaTeX$-like math.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am using a macbook, so the steps will be described assuming the system is macOS. When in doubt, just google the relevant steps for other OSs.&lt;/p&gt;

&lt;h2 id=&quot;1-set-up-jekyll&quot;&gt;1. Set up Jekyll&lt;/h2&gt;
&lt;p&gt;Jekyll is the package that is generating all your website pages. First thing you want to do is to make sure that &lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is installed and ready to run.&lt;/p&gt;

&lt;p&gt;Follow the official &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;instructions&lt;/a&gt;. If you successfully made a new site, good!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;But if you ran into a &lt;strong&gt;failed to build native extension error&lt;/strong&gt;, install macOS SDK headers with the following line.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;If you ran into a &lt;strong&gt;file permission error&lt;/strong&gt;, run the following lines to set GEM_HOME to your user directory.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'# Install Ruby Gems to ~/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export GEM_HOME=$HOME/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export PATH=$HOME/gems/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now you can proceed with the following line and the rest of the steps.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gem &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;bundler jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is caused by the macOS Mojave update. The above solution is provided by &lt;a href=&quot;https://talk.jekyllrb.com/t/issues-installing-jekyll-on-macos-mojave/2400/3&quot;&gt;desiredpersona and Frank&lt;/a&gt;. Make sure Jekyll can run normally before proceeding to step 2.&lt;/p&gt;

&lt;h2 id=&quot;2-set-up-github-repo&quot;&gt;2. Set up GitHub repo&lt;/h2&gt;
&lt;p&gt;Jekyll generates web pages locally; we need a GitHub repository to host our pages so that they can be accessed on the internet. For this part, setup can be done by following GitHub’s official &lt;a href=&quot;https://pages.github.com&quot;&gt;instructions&lt;/a&gt;. In the end, you should have a repo on GitHub called &lt;em&gt;username&lt;/em&gt;.github.io, and the corresponding local folder on your computer. In my case, the name of my repo is yangxiaozhou.github.io.&lt;/p&gt;

&lt;p&gt;By the end of Step 1 and 2, we have set up the local engine for generating web pages and the GitHub repo for hosting and publishing your pages. Now we proceed to the actual website construction.&lt;/p&gt;

&lt;h2 id=&quot;3-use-a-website-template&quot;&gt;3. Use a website template&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://hyde.getpoole.com&quot;&gt;Hyde&lt;/a&gt; is a Jekyll website theme built on &lt;a href=&quot;https://github.com/poole/poole&quot;&gt;Poole&lt;/a&gt;. They provide the template and the theme for the website. There are many themes for Jekyll, but I decided to use Hyde because I like the elegant design and it’s easy to customize.&lt;/p&gt;

&lt;p&gt;To get Hyde, just download &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;the repo&lt;/a&gt; and move all the files into the local folder that you have just created in Step 2. Remember to clear any existing file in that folder before moving in Hyde files. From here, you just have to edit parts of those files to make the website yours (or use it as it is). I changed the following two lines in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; since redcarpet and pygments are not supported anymore. Other variables can also be changed such as name, GitHub account, etc.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kramdown&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;highlighter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rouge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At this point, it would be a good idea to learn some basics of &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;Jekyll&lt;/a&gt;, e.g. what is a front matter, what is a page, how to create a layout, etc. After learning these, you can go ahead and customize the website as you’d like.&lt;/p&gt;

&lt;p&gt;One problem that I ran into is that pages look fine in local serve, but when I publish them to the web, all pages other than the home page have suddenly lost all their style elements. After searching through the internet, I realize that this has to do with the &lt;code class=&quot;highlighter-rouge&quot;&gt;url&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;baseurl&lt;/code&gt; usage. If you also have this problem, consider doing the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;change all the &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ site.baseurl }}&lt;/code&gt;
instances in &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sidebar.html&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ '/' | relative_url }}&lt;/code&gt; so that the correct files can be located.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-add-tags--categories&quot;&gt;4. Add tags &amp;amp; categories&lt;/h2&gt;
&lt;p&gt;I want to add tags and categories to my posts and create a dedicated page where posts can be arranged according to &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;tags&lt;/a&gt;/&lt;a href=&quot;https://yangxiaozhou.github.io/categories/&quot;&gt;categories&lt;/a&gt;. This should be easy since tags and categories are default front matter variables that you can define in Jekyll. For example, tags and categories of my LDA post are defined like this:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;post&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;mathjax&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;discriminant&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;analysis&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;(LDA)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;beyond&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;s&quot;&gt;2019-09-26 08:00:00 +0800&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;statistics&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;supervised-learning classification&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;categories&lt;/strong&gt;, I created one page where posts of different categories are collected and the page is accessible through the sidebar link. To do this, just create a &lt;code class=&quot;highlighter-rouge&quot;&gt;category.html&lt;/code&gt; in the root folder:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: page
permalink: /categories/
title: Categories
---

&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archives&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
{% for category in site.categories %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-group&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% capture category_name %}{{ category | first }}{% endcapture %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h3&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;category-head&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ category_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h3&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    {% for post in site.categories[category_name] %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;article&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-item&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;h4&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.baseurl }}{{ post.url }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{post.title}}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/article&amp;gt;&lt;/span&gt;
    {% endfor %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;tags&lt;/strong&gt;, I did two things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Show the tags of a post at the end of the content.&lt;/li&gt;
  &lt;li&gt;For every tag, create a page where posts are collected, i.e. &lt;a href=&quot;https://yangxiaozhou.github.io/tag/classification&quot;&gt;classification&lt;/a&gt;, &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;supervised-learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To do 1, include the following lines after the &lt;code class=&quot;highlighter-rouge&quot;&gt;content&lt;/code&gt; section in your &lt;code class=&quot;highlighter-rouge&quot;&gt;post.html&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-tags&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% for tag in page.tags %}
      {% capture tag_name %}{{ tag }}{% endcapture %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;no-underline&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tag/{{ tag_name }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;code&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;highligher-rouge&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;nobr&amp;gt;&lt;/span&gt;{{ tag_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/nobr&amp;gt;&amp;lt;/code&amp;gt;&lt;/span&gt;&lt;span class=&quot;ni&quot;&gt;&amp;amp;nbsp;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;    
    {% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To do 2, Long Qian has written a very clear &lt;a href=&quot;https://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;post&lt;/a&gt; about it.&lt;/p&gt;

&lt;h2 id=&quot;5-add-mathjax&quot;&gt;5. Add MathJax&lt;/h2&gt;
&lt;p&gt;The last piece to my website is to add the support of $\LaTeX$-like math. This is done through MathJax. There are two steps to achieve it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a &lt;code class=&quot;highlighter-rouge&quot;&gt;mathjax.html&lt;/code&gt; file and put it in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_includes&lt;/code&gt; folder. Download the file &lt;a href=&quot;https://github.com/YangXiaozhou/yangxiaozhou.github.io/blob/master/_includes/mathjax.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Put the following line before &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/head&amp;gt;&lt;/code&gt; in your &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt;:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; {% include mathjax.html %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;to enbale MathJax on the page.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tips&quot;&gt;Tips&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;To use the normal dollar sign instead of the MathJax command (escape), put &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;span class=&quot;tex2jax_ignore&quot;&amp;gt;...&amp;lt;/span&amp;gt;&lt;/code&gt; around the text you don’t want MathJax to process.&lt;/li&gt;
  &lt;li&gt;Check out currently supported Tex/LaTeX commands by MathJax &lt;a href=&quot;https://docs.mathjax.org/en/latest/input/tex/macros/index.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;math-rendering-showcase&quot;&gt;Math Rendering Showcase&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Inline math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$...\$&lt;/code&gt;: $\mathbf{x}+\mathbf{y}$.&lt;/li&gt;
  &lt;li&gt;Displayed math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$\$...\$\$&lt;/code&gt; on a new paragraph:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}+\mathbf{y} \,.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Automatic numbering and referencing using &lt;span class=&quot;tex2jax_ignore&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\ref{label}&lt;/code&gt;&lt;/span&gt;:
In (\ref{eq:sample}), we find the value of an interesting integral:
\begin{align}
\int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15} \, .
\label{eq:sample}
\end{align}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiline equations using &lt;code class=&quot;highlighter-rouge&quot;&gt;\begin{align*}&lt;/code&gt;:
\begin{align*}
\nabla \times \vec{\mathbf{B}} -\, \frac1c\, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp;amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \,,\newline
\nabla \cdot \vec{\mathbf{E}} &amp;amp; = 4 \pi \rho \,.
\end{align*}&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s it for now. Happy blogging.&lt;/p&gt;

&lt;p&gt;Additional resources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set up &lt;a href=&quot;https://blog.webjeda.com/jekyll-categories/&quot;&gt;categories &amp;amp; tags&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Set up &lt;a href=&quot;http://joshualande.com/jekyll-github-pages-poole&quot;&gt;Disqus comments &amp;amp; Google Analytics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Add in social media &lt;a href=&quot;https://jreel.github.io/social-media-icons-on-jekyll/&quot;&gt;icons&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;kramdown &lt;a href=&quot;https://kramdown.gettalong.org/quickref.html&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MathJax &lt;a href=&quot;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>菜谱</title>
   <link href="http://localhost:4000/learning/2019/01/01/recipe.html"/>
   <updated>2019-01-01T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2019/01/01/recipe</id>
   <content type="html">&lt;p&gt;做过的好吃的东西，为了不忘记怎么做，把菜谱都收集起来放在这儿。只要没有单独说的，一律按适量原则。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#宫保鸡丁&quot;&gt;宫保鸡丁&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#油泼猪手&quot;&gt;油泼猪手&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#腊肉糯米圆子&quot;&gt;腊肉糯米圆子&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;宫保鸡丁&quot;&gt;宫保鸡丁&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/kong_pao_chicken.jpg&quot; alt=&quot;宫保鸡丁&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;准备材料：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;肉：鸡腿肉去骨，切丁。&lt;/li&gt;
  &lt;li&gt;上浆：上一次葱姜水，抓匀，上蛋清，抓匀，再上一次姜葱水，抓匀。放入生抽、老抽、花雕酒，抓匀，再放入胡椒粉、盐、淀粉，抓匀。最后用&lt;strong&gt;少量油封住&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;小料：大葱花，蒜片，姜片。&lt;/li&gt;
  &lt;li&gt;料汁儿：少量盐，多点糖，胡椒粉，料酒，酱油，老抽，适量水淀粉，最后加上点&lt;strong&gt;葱姜蒜片尝味道&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;花生：热水泡了去皮。&lt;/li&gt;
  &lt;li&gt;干辣椒：切段。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;开始做：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;小火炸花生，慢慢炸到香脆。&lt;/li&gt;
  &lt;li&gt;另起锅，热锅冷油，小火滑鸡丁，滑好倒出。&lt;/li&gt;
  &lt;li&gt;小火煸炒花椒，捞出花椒，煸干辣椒。&lt;/li&gt;
  &lt;li&gt;煸好的辣椒花椒油中加入鸡丁，开大火，翻炒几下下葱姜蒜，炒出香味，下料汁儿，看颜色适量放老抽上色。&lt;/li&gt;
  &lt;li&gt;出锅前倒入少量花椒油和花生。&lt;/li&gt;
  &lt;li&gt;上菜！&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;上浆一定要耐心，每放一次调料就要抓匀。&lt;/li&gt;
  &lt;li&gt;调好的料汁儿需要尝一尝，根据味道再做调整，缺啥补啥。&lt;/li&gt;
  &lt;li&gt;花生需要小火慢炸，炸到香脆，微黄。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;油泼猪手&quot;&gt;油泼猪手&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/splash_oil_pig_trotters.jpg&quot; alt=&quot;油泼猪手&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;准备材料：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;洗净小猪手。&lt;/li&gt;
  &lt;li&gt;小葱切段，姜切片，蒜切片。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;开始做：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;焯水：小猪手冷水下锅焯水，&lt;strong&gt;水中放醋&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;备汤：高压锅内放水，盐，花雕酒，葱叶，姜片。&lt;/li&gt;
  &lt;li&gt;炖肉：焯好水的猪脚拿出用热水清洗干净，放入高压锅，炖四十分钟左右。&lt;/li&gt;
  &lt;li&gt;小火炸花生，把花生炸至香脆。&lt;/li&gt;
  &lt;li&gt;猪手炖好拿出来泡在冰水里降温，用手尽量掰成小块儿状，方便入味。&lt;/li&gt;
  &lt;li&gt;沥干水分的猪手中放入花生米，小葱，蒜片，香油，盐，醋，搅拌均匀。&lt;/li&gt;
  &lt;li&gt;另起锅，放入油和花椒，&lt;strong&gt;小火煸出花椒油&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;拌好的猪手中放入辣椒面（量依照个人口味），泼上热的花椒油（油泼猪手）。&lt;/li&gt;
  &lt;li&gt;上菜！&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;高压锅炖久一点能使猪手更软糯，根据喜好的口感可以自己选择时长。&lt;/li&gt;
  &lt;li&gt;提前备好足够的冰块，猪手出锅后让它尽量冷却，不然手掰起来非常烫。&lt;/li&gt;
  &lt;li&gt;辣椒面根据自己喜好放入，不能吃辣的注意别放太多。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;腊肉糯米圆子&quot;&gt;腊肉糯米圆子&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/sticky_rice_ball.jpg&quot; alt=&quot;腊肉糯米圆子&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>日本語の練習</title>
   <link href="http://localhost:4000/learning/2018/11/21/japanese-essay.html"/>
   <updated>2018-11-21T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2018/11/21/japanese-essay</id>
   <content type="html">&lt;p&gt;在学习日语过程中零零散散写过一些小短文，想着不如全部放到这儿来，当做自己的一个记录，也希望收到同在日语学习路上的朋友们的反馈。
A collection of Japanese practice writings. All comments and corrections are welcomed.&lt;/p&gt;

&lt;h3 id=&quot;2019年4月5日--私の夢&quot;&gt;2019年4月5日 ｜ 私の夢&lt;/h3&gt;

&lt;p&gt;　私の夢は好きな仕事を自由にしながら、社会の人たちに何か役に立つことができるようになることです。自由は一番大切なことだと若いから決めました。やっている仕事は自分が好きなのなら、いくら難しければ、続けられますから。それに、卒業の時、父に「好きなことをやろう」と言われて、安心しました。また、人たちの生活が良くなるために、大学で勉強しています。大学院の生活はいつも忙しいですが、日本語が習えて、好きな仕事ができて毎日とても嬉しいです。夢を持っている人生は幸せだと思っています。&lt;/p&gt;

&lt;h3 id=&quot;2019年4月3日&quot;&gt;2019年4月3日&lt;/h3&gt;
&lt;p&gt;｜ 祖母について&lt;/p&gt;

&lt;p&gt;　若い時から、祖母は娘の息子の私にいつも親切にしてくれています。家で宿題をする時、よく中国語の文法や数え方などの問題があったら、一生懸命に教えてくれていました。「自分で間違いを直せるよいに、頑張って。」と言われた、今でも、覚えて仕事をしています。旅行する時、色々な所のお土産を探して持って家に帰って、お祝いを祖母にあげれば、祖母は嬉しくなるので、私も嬉しいです。&lt;/p&gt;

&lt;p&gt;｜ 新聞の読み方について&lt;/p&gt;

&lt;p&gt;　先ず、見出しは何か読んでみます。これは表であるページです。このページを読めば、世界中で一番大きいニュースがわかります。よく政治や事故なニュースがこのページでありますが、時々社会と文化のニュースもあります。見出しのニュースは私の生活から遠いですが、知るのは必要だと思います。次に、スポーツが好きで、スポーツニュースへ行きます。全部読まなくて、バスケットボールの試合が気に入っています。両親は町の新聞を一番に読んでいます。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月28日特別な思い出が作るのはどうすればいいですか&quot;&gt;2019年3月28日　｜　特別な思い出が作るのは、どうすればいいですか。&lt;/h3&gt;

&lt;p&gt;AさんとBさんはハジレーンへ行ったことがないから、初めてハジレーンをゆっくり体験できるように、ツアーを作りました。ツアーの日は天気が良くなくて、シンガポールの普通の蒸し暑い天気でした。それに、私たちの四人ガイドは時々複雑な考えが話せなかったので、英語と日本語も上手なBさんに翻訳してもらいました。でも、ツアーの終わりにAさんとBさんは「今日は、ハジチャレンジに参加してよかったね。」と思ったと思います。&lt;/p&gt;

&lt;p&gt;いい写真が撮れるだけじゃなくて、何が特別な思い出が作れるように、ツアーの形を考えました。ハジーレンにはたくさん綺麗な落書きがあって、あの落書きを詳しく見れば、町のことがよく分かるし、特別な思い出もできると思いました。それで、ハジチャレンジが生まれました。でも、このツアーが成功した一番大切な理由はニコルさんが自分で作ったパンフレットでした。AさんとBさんはその美しいパンフレットを準備してもらって、びっくりしました。二人達は探している落書きが見つかった時、きっとパンフレットにはんこを押されるのは嬉しかったでしょう。手で作ったパンフレットを使うことや、紙で書いたものに、はんこを押すのは、何が特別な感じがします。今、本やパンフレットよりもっと情報が多くて、速くて、便利な携帯電話は大変人気がありますが、地図やパンフレットなどをまだ使っている人もいます。情報は変わらないので、紙のパンフレットを使えば周りの物にもっと気がつくかもしれません。それで、その時、そこで、人と自然の、また、人と人の意味がある関係ができます。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月20日&quot;&gt;2019年3月20日&lt;/h3&gt;

&lt;p&gt;　クレメンティは私の大好きな町です。ここは人が大勢いて、いつもにぎやかなんですが、困りません。それは、色々な店が品物を売って、買い物の途中でお腹がへったら、狭い道で値段がそんなに高くない食べ物や飲み物が買えます。恥ずかしくなくて、飲み物を飲みながら、買い物を選びましょう。&lt;/p&gt;

&lt;h3 id=&quot;2019年3月4日&quot;&gt;2019年3月4日&lt;/h3&gt;

&lt;p&gt;　昔から、中国は世界中の色々な国の人を招待しています。他の国と仲良くすれば、商品を輸入できます。たとれば、よく石油と他の原料を輸入して、美しい池の絵とか、米とか、お茶を輸出しています。今も、たくさんの国に頼っています。&lt;/p&gt;

&lt;p&gt;　去年11月に行ったフィリピンにあるマラパスクア島はとても綺麗な島です。毎日海岸で散歩する前に卵入れて朝ごはんを食べました。この島は小さいですから、工場は全然建ててありません。村の中で、豚と鳥をたくさん育てています。しかし、飲まれる水はあまり足らなくて、人たちの生活は難しいです。&lt;/p&gt;

&lt;h3 id=&quot;2019年2月18日&quot;&gt;2019年2月18日&lt;/h3&gt;

&lt;p&gt;　シンガポール島はマレーシアの南向こうの島です。この島は小さいですが、木や花が多くて、そして、木の葉も道でよく見えます。マレーシアまで二つ橋を建てています。この島で、人たちの生活は昔から大変じゃなくて、早く結婚すれば、住む家が買えます。&lt;/p&gt;

&lt;p&gt;　今日は土曜日です。初めて船で乗って、海を渡って，ウビン島に行った。一時間ぐらい、海で泳いだ。島でたくさん野菜と魚を食べられた。味はシンガポールのと違っだ。&lt;/p&gt;

&lt;h3 id=&quot;2019年1月28日&quot;&gt;2019年1月28日&lt;/h3&gt;

&lt;p&gt;　私の家の近くに空港がありませんから、飛行機を見ていませんでした。でも、家の近くにたくさん綺麗な山や川があります。春になれば、山は緑の衣をつけています。私の子供の時、よく父は山登りに連れて行ってくれました。公園へ行くのより面白いと思っていました。それで、地質学者になろうと決めました。&lt;/p&gt;

&lt;p&gt;　自動車を運転している時、とても気を付けなければなりませんでした。特に、曲がる時は、よく交通事故があるから、本当に危険だと思います。曲がる前には、車の席から左と右を見たらいいと思います。&lt;/p&gt;

&lt;h3 id=&quot;2018年11月21日順天堂大学のみなさんとの交流から学んだこと&quot;&gt;2018年11月21日　｜　順天堂大学のみなさんとの交流から学んだこと&lt;/h3&gt;

&lt;p&gt;　年を取る人が多くて、若い人があまり多くないので、介護離職はたしかに日本にとって大きい社会的問題です。そして、この問題の答えは、早く考えなければなりません。日本では介護サービスにかかる費用がとても高いので、普通の家族は、あまり払う事ができないですから、主に家族が介護をしています。さらに、介護者の人数は今よりもっとたくさん必要になります。もしそうでなければ、コストは絶対より高くなると思います。順天堂大学の学生が発表した事について、第一と第二とあんの会社員が介護休暇や介護休業を取ったり、フレックスタイム制が有ったりする会社は大切だと言いました。それは私もそう思いますが、日本の会社は残業が多くて、競争がとても厳しいですから、難しいと思います。三つ目のあんは、いいと思います。外国人の労働者は、もうシンガポールや香港では長い間います。確かに、おじいさんやおばあさんは、外国人と一緒に暮らして、世話をしてもらう事が嫌いかもしれません。でも、私は半年ぐらい京都で住んでいましたから、外国人は日本人と一緒に暮らすことができると思います。外国人のヘルパーさんは日本語や日本の文化を勉強してから、働くことが解決策の一つだと思います。&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>东南亚背包：一路向北！新加坡-马来西亚-泰国-老挝-云南-成都</title>
   <link href="http://localhost:4000/travel/2014/07/02/backpacking-in-sea-2014.html"/>
   <updated>2014-07-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/travel/2014/07/02/backpacking-in-sea-2014</id>
   <content type="html">&lt;p&gt;先交代一下背景，同行一共四个男生，大学同学，在 新加坡 念书。此次的路线实际上很多人已经走过（不过好像国内的朋友从北往南走的比较多）。&lt;/p&gt;

&lt;p&gt;一张图顶千个字👇&lt;/p&gt;

&lt;p&gt;具体行程：
吉隆坡 （3天）—-&amp;gt; 槟城 （2天）—-&amp;gt; 涛岛 (Koh Tao)（6天）—-&amp;gt; 曼谷 （2天）
—-&amp;gt; 大城 (Ayuthaya)（1天）—-&amp;gt; 清迈 （4天）—-&amp;gt; 清莱 （2天）—-&amp;gt; 美斯乐(Mae Salong) （1天）
—-&amp;gt; 会晒 (Huay Xai)（1天）—-&amp;gt; 湄公河 （2天）—-&amp;gt; 琅勃拉邦 （3天）—-&amp;gt;乌多姆赛(Oudomxay)（1天）
—-&amp;gt; 景洪 —-&amp;gt; 昆明 （1天）—-&amp;gt; 成都 （5天）&lt;/p&gt;

&lt;p&gt;预算：一万RMB，实际没用完。&lt;/p&gt;

&lt;p&gt;吃喝住行（穷游）：
住宿基本不预定，一般是在到达下一站之前会做一些功课，去到那边客栈聚集的地方再一家一家地找。
吃的话是尽量去吃当地人吃的东西，但也会有大餐上桌的情况， 比如 涛岛 的海鲜烧烤， 清迈 的泰北美食。
交通方面也是尽量选择当地人会坐的交通，大巴，火车，渡船，山地自行车，步行。什么便宜什么就可以有， 比如 下面这货👇。&lt;/p&gt;

&lt;p&gt;哈哈，竹筏是开个玩笑。关于如何在吃喝住行方面省钱防止被宰，我会在后面写到。&lt;/p&gt;

&lt;p&gt;行前准备（两周）：&lt;/p&gt;

&lt;p&gt;签证：
马来西亚 ：驻 新加坡 办事处（High Commission of Malaysia in Singapore）指定的旅行社签发，四五十新币，一年之内多次入境。指定的旅行社名单和联系方式在 马来西亚 驻 新加坡 办事处的网站上找得到，或者也可以直接去珍珠大厦逛逛，那边很多被指定的旅行社。
链接: http://www.kln.gov.my/web/sgp_singapore/requirement_foreigner&lt;/p&gt;

&lt;p&gt;泰国 ：驻 新加坡 大使馆签发。这个签证办得不那么省心，官网和电话里都告诉我说除了正常的签证申请资料外，还要有进入和离开 泰国 的交通证明，机票，船票，车票都可以。入境的车票好定，拿到了电子发票，可是出 泰国 的交通在 新加坡 实在是没法定，要么是电话预定之后需要在当地的7-11付款，要么就是自己亲自到了 泰国 老挝 边境的时候再去买票。最后实在没办法，只能带了张入境的车票去大使馆，结果居然很顺利地过了，一天就能取，好像也是四五十新币，单次入境。&lt;/p&gt;

&lt;p&gt;老挝 ：入境关口签，后面会讲到。&lt;/p&gt;

&lt;p&gt;装备：
背包（65L）＋冲锋包，短裤短袖，书笔，电脑，相机（尼康D90），手机，手电，防蚊剂＊，拖鞋＊，等等。&lt;/p&gt;
</content>
 </entry>
 

</feed>
