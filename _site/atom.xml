<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xiaozhou's Notes</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-10-20T12:40:14+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Yang Xiaozhou</name>
   <email></email>
 </author>

 
 <entry>
   <title>Domain Expertise: What deep learning needs for better COVID-19 detection</title>
   <link href="http://localhost:4000/data/2020/09/27/detecting-covid19-using-cnn.html"/>
   <updated>2020-09-27T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/27/detecting-covid19-using-cnn</id>
   <content type="html">&lt;p&gt;By now, you‚Äôve probably seen a few, if not many, &lt;a href=&quot;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=detecting+covid-19+using+neural+network&amp;amp;btnG=&quot;&gt;articles&lt;/a&gt; on how deep learning could help detect COVID-19. In particular, convolutional neural networks (CNNs) have been studied as a faster and cheaper alternative to the gold-standard PCR test by just analyzing the patient‚Äôs computed tomography (CT) scan. It‚Äôs not surprising since CNN is excellent at image recognition; many places have CT scanners rather than COVID-19 testing kits (at least initially).&lt;/p&gt;

&lt;p&gt;Despite its success in image recognition tasks such as the ImageNet challenge, can CNN really help doctors detect COVID-19? If it can, how accurately can it do so? It‚Äôs well known that CT scans are sensitive but not &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7227176/&quot;&gt;specific&lt;/a&gt; to COVID-19. That is, COVID-19 almost always produces abnormal lung patterns visible from CT scans. However, other pneumonia can create the same abnormal patterns. Can the powerful and sometimes magical CNN tackle this ambiguity issue?&lt;/p&gt;

&lt;p&gt;We had a chance to answer these questions ourselves (with my colleague &lt;a href=&quot;https://www.linkedin.com/in/yuchen-shi-2830ba158/?originalSubdomain=sg&quot;&gt;Yuchen&lt;/a&gt; and advisor &lt;a href=&quot;https://www.eng.nus.edu.sg/isem/staff/chen-nan/&quot;&gt;A/P Chen&lt;/a&gt;). I‚Äôll walk you through a COVID-19 classifier that we‚Äôve built as an entry to 2020 INFORMS QSR Data &lt;a href=&quot;https://connect.informs.org/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=f404f7b8-fcd6-75d5-f7a7-d262eab132e7&quot;&gt;Challenge&lt;/a&gt;. If you are not familiar with CNN or would like a refresher on the key features of CNNs, I highly recommend reading &lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/09/24/intro-to-cnn.html&quot;&gt;Convolutional Neural Network: How is it different from the other networks?&lt;/a&gt; first. Also, if you‚Äôd like to get hands-on, you can get all the code and data from my Github &lt;a href=&quot;https://github.com/YangXiaozhou/CNN-COVID-19-classification-using-chest-CT-scan&quot;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Transfer learning using pre-trained CNN can achieve a really strong baseline performance on COVID-19 classification (85% accuracy).&lt;/li&gt;
  &lt;li&gt;However, domain expertise-informed feature engineering and adaptation are required to elevate the CNN (or other ML methods) to a medically convincing level.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;whats-the-challenge&quot;&gt;What‚Äôs the challenge?&lt;/h1&gt;

&lt;p&gt;COVID-19 pandemic has changed lives around the world. This is the current situation as of 2020/09/26, according to &lt;a href=&quot;https://covid19.who.int/&quot;&gt;WHO&lt;/a&gt;. &lt;img src=&quot;/assets/cnn-covid-19/covid-19-pandemic.png&quot; alt=&quot;current situation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CT scans have been used to screen and diagnose COVID-19, especially in areas where swab test resources are severely lacking. The goal of this data challenge is to diagnose COVID-19 using chest CT scans. Therefore, we need to build a &lt;strong&gt;classification model&lt;/strong&gt; that can classify patients to COVID or NonCOVID based on their chest CT scans, &lt;strong&gt;as accurately as possible&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-provided&quot;&gt;What‚Äôs provided?&lt;/h2&gt;
&lt;p&gt;Relatively even number of COVID and NonCOVID images are provided to train the model. While meta-information of these images is also provided, they will not be provided during testing. The competition also requires that the model‚Äôs training with provided data must take less than one hour.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training data set
    &lt;ul&gt;
      &lt;li&gt;251 COVID-19 CT images&lt;/li&gt;
      &lt;li&gt;292 non-COVID-19 CT images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Meta-information
    &lt;ul&gt;
      &lt;li&gt;e.g., patient information, severity, image caption&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All challenge data are taken from a public &lt;a href=&quot;https://github.com/UCSD-AI4H/COVID-CT&quot;&gt;data set&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;model-performance&quot;&gt;Model performance&lt;/h1&gt;
&lt;p&gt;Let‚Äôs first take a look at the result, shall we?&lt;/p&gt;

&lt;p&gt;The trained model is evaluated with an independent set of test data. Here you can see the confusion matrix. The overall accuracy is about 85% with slightly better sensitivity than specificity, i.e., true positive rate &amp;gt; true negative rate. 
&lt;img src=&quot;/assets/cnn-covid-19/confusion_matrix.png&quot; alt=&quot;confusion&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;implementation&quot;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Here are some of the provided NonCOVID and COVID CT scans. It‚Äôs important to note that the challenge is to distinguish between COVID and NonCOVID CT scans, rather than COVID and Normal scans. In fact, there may be some NonCOVID CT scans that belong to other pneumonia patients. 
&lt;img src=&quot;/assets/cnn-covid-19/first_look.png&quot; alt=&quot;first_look&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;train-validation-split&quot;&gt;Train-validation split&lt;/h2&gt;
&lt;p&gt;We reserve 20% of the data for validation. Since some consecutive images come from the same patient, they tend to be similar to each other.  That is, many of our data are &lt;strong&gt;not independent&lt;/strong&gt;. To prevent data leakage (information of training data spills over to validation data), we keep the original image sequence and hold out the last 20% as the validation set.&lt;/p&gt;

&lt;p&gt;After the splitting, we have two pairs of data:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;X_train&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;y_train&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;X_val&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;y_val&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;X is a list of CT scans, and y is a list of binary labels (0 for NonCOVID, 1 for COVID).&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h2&gt;
&lt;p&gt;Data augmentation is a common way to include more random variations in the training data. It helps to prevent overfitting. For image-related learning problems, augmentation typically means applying &lt;strong&gt;random&lt;/strong&gt; geometric (e.g., crop, flip, rotate, etc.) and appearance transformation (e.g., contrast, edge filter, Gaussian blur, etc.). Here we use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.Sequential&lt;/code&gt; to create a pipeline in which the input image is randomly transformed through the following operations:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Random horizontal and vertical flip&lt;/li&gt;
  &lt;li&gt;Rotation by a random degree in the range of $[-5\%, 5\%]*2\pi$&lt;/li&gt;
  &lt;li&gt;Random zoom in height by $5\%$&lt;/li&gt;
  &lt;li&gt;Random translation by $5\%$&lt;/li&gt;
  &lt;li&gt;Random contrast adjustment by $5\%$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is how they look after the augmentation. 
&lt;img src=&quot;/assets/cnn-covid-19/augmented_scans.png&quot; alt=&quot;augmented_scans&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-pre-trained-cnn-as-the-backbone&quot;&gt;Using pre-trained CNN as the backbone&lt;/h2&gt;
&lt;p&gt;We do not build a CNN from scratch. For an image-related problem with only a modest number of training images, it is recommended to use a pre-trained model as the backbone and do &lt;a href=&quot;https://cs231n.github.io/transfer-learning/&quot;&gt;transfer learning&lt;/a&gt; on that. The chosen model is &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0&quot;&gt;EfficientNetB0&lt;/a&gt;. It belongs to the family of models called &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNets&lt;/a&gt; proposed by researchers from Google. EfficientNets are among the current state-of-the-art CNNs for computer vision tasks. They&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;need a considerably lower number of parameters,&lt;/li&gt;
  &lt;li&gt;achieved very high accuracies on ImageNet,&lt;/li&gt;
  &lt;li&gt;transferred well to other image classification tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here‚Äôs a performance &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;comparison&lt;/a&gt; between EfficientNets and other well-known models:&lt;img src=&quot;/assets/cnn-covid-19/efficientnets.png&quot; alt=&quot;EfficientNet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EfficientNets, and other well-known pre-trained models, can be easily loaded from &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.applications&lt;/code&gt;. We first import the pre-trained EfficientNetB0 and use it as our model backbone. We remove the original output layer of EfficientNetB0 since it was trained for 1000-class classification. Also, we freeze the model‚Äôs weights so that they won‚Äôt be updated during the initial training.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Create a base model from the pre-trained EfficientNetB0
base_model = keras.applications.EfficientNetB0(input_shape=IMG_SHAPE, include_top=False)
base_model.trainable = False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;wrap-our-model-around-it&quot;&gt;Wrap our model around it&lt;/h2&gt;

&lt;p&gt;With EfficientNet imported, we can use it to our problem by wrapping our classification model around it. You can think of the EfficientNetB0 as a trained feature extractor. The final model has:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;An input layer&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;EfficientNetB0 base model&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;An average pooling layer: Pool the information by average operation&lt;/li&gt;
  &lt;li&gt;A dropout layer: Set a percentage of inputs to zero&lt;/li&gt;
  &lt;li&gt;A classification layer: Output the probability of NonCOVID&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can also use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras.utils.plot_model&lt;/code&gt; to visualize our model. &lt;img src=&quot;/assets/cnn-covid-19/model.png&quot; alt=&quot;model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;?&lt;/code&gt; in input and output shape is a reserved place for the number of samples, which the model does not know yet.&lt;/li&gt;
  &lt;li&gt;EfficientNetB0 sits right after the input layer.&lt;/li&gt;
  &lt;li&gt;The last (classification) layer has an output of dimension 1: The probability for NonCOVID.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;training-our-model&quot;&gt;Training our model&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Public data pre-training&lt;/strong&gt;: To help EfficientNetB0 adapt to COVID vs NonCOVID image classification, we‚Äôve actually trained our model on another public CT scan data &lt;a href=&quot;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&quot;&gt;set&lt;/a&gt;. The hope is that training the model on CT scans would allow it to learn features specific to our COVID-19 classification task. We will not go into the public data training part, but the procedure is essentially the same as what I will show below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transfer learning workflow&lt;/strong&gt;: We use a typical transfer-learning workflow:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Phase 1 (Feature extraction): Fix EfficientNetB0‚Äôs weights, only update the last classification layer‚Äôs weights.&lt;/li&gt;
  &lt;li&gt;Phase 2 (Fine tuning): Allow some of EfficientNetB0‚Äô weights to update as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can read more about the workflow &lt;a href=&quot;https://www.tensorflow.org/guide/keras/transfer_learning#the_typical_transfer-learning_workflow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key configurations&lt;/strong&gt;: We use the following metrics and loss function:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: to evaluate model performance
    &lt;ul&gt;
      &lt;li&gt;Binary accuracy&lt;/li&gt;
      &lt;li&gt;False and true positives&lt;/li&gt;
      &lt;li&gt;False and true negatives&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: to guide gradient search
    &lt;ul&gt;
      &lt;li&gt;Binary cross-entropy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We use the &lt;code class=&quot;highlighter-rouge&quot;&gt;Adam&lt;/code&gt; optimizer, the learning rates is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;[1e-3, 1e-4]&lt;/code&gt; and the number of training epochs is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;[10, 30]&lt;/code&gt; for the two phases, respectively. The two-phase training is iterated for two times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training history&lt;/strong&gt;: Let‚Äôs visualize the training history: &lt;img src=&quot;/assets/cnn-covid-19/training_history.png&quot; alt=&quot;training_history&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here you can see that after we‚Äôve allowed some layers of EfficientNets to update (after Epoch 10), we obtain a significant improvement in classification accuracy. The final training and validation accuracy is around 98% and 82%.&lt;/p&gt;

&lt;h1 id=&quot;how-does-it-perform-on-test-data&quot;&gt;How does it perform on test data?&lt;/h1&gt;
&lt;p&gt;We can obtain a set of test data from the same data repo that contains 105 NonCOVID and 98 COVID images. Let‚Äôs see how the trained model performs on them. Here‚Äôs a result breakdown using &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.metrics.classification_report&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;              precision    recall  f1-score   support

       COVID       0.85      0.83      0.84        98
    NonCOVID       0.84      0.87      0.85       105

    accuracy                           0.85       203
   macro avg       0.85      0.85      0.85       203
weighted avg       0.85      0.85      0.85       203
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And the ROC curve: &lt;img src=&quot;/assets/cnn-covid-19/roc_curve.png&quot; alt=&quot;roc_curve&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-are-correctly-and-incorrectly-classified-ct-scans&quot;&gt;What are correctly and incorrectly classified CT scans?&lt;/h2&gt;

&lt;p&gt;We can dive into the classification result and see which ones are identified correctly and identified incorrectly. &lt;strong&gt;Potential patterns&lt;/strong&gt; found could be leveraged to help further improve the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Could you identify some patterns?&lt;/strong&gt;
&lt;img src=&quot;/assets/cnn-covid-19/true_positives.png&quot; alt=&quot;true_positives&quot; /&gt;
&lt;img src=&quot;/assets/cnn-covid-19/true_negatives.png&quot; alt=&quot;true_negatives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here are the incorrect ones:
&lt;img src=&quot;/assets/cnn-covid-19/false_positives.png&quot; alt=&quot;false_positives&quot; /&gt;
&lt;img src=&quot;/assets/cnn-covid-19/false_negatives.png&quot; alt=&quot;false_negatives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can probably make several observations here:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;True positives have obvious abnormal patterns, and the lung structures are well-preserved.&lt;/li&gt;
  &lt;li&gt;Many of the true negatives have complete black lungs (no abnormal pattern).&lt;/li&gt;
  &lt;li&gt;The lung boundaries of many false positives are not clear.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The point is, to a non-medical person like me, many of the COVID and NonCOVID images look the same. The ambiguity is even more severe when some images have unclear lung boundaries. It seems like our CNN is also having trouble distinguishing those images.&lt;/p&gt;

&lt;h1 id=&quot;where-do-we-go-from-here&quot;&gt;Where do we go from here?&lt;/h1&gt;

&lt;p&gt;From the above results, we can see that a pre-trained CNN can be adapted to achieve a really strong baseline performance. However, there are clear limitations to what a deep learning model (or any other model) alone can achieve. In this case, computer vision researchers and medical experts need to collaborate in a meaningful way so that the end model is both computationally capable and medically reliable.&lt;/p&gt;

&lt;p&gt;There are several directions for which we could make the model better:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Lung segmentation&lt;/strong&gt;: Process each image and retain only the lung area of the CT scan, for example, see &lt;a href=&quot;https://pubs.rsna.org/doi/full/10.1148/rg.2015140232&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;More sophisticated &lt;strong&gt;transfer learning&lt;/strong&gt; design: For example, see multi-task &lt;a href=&quot;https://ruder.io/multi-task/&quot;&gt;learning&lt;/a&gt; or supervised domain &lt;a href=&quot;https://en.wikipedia.org/wiki/Domain_adaptation#The_different_types_of_domain_adaptation&quot;&gt;adaptation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ensemble&lt;/strong&gt; model: This seems like a common belief, especially among Kaggle users, that building an ensemble &lt;a href=&quot;https://scikit-learn.org/stable/modules/ensemble.html&quot;&gt;model&lt;/a&gt; would almost always give you an extra few percent accuracy increases.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;That‚Äôs it for our CNN COVID-19 CT scan classification! Thank you!&lt;/em&gt; üëèüëèüëè&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Convolutional Neural Network: How is it different from the other networks?</title>
   <link href="http://localhost:4000/data/2020/09/24/intro-to-cnn.html"/>
   <updated>2020-09-24T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/24/intro-to-cnn</id>
   <content type="html">&lt;p&gt;I am not a deep learning researcher, but I‚Äôve come to know a few things about neural networks through various exposures. I‚Äôve always heard that CNN is a type of neural network that‚Äôs particularly good at image-related problems. But, what does that really mean? What‚Äôs with the word ‚Äúconvolutional‚Äù? What‚Äôs so unusual about an image-related problem that a different network is required?&lt;/p&gt;

&lt;p&gt;Recently I had the opportunity to work on a COVID-19 image classification problem and built a CNN-based classifier using tensorflow.kerasthat achieved an 87% accuracy rate. More importantly, I think that I‚Äôve figured out the answers to those questions. In this post, I share with you those answers in an intuitive math-free way. If you are already familiar with DNNs and CNNs, this post should feel like a good refresher. If not, at the end of this post, you could gain an intuitive understanding of the motivation behind CNN and the unique features that define a CNN.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#deep-neural-network&quot; id=&quot;markdown-toc-deep-neural-network&quot;&gt;Deep neural network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#forward-propagation&quot; id=&quot;markdown-toc-forward-propagation&quot;&gt;Forward propagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backpropagation&quot; id=&quot;markdown-toc-backpropagation&quot;&gt;Backpropagation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convolutional-neural-network&quot; id=&quot;markdown-toc-convolutional-neural-network&quot;&gt;Convolutional neural network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-power-of-convolution&quot; id=&quot;markdown-toc-the-power-of-convolution&quot;&gt;The power of convolution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#key-features-of-a-cnn&quot; id=&quot;markdown-toc-key-features-of-a-cnn&quot;&gt;Key features of a CNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-next&quot; id=&quot;markdown-toc-whats-next&quot;&gt;What‚Äôs next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;deep-neural-network&quot;&gt;Deep neural network&lt;/h1&gt;

&lt;p&gt;If you are familiar with DNNs, feel free to skip to &lt;a href=&quot;#Convolutional neural network&quot;&gt;Convolutional neural network&lt;/a&gt;. Before diving into neural networks, let‚Äôs first see the machine learning big picture:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; (ML) is the study of computer algorithms that improve automatically through experience. ‚Äì Wikipedia&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking at the problems that ML tries to solve, ML is often sliced into&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Supervised learning: predicting a label, e.g., classification, or a continuous variable;&lt;/li&gt;
  &lt;li&gt;Unsupervised learning: pattern recognition for unlabeled data, e.g., clustering;&lt;/li&gt;
  &lt;li&gt;Reinforcement learning: algorithms learn the best way to ‚Äúbehave‚Äù, e.g., AlphaGo, self-driving cars.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among others, deep learning is a powerful form of machine learning that has garnered much attention for its successes in computer vision (e.g., image recognition), natural language processing, and beyond. The neural network is inspired by information processing and communication nodes in biological systems. By design, input data is passed through layers of the network, containing several nodes, analogous to ‚Äúneurons‚Äù. The system then outputs a particular representation of the information. DNN is probably the most well-known network for deep learning, and it can be trained to learn the features of the data very well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/deep-nn.jpg&quot; alt=&quot;Deep neural network&quot; /&gt;Image &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347705/&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Roughly speaking, there are two important operations that make a neural network.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Forward propagation&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward propagation&lt;/h2&gt;
&lt;p&gt;This is the &lt;strong&gt;prediction&lt;/strong&gt; step. The network reads the input data, computes its values across the network, and gives a final output value.&lt;/p&gt;

&lt;p&gt;But how does the network computes an output value? Let‚Äôs see what happens in a single layer network when it makes one prediction. It takes input as a vector of numbers. Each node in the layer has its weight. When the input value is passed through the layer, the network computes its weighted sum. This is usually followed by a (typically nonlinear) activation function, e.g., step function, where the weighted sum is ‚Äúactivated‚Äù.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/perceptron.jpg&quot; alt=&quot;perceptron&quot; /&gt; Image &lt;a href=&quot;https://deepai.org/machine-learning-glossary-and-terms/perceptron&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you know a bit about algebra, this is what the operation is doing:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(\mathbf{w}\cdot \mathbf{x} + b)&lt;/script&gt;

&lt;p&gt;where $\mathbf{w}\cdot \mathbf{x} + b$ is the weighted sum, $f(\cdot)$ is the activation function, and $y$ is the output. Now, in a deeper neural network, the procedure is essentially the same, i.e., the &lt;strong&gt;input ‚Äì&amp;gt; weighted sum ‚Äì&amp;gt; activation&lt;/strong&gt; process is repeated for each layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/mlp.png&quot; alt=&quot;mlp&quot; /&gt; Image &lt;a href=&quot;https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2020/lectures/03/MLP_and_backprop.html&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;backpropagation&quot;&gt;Backpropagation&lt;/h2&gt;
&lt;p&gt;This is the &lt;strong&gt;training&lt;/strong&gt; step. By comparing the network‚Äôs predictions/outputs and the ground truth values, i.e., compute loss, the network adjusts its parameters to improve the performance.&lt;/p&gt;

&lt;p&gt;How does the network adjust the parameters (weights and biases) through training? This is done through an operation called &lt;strong&gt;backpropagation&lt;/strong&gt;, or backprop. The network takes the loss and recursively calculates the loss function‚Äôs slope with respect to each parameter. Calculating these slopes requires the usage of chain rule from calculus; you can read more about it &lt;a href=&quot;https://sebastianraschka.com/faq/docs/backprop-arbitrary.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An optimization algorithm is then used to update network parameters using the gradient information until the performance cannot be improved anymore. One commonly used optimizer is stochastic gradient descent.&lt;/p&gt;

&lt;p&gt;One analogy often used to explain gradient-based optimization is hiking. Training the network to minimize loss is like getting down to the lowest point on the ground from a mountain. Backprop operation finding the loss function gradients is like finding the path on your way down. The optimization algorithm is the step where you actually take the path and eventually reach the lowest point. &lt;img src=&quot;/assets/cnn-covid-19/gradient-descent.png&quot; alt=&quot;gradient-descent&quot; /&gt; Image &lt;a href=&quot;https://www.datasciencecentral.com/profiles/blogs/alternatives-to-the-gradient-descent-algorithm&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am glossing over many details, but I hope you now know that DNN&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is a powerful &lt;strong&gt;machine learning&lt;/strong&gt; technique;&lt;/li&gt;
  &lt;li&gt;can be used to tackle &lt;strong&gt;supervised&lt;/strong&gt;, &lt;strong&gt;unsupervised&lt;/strong&gt; and &lt;strong&gt;reinforcement learning&lt;/strong&gt; problems;&lt;/li&gt;
  &lt;li&gt;consists of forward propagation (&lt;strong&gt;input to output&lt;/strong&gt;) and backpropagation (&lt;strong&gt;error to parameter update&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are ready to talk about CNN!&lt;/p&gt;

&lt;h1 id=&quot;convolutional-neural-network&quot;&gt;Convolutional neural network&lt;/h1&gt;

&lt;p&gt;Ordinary neural networks that we‚Äôve talked about above expect input data to be a &lt;strong&gt;vector of numbers&lt;/strong&gt;, i.e., $\mathbf{x} = [x_1, x_2, x_3, \dots]$. What if we want to train an &lt;strong&gt;image classifier&lt;/strong&gt;, i.e., use an image as the input? Let‚Äôs talk about some digital image basics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An image is a &lt;strong&gt;collection of pixels&lt;/strong&gt;. For example, a 32-by-32 image has $32 \times 32 = 1024$ pixels.&lt;/li&gt;
  &lt;li&gt;Each pixel is an &lt;strong&gt;intensity represented by a number&lt;/strong&gt; in the range $[0, 255]$, $0$ is black and $255$ is white.&lt;/li&gt;
  &lt;li&gt;Color images have three dimensions: &lt;strong&gt;[width, height, depth]&lt;/strong&gt; where depth is usually 3.&lt;/li&gt;
  &lt;li&gt;Why is depth 3? That‚Äôs because it encodes the intensity of [&lt;strong&gt;R&lt;/strong&gt;ed, &lt;strong&gt;G&lt;/strong&gt;reen, &lt;strong&gt;B&lt;/strong&gt;lue], i.e., RGB values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, this black and white Lincoln image is just a matrix of integers. 
&lt;img src=&quot;/assets/cnn-covid-19/image_pixel.png&quot; alt=&quot;image_pixel&quot; /&gt; Image &lt;a href=&quot;https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since a digital image can be represented as a 2D grid of pixel values, we could stretch out/flatten the grid, make it into a vector of numbers and feed it into a neural network. That solves the problem‚Ä¶right?&lt;/p&gt;

&lt;p&gt;However, there are two major limitations to this approach.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;It does not scale well to bigger images.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;While it is still manageable for an input with $32\times32 = 1024$ dimensions, most real-life images are bigger than this.&lt;/li&gt;
      &lt;li&gt;For example, a color image of size 320x320x3 would translate to an input with dimension &lt;strong&gt;307200&lt;/strong&gt;!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;It does not consider the properties of an image.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Locality&lt;/em&gt;: Nearby pixels are usually strongly correlated (e.g., see the outline of Lincoln‚Äôs face). Stretching it out breaks the pattern.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Translation invariance&lt;/em&gt;: Meaningful features could occur anywhere on an image, e.g., see the flying bird.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/flying-bird.png&quot; alt=&quot;bird&quot; /&gt; Image &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L3%20-%20UUCLxDeepMind%20DL2020.pdf&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-power-of-convolution&quot;&gt;The power of convolution&lt;/h2&gt;

&lt;p&gt;On the other hand, CNN is designed to scale well with images and take advantage of these unique properties. It does with two unique features:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Weight sharing&lt;/strong&gt;: All local parts of the image are processed with the same weights so that identical patterns could be detected at many locations, e.g., horizontal edges, curves and etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hierarchy of features&lt;/strong&gt;: Lower-level patterns learned at the start are composed to form higher-level ones across layers, e.g., edges to contours to face outline.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is done through the operation of &lt;strong&gt;convolution&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Define a filter: a 2D weight matrix of a certain size, e.g. 3-by-3 filter.&lt;/li&gt;
  &lt;li&gt;Convolve the whole image with the filter: multiply each pixel under the filter with the weight.&lt;/li&gt;
  &lt;li&gt;Convolution output forms a new image: a feature map.&lt;/li&gt;
  &lt;li&gt;Using multiple filters (each with a different weight matrix), different features can be captured.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;convolution-example-mean-filter&quot;&gt;Convolution example: mean filter&lt;/h4&gt;
&lt;p&gt;Actually, let‚Äôs see the operation in numbers and images. It will be easier to understand what‚Äôs really going on. Here we create an image of a bright square using 0s and 1s. &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; interprets values in [0,1] the same as in [0, 255].&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Original image pixel values: 
 [[0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And this is how the image looks like: &lt;img src=&quot;/assets/cnn-covid-19/bright_square.png&quot; alt=&quot;square&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that a filter is a 2D weight matrix. Let‚Äôs create an example filter, and call it the &lt;strong&gt;‚Äúmean filter‚Äù&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[[0.11 0.11 0.11]
 [0.11 0.11 0.11]
 [0.11 0.11 0.11]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In a convolution, this ‚Äúmean filter‚Äù actually slides across the image, takes the values of 9 connected pixels, multiplies each with the weight (0.11), and returns the sum, i.e., the weighted average of the original 9 values, hence the name ‚Äúmean filter‚Äù: 
&lt;img src=&quot;/assets/cnn-covid-19/convolution.gif&quot; alt=&quot;convolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the averaging effect from the filtered image pixel values. It blurs out any edges in the image.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Filtered image pixel values: 
 [[0.11 0.22 0.33 0.22 0.11]
 [0.22 0.44 0.67 0.44 0.22]
 [0.33 0.67 1.   0.67 0.33]
 [0.22 0.44 0.67 0.44 0.22]
 [0.11 0.22 0.33 0.22 0.11]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;What‚Äôs this to do with a convolutional neural network?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Well, CNN essentially applies the same convolution procedure, but the key difference is it &lt;strong&gt;learns the filter weights&lt;/strong&gt; through backpropagation (training).&lt;/p&gt;

&lt;p&gt;Also, there are usually many filters for each layer, each with a different weight matrix, applied to the same image. Each filter would capture a different pattern of the same image. A CNN could also have many layers of convolution. The complexity of the network allows features at different scales to be captured. This is the hierarchy of features mentioned above.&lt;/p&gt;

&lt;p&gt;For example, here‚Äôs an illustration of features learned by filters from early to the latter part of the network.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Early filters capture edges and textures. (&lt;strong&gt;General&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;Latter filters form parts and objects. (&lt;strong&gt;Specific&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cnn-covid-19/feature.png&quot; alt=&quot;title&quot; /&gt; Image &lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;credit&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;key-features-of-a-cnn&quot;&gt;Key features of a CNN&lt;/h2&gt;

&lt;p&gt;While DNN uses many fully-connected layers, CNN contains mostly convolutional layers. In its simplest form, CNN is a network with a set of layers that transform an image to a set of class probabilities. Some of the most popular types of layers are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Convolutional layer&lt;/strong&gt; (CONV): Image undergoes a convolution with filters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RELU layer&lt;/strong&gt; (RELU): Element-wise nonlinear activation function (same as those in DNN before).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pooling layer&lt;/strong&gt; (POOL): Image undergoes a convolution with a mean (or max) filter, so it‚Äôs down-sampled.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fully-connected layer&lt;/strong&gt; (FC): Usually used as the last layer to output a class probability prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, if you are &lt;em&gt;designing your own CNN&lt;/em&gt;, there are many elements to play with. They generally fall into two categories:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Type of convolutional layer
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Depth&lt;/strong&gt;: The number of filters to use for each layer.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Stride&lt;/strong&gt;: How big of a step to take when sliding the filter across the image, usually 1 (see the convolution GIF above) or 2.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: Size of each convolution filter, e.g., the mean filter is 3-by-3.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Padding&lt;/strong&gt;: Whether to use paddings around images when doing convolution. This determines the output image size.&lt;/li&gt;
      &lt;li&gt;And others.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How to connect each layer?
    &lt;ul&gt;
      &lt;li&gt;Besides the type of layers, you need to design an architecture for your CNN. This is an active field of research, e.g., what‚Äôs a better architecture? or can we automatically search for a better architecture? Check ‚Äúneural architecture search‚Äù out if you are interested.&lt;/li&gt;
      &lt;li&gt;A commonly used architecture goes like this:
        &lt;ul&gt;
          &lt;li&gt;$\text{INPUT} \rightarrow [ [\text{CONV} \rightarrow \text{RELU}]^N \rightarrow \text{POOL}]^M \rightarrow [\text{FC} \rightarrow \text{RELU}]^K \rightarrow \text{FC}$&lt;/li&gt;
          &lt;li&gt;The power $N, M, K$ means that the operation is repeated those number of times.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What‚Äôs next?&lt;/h1&gt;

&lt;p&gt;Thank you for reading until the end! I hope by now you could see the difference between a CNN and a regular DNN, and also gained an intuitive understanding of what convolution operation is all about. Please let me know your thoughts or any feedback in the comment section below.&lt;/p&gt;

&lt;p&gt;In the next article, we explore how CNN can be used to build a COVID-19 CT scan image classifier. Unsurprisingly, a pre-trained CNN can achieve strong baseline performance (85% accuracy). However, it would take more than a neural network to produce reliable and convincing results. Here‚Äôs the article:&lt;br /&gt;
&lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/09/27/detecting-covid19-using-cnn.html&quot;&gt;What deep learning needs for better COVID-19 detection&lt;/a&gt;&lt;/p&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;further-resources&quot;&gt;Further resources&lt;/h1&gt;

&lt;p&gt;If you are interested in knowing more about CNNs, check out üëâ:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/&quot;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=shVKhOmT0HE&amp;amp;ab_channel=DeepMind&quot;&gt;DeepMind x UCL | Convolutional Neural Networks for Image Recognition
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and how to implement them üëâ:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://keras.io/getting_started/intro_to_keras_for_engineers/&quot;&gt;Introduction to Keras for Engineers
&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/images/cnn&quot;&gt;Tensorflow Keras CNN Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enjoy! üëèüëèüëè&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Statistical learning knowledge repository</title>
   <link href="http://localhost:4000/data/2020/09/17/knowledge-repository.html"/>
   <updated>2020-09-17T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/09/17/knowledge-repository</id>
   <content type="html">&lt;p&gt;This is a collection of my notes on various topics of statistical learning. It is intended as a knowledge repository for some of the unexpected discoveries, less-talked-about connections, and under-the-hood concepts for statistical learning. It‚Äôs a work in progress that I will periodically update.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ridge-regularization&quot; id=&quot;markdown-toc-ridge-regularization&quot;&gt;Ridge regularization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#relationship-between-ols-ridge-regression-and-pca&quot; id=&quot;markdown-toc-relationship-between-ols-ridge-regression-and-pca&quot;&gt;Relationship between OLS, Ridge regression, and PCA&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;ridge-regularization&quot;&gt;Ridge regularization&lt;/h1&gt;
&lt;h2 id=&quot;relationship-between-ols-ridge-regression-and-pca&quot;&gt;Relationship between OLS, Ridge regression, and PCA&lt;/h2&gt;
&lt;p&gt;Simple yet elegant relationships between OLS estimates, ridge estimates and PCA can be found through the lens of spectral decomposition. We see these relationships through Exercise 8.8.1 of MA&lt;sup id=&quot;fnref:MA&quot;&gt;&lt;a href=&quot;#fn:MA&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;set-up&quot;&gt;Set-up&lt;/h3&gt;

&lt;p&gt;Given the following regression model:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\mu \mathbf{1}+\mathbf{u}, \quad \mathbf{u} \sim N_{\mathrm{n}}\left(\mathbf{0}, \sigma^{2} \mathbf{I}\right),&lt;/script&gt;

&lt;p&gt;consider the columns of $\mathbf{X}$ have been standardized to have mean 0 and variance 1. Then the ridge estimate of $\boldsymbol{\beta}$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\beta}^* = (\mathbf{X}^{\top} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^{\top} \mathbf{y}&lt;/script&gt;

&lt;p&gt;where for given $\mathbf{X}$, $\lambda \ge 0$ is a small fixed ridge regularization parameter. Note that when $\lambda = 0$, it is just the OLS formulation. Also, consider the spectral decomposition of the var-cov matrix $\mathbf{X}^{\top} \mathbf{X} = \mathbf{G} \mathbf{L} \mathbf{G}^{\top}$. Let $\mathbf{W} = \mathbf{X}\mathbf{G}$ be the principal component transformation of the original data matrix.&lt;/p&gt;

&lt;h3 id=&quot;result-11&quot;&gt;Result 1.1&lt;/h3&gt;

&lt;p&gt;If $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ represents the parameter vector of principal components, then we can show that the ridge estimates $\boldsymbol{\alpha}^*$ can be obtained from OLS estimates $\hat{\boldsymbol{\alpha}}$ by simply scaling them with the ridge regularization parameter:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p.&lt;/script&gt;

&lt;p&gt;This result shows us two important insights:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For PC-transformed data, we can obtain the ridge estimates through a simple element-wise scaling of the OLS estimates.&lt;/li&gt;
  &lt;li&gt;The shrinking effect of ridge regularization depends on both $\lambda$ and eigenvalues of the corresponding PC:
    &lt;ul&gt;
      &lt;li&gt;Larger $\lambda$ corresponds to heavier shrinking for every parameter.&lt;/li&gt;
      &lt;li&gt;However, given the same $\lambda$, principal components corresponding to larger eigenvalues receive the least shrinking.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To demonstrate the two shrinking effects, I plotted the percentage shrunken ($1- l_{i}/(l_{i}+\lambda)$) as a function of the ordered principal components as well as the value of the ridge regularization parameter. The two shrinking effects are clearly visible from this figure.
&lt;img src=&quot;/assets/learning-repo/pca_ridge_lambda_effect.png&quot; alt=&quot;lambda_effect&quot; /&gt;&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.1:&lt;/summary&gt;
Since $\boldsymbol{\alpha} = \mathbf{G}^{\top}\boldsymbol{\beta}$ and $\mathbf{W} = \mathbf{X}\mathbf{G}$, then 

$$
\begin{align*}
\boldsymbol{\alpha}^* &amp;amp;= (\mathbf{W}^{\top}\mathbf{W} + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= (\Lambda + \lambda \mathbf{I})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= \operatorname{diag}((l_{i} + \lambda)^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

Hence 

$$
\alpha^*_{i} = (l_i + \lambda)^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$ 

for $i=1, \ldots, p$, where $\mathbf{w}_{(i)}$ is the $i$th column of $\mathbf{W}$. Since 

$$
\begin{align*}
\hat{\boldsymbol{\alpha}} &amp;amp;= (\mathbf{W}^{\top}\mathbf{W})^{-1} \mathbf{W}^{\top}\mathbf{y} \\
&amp;amp;= \operatorname{diag}(l_{i}^{-1}) \mathbf{W}^{\top}\mathbf{y}, \quad i=1, \ldots, p.
\end{align*}
$$

We have 

$$
\hat{\alpha}_{i} = l_i^{-1}\mathbf{w}^{\top}_{(i)}\mathbf{y} \,,
$$

for $i=1, \ldots, p$. Therefore, by comparing the two estimate expressions, we have the result

$$
\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}, \quad i=1, \ldots, p. \blacksquare
$$

&lt;/details&gt;

&lt;h3 id=&quot;result-12&quot;&gt;Result 1.2&lt;/h3&gt;

&lt;p&gt;It follows from Result 1.1 that we can establish a direct link between the OLS estimate $\hat{\boldsymbol{\beta}}$ and the ridge estimate $\boldsymbol{\beta}^*$ through spectral decomposition of the var-cov matrix. Specifically, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\beta}^{*}=\mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}, \quad \text { where } \mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right),&lt;/script&gt;

&lt;p&gt;for $i=1, \ldots, p$.&lt;/p&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.2:&lt;/summary&gt;
Since $\alpha_{i}^{*}=\frac{l_{i}}{l_{i}+\lambda} \hat{\alpha}_{i}$, for $i=1, \ldots, p$, and $\hat{\boldsymbol{\alpha}} = \mathbf{G}^{\top}\hat{\boldsymbol{\beta}}$, then

$$
\boldsymbol{\alpha}^{*} =  \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \,,
$$

by writing in matrix form. Also, since 

$$
\boldsymbol{\alpha}^* = \mathbf{G}^{\top}\boldsymbol{\beta}^*
$$

where $\boldsymbol{\beta}^*$ is the ridge estimate of $\boldsymbol{\beta}$, then we have

$$
\begin{align*}
\mathbf{G}^{\top}\boldsymbol{\beta}^* &amp;amp;= \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
\boldsymbol{\beta}^* &amp;amp;= \mathbf{G} \operatorname{diag}(l_{i}/(l_{i}+k)) \mathbf{G}^{\top}\hat{\boldsymbol{\beta}} \\
&amp;amp;= \mathbf{G D G}^{\top} \hat{\boldsymbol{\beta}}
\end{align*}
$$

where $\mathbf{D}=\operatorname{diag}\left(\frac{l_{i}}{l_{i}+k} \right)$, for $i=1, \ldots, p$. $\blacksquare$
&lt;/details&gt;

&lt;h3 id=&quot;result-13&quot;&gt;Result 1.3&lt;/h3&gt;

&lt;p&gt;One measure of the quality of the estimators $\boldsymbol{\beta}^*$ is the trace mean square error (MSE):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\phi(\lambda) &amp;= \operatorname{tr} E[(\boldsymbol{\beta}^* - \boldsymbol{\beta})(\boldsymbol{\beta}^* - \boldsymbol{\beta})^{\top}] \\
&amp;= \sum_{i=1}^{p} E[(\beta_{i}^* - \beta_{i})^2] \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, from the previous two results, we can show that the trace MSE of the ridge estimates can be decomposed into two parts: &lt;strong&gt;variance&lt;/strong&gt; and &lt;strong&gt;bias&lt;/strong&gt;, and obtain an explicit formula for them. The availability of the exact formula for MSE allows things like regularization path to be computed easily.&lt;/p&gt;

&lt;p&gt;Specifically, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(\lambda) = \gamma_1(\lambda) + \gamma_2(\lambda)&lt;/script&gt;

&lt;p&gt;where the first component is the sum of variances:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_1(\lambda) = \sum_{i=1}^{p} V(\beta_{i}^*) = \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,,&lt;/script&gt;

&lt;p&gt;and the second component is the sum of squared biases:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_2(\lambda) = \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 = \lambda^2 \sum_{i=1}^{p} \frac{\alpha_i^2}{(l_i + \lambda)^2} \,.&lt;/script&gt;

&lt;details&gt;
    &lt;summary&gt;Proof of Result 1.3:&lt;/summary&gt;
First let's look at the sum of variances. We start by writing out the expression for the variance of the ridge estimates:

$$
\begin{align*}
V(\boldsymbol{\beta}^*) &amp;amp;= \mathbf{GDG}^{\top} V(\hat{\boldsymbol{\beta}}) \mathbf{GDG}^{\top} \\
&amp;amp;= \mathbf{GDG}^{\top} \sigma^2 \mathbf{X^{\top}X}^{-1} \mathbf{GDG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{GDG}^{\top} (\mathbf{GLG})^{-1} \mathbf{GDG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{GD} L^{-1} \mathbf{DG}^{\top} \\
&amp;amp;= \sigma^2 \mathbf{G} \operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \,, \quad i=1, \ldots, p.
\end{align*}
$$ 

Hence, by extracting out the variances from the diagonal, we obtain the first expression:

$$
\begin{align*}
\gamma_1(\lambda) &amp;amp;= \sum_{i=1}^{p} V(\beta_{i}^*) \\
&amp;amp;= \operatorname{tr} V(\boldsymbol{\beta}^*) \\
&amp;amp;= \sigma^2 \operatorname{tr}(\operatorname{diag}(l_i/(l_i + \lambda)^2) \mathbf{G}^{\top} \mathbf{G}) \\
&amp;amp;= \sigma^2 \sum_{i=1}^{p} \frac{l_i}{(l_i + \lambda)^2} \,.
\end{align*}
$$

Now let's look at the bias term. We write it in matrix form to see that:

$$
\begin{align*}
\gamma_2(\lambda) &amp;amp;= \sum_{i=1}^{p} (E[\beta_{i}^* - \beta_{i}])^2 \\
&amp;amp;= (E[\boldsymbol{\beta}^*] - \boldsymbol{\beta})^{\top}(E[\boldsymbol{\beta}^*] - \boldsymbol{\beta}) \\
&amp;amp;= (\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta})^{\top}(\mathbf{GDG}^{\top}\boldsymbol{\beta} - \boldsymbol{\beta}) \\
&amp;amp;= \mathbf{B}^{\top}\mathbf{GD}^2\mathbf{G}^{\top}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\top}\mathbf{GDG^{\top}}\boldsymbol{\beta} + \boldsymbol{\beta}^{\top}\boldsymbol{\beta} \\
&amp;amp;= \boldsymbol{\alpha}^{\top}\mathbf{D}^2\boldsymbol{\alpha} - 2\boldsymbol{\alpha}^{\top}\mathbf{D}\boldsymbol{\alpha} + \boldsymbol{\alpha}^{\top}\mathbf{G^{\top}G}\boldsymbol{\alpha} \\
&amp;amp;= \boldsymbol{\alpha}^{\top} (\mathbf{D}^2 - 2\mathbf{D} + 1) \boldsymbol{\alpha} \\
&amp;amp;= \boldsymbol{\alpha}^{\top} \operatorname{diag}(\lambda^2/(l_i + \lambda)^2) \boldsymbol{\alpha} \\
&amp;amp;= \lambda^2 \sum_{i=1}^{p}(\alpha_{i}^2/(l_i + \lambda)^2) \,.
\end{align*}
$$

Combining the two gamma terms completes the proof. $\blacksquare$
&lt;/details&gt;

&lt;h3 id=&quot;result-14&quot;&gt;Result 1.4&lt;/h3&gt;

&lt;p&gt;This is a quick but revealing result that follows from Result 1.3. Taking a partial derivative of the trace MSE function with respect to $\lambda$ and take $\lambda = 0$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial{\phi(\lambda)}}{\partial{\lambda}} = -2 \sigma^2 \sum_{i=1}^{p} 1/l_i^2 \,.&lt;/script&gt;

&lt;p&gt;Notice that the gradient of the trace MSE function is negative when $\lambda$ is 0. This tells us two things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;We can reduce the trace MSE by taking a non-zero $\lambda$ value. In particular, we are trading a bit of bias for a reduction in variance as the variance function ($\gamma_1$) is monotonically decreasing in $\lambda$. However, we need to find the right balance between variance and bias so that the overall trace MSE is minimized.&lt;/li&gt;
  &lt;li&gt;The reduction in trace MSE by ridge regularization is higher when some $l_i$s are small. That is, when there is considerable collinearity among the predictors, ridge regularization can achieve much smaller trace MSE than OLS.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;visualization&quot;&gt;Visualization&lt;/h3&gt;

&lt;p&gt;Using the &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.metrics.make_regression&lt;/code&gt; function, I generated a noisy regression data set with 50 samples and 10 features. However, most of the variances (in PCA sense) are explained by 5 of those 10 features, i.e. last 5 eigenvalues are relatively small. Here are the regularization path and the coefficient error plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/learning-repo/ridge_lambda_mse.png&quot; alt=&quot;ridge_error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figure, we can clearly see that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Increasing $\lambda$ shrinks every coefficient towards 0.¬†&lt;/li&gt;
  &lt;li&gt;OLS procedure (left-hand side of both figures) produces erroneous (and with a large variance) estimate. the estimator MSE is significantly larger than that of the ridge regression.¬†&lt;/li&gt;
  &lt;li&gt;An optimal $\lambda$ is found at around 1 where the MSE of ridge estimated coefficients is minimized.¬†&lt;/li&gt;
  &lt;li&gt;On the other hand, $\lambda$ values larger and smaller than 1 are suboptimal as they lead to over-regularization and under-regularization in this case.&lt;/li&gt;
&lt;/ul&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot, credit to &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_regression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coef&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;n&quot;&gt;effective_rank&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train the model with different regularisation strengths
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Display results
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;121&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coefs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xscale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'log'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'weights'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Ridge coefficients as a function of $&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tight'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;122&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lambdas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xscale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'log'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Coefficient error as a function of $&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lambda$'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tight'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:MA&quot;&gt;
      &lt;p&gt;Mardia, K. V., Kent, J. T., &amp;amp; Bibby, J. M. &lt;em&gt;Multivariate Analysis&lt;/em&gt;. 1979. Probability and mathematical statistics. Academic Press Inc.¬†&lt;a href=&quot;#fnref:MA&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Ëèä‰∏éÂàÄÔºö‰∏â‰∏™ÊúàÁöÑÁ†îÁ©∂ÂÜôÂá∫‰∫Ü‰∫∫Á±ªÂ≠¶ÊúÄÂá∫ÂêçÁöÑ‰π¶Ôºü</title>
   <link href="http://localhost:4000/reading/2020/08/02/The-Chrysanthemum-and-the-Sword.html"/>
   <updated>2020-08-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2020/08/02/The-Chrysanthemum-and-the-Sword</id>
   <content type="html">&lt;p&gt;Êó∂Èó¥ÊòØ‰∏™Â•áÂ¶ôÁöÑ‰∏úË•ø„ÄÇÁ¨¨‰∏ÄÊ¨°Áü•ÈÅì„ÄäËèä‰∏éÂàÄ„ÄãËøòÊòØÂú®‰∏≠Â≠¶ÁöÑËØæÂ†Ç‰∏äÔºåËøô‰π¶ÂèØËÉΩÊòØÊé®ËçêËØæÂ§ñÈòÖËØª‰∏õ‰π¶‰πã‰∏ÄÔºåÈÇ£Êó∂ÁöÑÊàëÂØπÊó•Êú¨Ê≤°‰ªÄ‰πàÁâπÂà´ÂÖ¥Ë∂£ÔºåÂè™‰æùÁ®ÄËÆ∞ÂæóÂæêÊÖßËÄÅÂ∏àÂú®ËÆ≤Âè∞‰∏ä‰ªãÁªçËøôÊú¨‰π¶ÁöÑÊ®°Ê†∑„ÄÇÂçÅÂπ¥‰πãÂêéÔºåÂÅ∂ÁÑ∂ÂÜçÊ¨°ÁúãÂà∞Ê≠§‰π¶ÔºåÂØπÊ®±Ëä±‰πãÂõΩÁöÑÂéÜÂè≤ÂíåÊñáÂåñÊúâ‰∫Ü‰∏ÄÂÆö‰∫ÜËß£ÔºåËØªÁΩ¢ÔºåÊÑüËß¶È¢áÂ§ö„ÄÇ&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 50%;&quot;&gt;&lt;img src=&quot;/assets/books/C_and_S_cover&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‰π¶ÂêçÔºö&lt;a href=&quot;https://book.douban.com/subject/26171368/&quot;&gt;Ëèä‰∏éÂàÄ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ÂéüÂêçÔºöThe Chrysanthemum and the Sword&lt;/li&gt;
  &lt;li&gt;‰ΩúËÄÖÔºöÈ≤ÅÊÄù¬∑Êú¨Â∞ºËø™ÂÖãÁâπ (Ruth Benedict)&lt;/li&gt;
  &lt;li&gt;ËØëËÄÖÔºöËÉ°Êñ∞Ê¢Ö&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-ËÉåÊôØ&quot;&gt;1. ËÉåÊôØ&lt;/h3&gt;

&lt;p&gt;È≤ÅÊÄùÊòØÁæéÂõΩÁöÑ‰∏Ä‰Ωç‰∫∫Á±ªÂ≠¶ÂÆ∂ÔºåÂ•πÂÖ≥Ê≥®Ê∞ëÊóèÊñáÂåñÁöÑ‰∫ßÁîü‰ª•ÂèäÂ∑ÆÂºÇÔºåËÆ§‰∏∫ÂØπ‰∏çÂêåÊ∞ëÊóèÁöÑ‰∫ÜËß£‰∏éÂ∞äÈáçÊòØÊàë‰ª¨ËÉΩÂíåÂπ≥ÂÖ±Â≠òÁöÑÂü∫Á°ÄÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The purpose of anthropology is to make the world safe for human differences.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÁæéÂõΩÂä†ÂÖ•‰∫åÊàòÂêéÔºåÈ≤ÅÊÄùË¢´ÊàòÊó∂ÊÉÖÊä•Â±Ä(Office of War Information)ËÅò‰∏∫ÁâπÊÆäÈ°æÈóÆÔºå‰∏ìËÅåÁ†îÁ©∂ÂêÑÂõΩÂõΩÊ∞ëÊÄßÔºåÂåÖÊã¨Êó•Êú¨„ÄÇÂü∫‰∫éÂΩìÊó∂ÁöÑÂ∑•‰ΩúÂÜôÂá∫ÁöÑ„ÄäËèä‰∏éÂàÄ„Äã(1946) Áé∞Âú®ÈÄöÂ∏∏Ë¢´ËÆ§‰∏∫ÊòØÊúÄÁïÖÈîÄ„ÄÅÊúÄÁªèÂÖ∏ÁöÑË•øÊñπÊàòÂêéÊó•Êú¨‰∫∫Ë´ñËëó‰Ωú„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºå„ÄäËèä‰∏éÂàÄ„ÄãÂú®Êó•Êú¨Âíå‰∏≠ÂõΩÂÆ∂ÂñªÊà∑ÊôìÔºåÂá†‰πéÊòØ‚ÄúÊó•Êú¨‰∫∫ÂõΩÊ∞ëÊÄßÁ†îÁ©∂‚ÄùÁöÑ‰ª£Ë®ÄËØçÔºåËÄåÂú®ÁæéÂõΩÔºåËøôÂè™ÊòØÈ≤ÅÊÄùËÅå‰∏öÁîüÊ∂Ø‰∏≠ÁöÑ‰∏Ä‰∏™Â∑•‰ΩúÔºåËØªËÄÖÂ§ßÂ§öÂ±ÄÈôê‰∫é‰∫öÊ¥≤Á†îÁ©∂Â≠¶ËÄÖ„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂêåÊ†∑‰∏ÄÊú¨‰π¶Âú®Â§™Âπ≥Ê¥ãÁöÑ‰∏§Â≤∏ÊúâÁùÄÂÆåÂÖ®‰∏çÂêåÁöÑÂõûÂìçÔºåËøôÊòØ‰∏∫‰ªÄ‰πàÔºüÊòØÂê¶Ë∑üËØªËÄÖÁöÑË∫´‰ªΩÊúâÂÖ≥ÔºüÊòØÂê¶Ë∑üËøô‰π¶Âá∫ÁâàÁöÑÊó∂Èó¥ÊúâÂÖ≥ÔºüËá™Âá∫Áâà‰ª•Êù•ÔºåÂØπ„ÄäËèä‰∏éÂàÄ„ÄãÁöÑËØÑ‰ª∑‰πü‰ªéÊù•Ê≤°Â∞ëËøáÔºåÊúâ‰∫∫Áß∞ËµûÈ≤ÅÊÄùÂ∑•‰ΩúÁöÑÂºÄÂàõÊÄßÔºå‰πüÊúâ‰∫∫Ë¥®ÁñëÂ•πÁ†îÁ©∂ÁöÑÂáÜÁ°ÆÊÄßÔºåÊØïÁ´üÂ•π‰ªéÊú™ÂéªËøáÊó•Êú¨„ÄÇËøô‰∫õÈóÆÈ¢òÂæÖÊàë‰ª¨ÁúãËøáÊú¨‰π¶ÁöÑÂÜÖÂÆπ‰πãÂêéÂÜçÊù•ËÅä„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2-Ê°ÜÊû∂&quot;&gt;2. Ê°ÜÊû∂&lt;/h3&gt;

&lt;p&gt;‰ΩúËÄÖÂú®Á¨¨‰∏ÄÁ´†ÂÆö‰πâ‰∫ÜÈóÆÈ¢òÂíå‰ΩøÁî®ÁöÑÂàÜÊûêÊñπÊ≥ïÔºåÂú®Á¨¨‰∫åÁ´†Ë∞àËÆ∫‰∫ÜÊó•Êú¨‰∫∫ÁöÑÊàò‰∫âËßÇÔºåÂΩìÊó∂ÁöÑÁæéÂõΩ‰∫∫Êé•Ëß¶Âà∞Êó•Êú¨Â§ßÂ§öÊòØÈÄöËøáÊàò‰∫âÔºåËøô‰∏ÄÁ´†ÁöÑÂÜÖÂÆπÊòØ‰Ωú‰∏∫ÂºïÂ≠êÊù•Â∏¶Âá∫ÂêéÈù¢ÁöÑËÆ∫Ëø∞„ÄÇÁ¨¨‰∏â„ÄÅÂõõÁ´†ËÆ®ËÆ∫‰∫ÜÊó•Êú¨Á§æ‰ºö‰ø°Â•âÁöÑÁ≠âÁ∫ßÂà∂‰ª•ÂèäÂÆÉÂú®ÊòéÊ≤ªÊîøÂ∫úÊâÄÂÅöÁ§æ‰ºöÊîπÈù©‰∏≠ÁöÑ‰ΩìÁé∞„ÄÇ‰πãÂêéÈ≤ÅÊÄùËØ¶ÁªÜËØ¥Êòé‰∫ÜÂΩ¢ÊàêÊó•Êú¨‰∫∫‰∫∫ÁîüËßÇÁöÑÂêÑÁßçÈÅìÂæ∑ÂáÜÂàôÔºàÁ¨¨‰∫îÂà∞Á¨¨‰πùÁ´†Ôºâ„ÄÅ‰ªñ‰ª¨Èù¢‰∏¥ÁöÑÈÅìÂæ∑Âõ∞Â¢ÉÔºàÁ¨¨ÂçÅÁ´†Ôºâ„ÄÅÂØπËá™Êàë‰øÆÁÇºÁöÑÊâßËëóÔºàÁ¨¨ÂçÅ‰∏ÄÁ´†Ôºâ‰ª•ÂèäÂØπÂ≠êÂ•≥ÊïôËÇ≤ÁöÑËßÇÂØüÔºàÁ¨¨ÂçÅ‰∫åÁ´†Ôºâ„ÄÇÊúÄÂêé‰∏ÄÁ´†ÊòØ‰ΩúËÄÖÂØπÊó•Êú¨ÊàòÂêéÁ§æ‰ºöÁöÑÈ¢ÑÊµã„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;3-ÊñπÊ≥ï&quot;&gt;3. ÊñπÊ≥ï&lt;/h3&gt;

&lt;p&gt;‰ΩúËÄÖÂú®Á¨¨‰∏ÄÁ´†ÊèêÂá∫‰∫ÜÂ•πÁöÑÁ†îÁ©∂ÈóÆÈ¢òÔºåËØ¥Êòé‰∫ÜËøô‰∏çÊòØ‰∏ÄÊú¨‰ªãÁªçÊó•Êú¨Á§æ‰ºöÂíåÂéÜÂè≤ÁöÑ‰π¶Ôºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Âõ†Ê≠§ÔºåÊú¨‰π¶Âπ∂Èùû‰∏ÄÊú¨‰∏ìÈó®ËÆ∫Ëø∞Êó•Êú¨ÁöÑÂÆóÊïô„ÄÅÁªèÊµé„ÄÅÊîøÊ≤ªÊàñÂÆ∂Â∫≠ËßÇÂøµÁöÑ‰π¶ÔºåËÄåÊòØËÄÉÂØüÊó•Êú¨‰∫∫Âú®Êó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑ
‰∏Ä‰∫õÂõ∫ÊúâËßÇÂøµÔºå‰ªéËÄåÊé¢ËÆ®Êó•Êú¨Ê∞ëÊóèÁâπÊÄßÂΩ¢ÊàêÁöÑÂéüÂõ†„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Âú®‰ΩúËÄÖÁúãÊù•ÔºåÂõ∫ÊúâËßÇÂøµÊòØ‰æùÊçÆÔºåÊ∞ëÊóèÁâπÊÄßÊòØËÆÆÈ¢ò„ÄÇÊâÄ‰ª•ÔºåË∑üÂÖ∂‰ªñÂ≠¶ÊúØÁ†îÁ©∂‰∏ÄÊ†∑ÔºåÊú¨‰π¶ÊúâÂÆÉËÆæÁ´ãÁöÑÂÅáÂÆöÔºåÊúâÂÆÉÈóÆÈ¢òÁöÑÂ±ÄÈôêÊÄßÔºåÊâÄ‰ΩøÁî®ÁöÑÁ†îÁ©∂ÊñπÊ≥ï‰πü‰∏ç‰∏ÄÂÆöÂÆåÁæé„ÄÇÊàë‰ª¨Â§ßÂèØ‰∏çÂøÖÊääÂÆÉÁúãÂÅöÊòØÁªùÂØπÊùÉÂ®ÅÁöÑÊó•Êú¨Á†îÁ©∂Ôºå‰ª•ÂÆ¢ËßÇ„ÄÅÂ∞äÈáçÁöÑÁúºÂÖâÊù•ËØÑÂà§ÂÆÉÔºåÊØîËæÉÂêàÈÄÇ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩúËÄÖÁöÑÁ†îÁ©∂Âª∫Á´ãÂú®ËøôÊ†∑‰∏Ä‰∏™ÂÅáÂÆö‰πã‰∏äÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;‚Ä¶Âç≥ËÆ§‰∏∫ÈÇ£‰∫õÊúÄÂ≠§Á´ãÁöÑÁªÜÂ∞èË°å‰∏∫‰πãÈó¥‰πü‰∏ÄÂÆöÂ≠òÂú®ÁùÄÊüêÁßçÁ≥ªÁªüÊÄßÁöÑËÅîÁ≥ª„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Â∏¶ÁùÄËøôÊ†∑ÁöÑ‰ø°ÂøµÔºå‰ΩúËÄÖ‰∏ªË¶Å‰ªé‰∏§‰∏™ÊñπÂêëÁùÄÊâãÔºå‰∏Ä‰∏™ÊòØÁ†îÁ©∂Êó•Êú¨‰∏é‰∫öÂ§™Âú∞Âå∫ÂÖ∂‰ªñÂõΩÂÆ∂Ôºà‰∏ªË¶ÅÊòØ‰∏≠ÂõΩÔºâ‰πãÈó¥ÁöÑÊñáÂåñÁõ∏‰ººÊÄßÂíåÂ∑ÆÂºÇÊÄßÔºåÂè¶‰∏Ä‰∏™ÊòØÁ†îÁ©∂Êó•Êú¨‰∫∫ÁöÑÊó•Â∏∏Áêê‰∫ã„ÄÇËÄåÈâ¥‰∫éÊàòÊó∂Êó†Ê≥ïÂâçÂæÄÊó•Êú¨ÁîüÊ¥ªÔºå‰ΩúËÄÖÂΩìÊó∂Á†îÁ©∂ÁöÑÊùêÊñô‰∏ªË¶ÅÂàÜ‰∏âÁßçÔºö‰∏éÂ±Ö‰ΩèÂú®ÁæéÂõΩÁöÑÊó•Êú¨‰∫∫ËÆøË∞àÔºåÂÖ≥‰∫éÊó•Êú¨ÁöÑÂ∑≤ÊúâÊñáÁåÆËµÑÊñô‰ª•ÂèäÊó•Êú¨‰∫∫Âàõ‰ΩúÁöÑÁîµÂΩ±„ÄÇÊàë‰ª¨ÂèØ‰ª•ËÆ∞‰Ωè‰ΩúËÄÖÁöÑÂÅáÂÆö„ÄÅÊñπÊ≥ï‰ª•ÂèäÊùêÊñôÔºåÂõ†‰∏∫Ëøô‰∫õ‰πüÊòØÂêé‰∫∫ÂØπÊú¨‰π¶ÊàñË§íÊàñË¥¨ÁöÑËØÑ‰ª∑ÁöÑÁÑ¶ÁÇπ„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;4-Á≠âÁ∫ßÂà∂&quot;&gt;4. Á≠âÁ∫ßÂà∂&lt;/h3&gt;

&lt;p&gt;‰ΩúËÄÖËÆ§‰∏∫ÔºåÊó•Êú¨ÂèëËµ∑‰æµÁï•Êàò‰∫âÊòØÂõ†‰∏∫‰ªñ‰ª¨Áõ∏‰ø°Á≠âÁ∫ßÂà∂ÔºåÊ¨≤Âú®Â§ß‰∏ú‰∫öÂª∫Á´ãÊñ∞ÁöÑÁß©Â∫èÔºåËÆ©ÂêÑÂõΩ‚ÄúÂêÑÂÆâÂÖ∂‰Ωç‚Äù„ÄÇÂú®ËøôÈáåÁöÑÂàÜÊûê‰∏≠Ôºå‰ΩúËÄÖÂáÜÁ°ÆÂú∞ÊçïÊçâÂà∞‰∫ÜÂ§©ÁöáÂú®ÂÜõ‰∫ãË°åÂä®‰∏≠ÁöÑËßíËâ≤ÔºöÂÜõ‰∫∫ÁöÑÁ≤æÁ•ûÈ¢ÜË¢ñ‰∏éËÆ©Ë°åÂä®Ê≠£ÂΩìÂåñÁöÑÁêÜÁî±„ÄÇÂõ†‰∏∫Â§©ÁöáÂ∞±ÊòØÊó•Êú¨ÁöÑË±°ÂæÅÔºåÂÜõ‰∫∫‰ª¨ËÆ§‰∏∫‰∏äÊàòÂú∫Â∞±ÊòØ‚ÄúÂØπÂ§©ÁöáÊÑèÂøóÁöÑÊïàÂø†‚Äù„ÄÇ&lt;/p&gt;

&lt;p&gt;ÈÇ£Á≠âÁ∫ßÂà∂Âè™ÊúâÂú®ÂÜõÈòüÈáåÈáçË¶ÅÂêóÔºü‰∏çÔºåÊÅ∞ÊÅ∞Áõ∏ÂèçÔºå‰ΩúËÄÖËÆ§‰∏∫Êó•Êú¨‰∫∫‰ø°Â•âÁöÑÁ≠âÁ∫ßÂà∂‰ΩìÁé∞Âú®Á§æ‰ºö‰∏≠ÁöÑÊñπÊñπÈù¢Èù¢ÔºåÊØîÂ¶ÇÁ§æ‰ºöÈò∂Á∫ß„ÄÅÂÆ∂Â∫≠ÊàêÂëò„ÄÅÂπ¥ÈæÑ‰∏éÊÄßÂà´ÔºåÂú®Ëøô‰∫õËØ≠Â¢ÉÈáåÈÉΩÊúâÊòéÁ°ÆÁöÑÁ≠âÁ∫ßÂå∫ÂàÜÔºöÁöáÂÆ§È´ò‰∫éÂπ≥Ê∞ëÔºåÁà∂‰∫≤È´ò‰∫éÂ≠©Â≠êÔºåÈïøËæàÈ´ò‰∫éÊôöËæà‰ª•ÂèäÁî∑‰∫∫È´ò‰∫éÂ•≥‰∫∫„ÄÇÂü∫‰∫éËøôÊ†∑ÁöÑËßÇÂØüÔºå‰ΩúËÄÖÊÄªÁªìËØ¥&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Êó•Êú¨‰∫∫ÂØπ‰∫éÁ≠âÁ∫ßÂà∂ÁöÑ‰ø°ËµñÊòØ‰ªñ‰ª¨Â§ÑÁêÜ‰∫∫‰∏é‰∫∫„ÄÅ‰∫∫‰∏éÂõΩÂÆ∂ÂÖ≥Á≥ªÁ≠â‰∏ÄÂàáÂÖ≥Á≥ªÁöÑÂü∫Á°Ä„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ËøôÈáåÊàë‰ª¨ÂÆûÈôÖ‰∏äËÉΩÁúãÂà∞‰∏§‰∏™Ë¢´Âêé‰∫∫ÊâπËØÑÁöÑÁÇπÔºö‰ΩúËÄÖÂ∏∏‰ª•‚ÄúÊó•Êú¨‰∫∫‚ÄùÂºÄÂ§¥ÔºåÊöóÊåáÊâÄÊúâ‰∫∫ÈÉΩ‰∏ÄÊ†∑ÔºåËøôÊ†∑ÁöÑËÆ∫Ëø∞ÊúâËøá‰∫éÁÆÄÂåñÁöÑÂ´åÁñëÔºõËÄå‚Äú‰∏ÄÂàá‚ÄùËøôÁ±ª‰ª£Ë°®ÁªùÂØπÁ®ãÂ∫¶ÁöÑËØçÊõ¥ÊòØÂú®Â≠¶ÊúØÁ†îÁ©∂‰∏≠ÊûÅÂ∞ëËßÅÂà∞ÔºåÊâπËØÑËÄÖÂè™ÈúÄÊâæÂá∫‰∏Ä‰∏™Âèç‰æã‰æøËÉΩËÆ©‰Ω†Âú®ÈóÆÁ≠îÁéØËäÇÁû¨Èó¥Â∞¥Â∞¨„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏çËøáÊàëÂÄíÊòØÂèØ‰ª•ÊÉ≥Âà∞‰∏Ä‰∏™ËØ¥ÊòéÁ≠âÁ∫ßÂà∂ËßÇÂøµÊ∏óÈÄè‰∫éÊó•Êú¨Á§æ‰ºöÁöÑ‰æãÂ≠êÔºöÊó•ËØ≠„ÄÇÂú®Êó•ËØ≠ÈáåÔºåÊ†πÊçÆËØ¥ËØù‰∫∫ÂèåÊñπÁöÑÁ≠âÁ∫ßÔºåÊâÄÁî®ÁöÑËØçËØ≠Ë¶ÅÁõ∏Â∫îÊîπÂèòÔºåÊØîÂ¶ÇÂΩìÊàëÂú®Ë∑ü‰∏äÁ∫ßËÆ≤ËØùÊó∂ÔºåÊàëÈúÄË¶Å‰ΩøÁî®ÊâÄË∞ìÁöÑÂ∞äÊï¨ÂûãËØçËØ≠ÂΩ¢ÂºèÔºåËÄåË∑üÊàëÁöÑÂêåËæàÂàô‰∏çÁî®ÔºåÁîöËá≥ÊàëÂèØ‰ª•‰ΩøÁî®‚ÄúÂêë‰∏ã‚ÄùÂûãË∑üÂ∞èÂ≠©ÂÑøËÆ≤ËØù(„Åï„Åó„ÅÇ„Åí„Åæ„Åô/„ÅÇ„Åí„Åæ„Åô/„ÇÑ„Çä„Åæ„Åô)„ÄÇ‰∏äÁ∫ßÂåÖÊã¨ËÄÅÊùø„ÄÅÂÆ¢Êà∑„ÄÅÈïøËæà‰ª•Âèä‰ªª‰ΩïÊàë‚ÄúÂ∫îËØ•‚Äù‰ª•Â∞äÊï¨ËØ≠Ê∞î‰∫§Ë∞àÁöÑ‰∫∫Ôºå‰πüÂ∞±ÊòØËØ¥ÔºåÂ∞èÂ≠©ÂÑø‰ª¨‰ªéËØ≠Ë®ÄÂ≠¶‰π†ÂºÄÂßãÂ∞±Ë¢´ÊïôÂØºÁùÄ‰∫∫ÂàÜÁ≠âÁ∫ßÔºå‰ª•ÂèäËØ•Â¶Ç‰ΩïÂéªÂà§Êñ≠ËØ¥ËØùÂèåÊñπÁöÑÁ≠âÁ∫ß„ÄÇËøôÂπ∂‰∏çËÆ©‰∫∫ÊÉäËÆ∂ÔºåÊØïÁ´üËØ≠Ë®ÄÂæÄÂæÄÊâøËΩΩÁùÄËøôÁ§æ‰ºöÁöÑÂéÜÂè≤„ÄÅ‰π†ÊÉØ‰∏éËÆ§Áü•ÔºåÂ≠¶‰π†ËØ≠Ë®Ä‰πüÂ∞±Êàê‰∫ÜÁªßÊâøËøô‰∫õ‰º†ÁªüÁöÑÊñπÂºè„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊúâÊÑèÊÄùÁöÑÊòØÔºå‰ΩúËÄÖËßÇÂØüÂà∞Á≠âÁ∫ßÈ´òÁöÑ‰∫∫Âπ∂‰∏çËÉΩ‰∏∫ÊâÄÊ¨≤‰∏∫Ôºå‰ªñ‰ª¨ÂèóÂà∞Â∞äÊï¨ÁöÑÂâçÊèêÊòØÊúâÂ∞ΩÂÖ∂ËÅåË¥£„ÄÇÂú®Ë∞àÂà∞ÂÆ∂ÈïøÂú®ÂÆ∂Â∫≠‰∏≠ÁöÑÂú∞‰ΩçÊó∂ÔºåÂ•πÁî®‰∫Ü‰∏Ä‰∏™ÂæàÂΩ¢Ë±°ÁöÑÊØîÂñªÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Êó•Êú¨‰∫∫‰ªéÂÆ∂Â∫≠ÁîüÊ¥ªÁªèÈ™å‰∏≠‰∫ÜËß£Âà∞Âè™ÊúâÁ¨¶ÂêàÂÆ∂Â∫≠ÂÖ±ÂêåÂà©ÁõäÁöÑÂÜ≥ÂÆöÊâçËÉΩÂæóÂà∞ËÆ§ÂèØÔºå‰ªª‰Ωï‰∫∫ÈÉΩ‰∏çËÉΩÂº∫Âà∂ÊâßË°åÔºåÂåÖÊã¨‰∏ÄÂÆ∂‰πãÈïøÔºåÊó•Êú¨ÁöÑÂÆ∂ÈïøÊõ¥ÂÉèÊòØÂÖ®ÂÆ∂‰∫∫ÁöÑÁâ©Ë¥®ÂíåÁ≤æÁ•ûË¥¢‰∫ßÁÆ°ÁêÜ‰∫∫„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;‰∏∫‰ªÄ‰πàÂú®ÂæàÂ§ö‰∫∫ÁúãÊù•Á≠âÁ∫ßÂà∂Êó†Â§Ñ‰∏çÂú®„ÄÅ‰∫∫ÁöÑË°å‰∏∫Ë¢´Êó†Êï∞Ê°ÜÊû∂Á∫¶ÊùüÁùÄÁöÑÊó•Êú¨Á§æ‰ºöËÉΩÈ´òÊïàËøê‰ΩúÔºåÂú®ÊàòÂêéÁªèÊµéËÖæÈ£ûÔºåÊàê‰∏∫‰∏∫Êï∞‰∏çÂ§öÁöÑ‰∫öÊ¥≤ÂèëËææÂõΩÂÆ∂Ôºü‰ΩúËÄÖÁü•ÈÅìÂØπ‰∫éÂ•πÁöÑÁæéÂõΩËØªËÄÖÊù•ËØ¥ÔºåËøôÂèØËÉΩÂæàÈöæÁêÜËß£ÔºåÂõ†‰∏∫ÁæéÂõΩÁ§æ‰ºöÁúã‰∏äÂéª‰ºº‰πéÊ≤°ÈÇ£‰πàÂ§öÁ≠âÁ∫ßÂ∑ÆÂºÇ‰πüÊõ¥Â¥áÂ∞ö‰∏™‰∫∫‰∏ª‰πâ„ÄÇÂ•πËÆ§‰∏∫ËøôÊòØÂõ†‰∏∫Êó•Êú¨‰∫∫‰π†ÊÉØ‚ÄúË°å‰∏∫ÂáÜÂàôË¢´ÁªÜÂåñ„ÄÅÂú∞‰ΩçË¢´ÊòéÁ°Æ‚ÄùÁöÑÁ§æ‰ºöÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ËôΩÁÑ∂ÁîüÊ¥ª‰∏≠Ë¶ÅÈÅµÂÆàÂêÑÁßçÁπÅÁêêÁöÑÁªÜÂàôÔºå‰ΩÜÂØπÊó•Êú¨‰∫∫Êù•ËØ¥ÔºåÂè™ÊúâËøôÊ†∑Ôºå‰ªñ‰ª¨ÁöÑÁîüÊ¥ªÊâçËÉΩÊúâÂ∫èËøõË°åÔºåÊâç‰ºöÊúâ‰∏ÄÁßçÂÆâÂÖ®ÊÑüÂíåÂΩíÂ±ûÊÑü„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ê≤°Âú®Êó•Êú¨ÁîüÊ¥ªËøá„ÄÅ‰∏çÊáÇÊó•ËØ≠‰πüÂπ∂ÈùûÊó•Êú¨Á†îÁ©∂Âá∫Ë∫´ÁöÑÈ≤ÅÊÄùÔºåËÉΩÂú®Áü≠Áü≠Âá†‰∏™Êúà‰πãÂÜÖÂ∞±Êäì‰Ωè‰∏Ä‰∏™Âèà‰∏Ä‰∏™Êó•Êú¨Á§æ‰ºöÁöÑÁã¨ÁâπÊ†∑Ë≤å„ÄÇÂØπËøô‰∏™ËÆ©ÁîüÊ¥ª‚ÄúÊúâÂ∫èËøõË°å‚ÄùÁöÑ‚ÄúÂÆâÂÖ®ÊÑüÂíåÂΩíÂ±ûÊÑü‚ÄùÊàë‰πüÊ∑±ÊúâÊÑüËß¶„ÄÇÂú®Êó•Êú¨ÁîüÊ¥ª‰∫Ü‰∏ÄÊÆµÊó∂Èó¥‰πãÂêéÊàëÂèëÁé∞Ôºå‰∏çÂêåÂ≤ó‰Ωç‰∏äÁöÑ‰∫∫‰ª¨ÁüúÁüú‰∏ö‰∏öÔºåËøõ‰æøÂà©Â∫óÊÄªÊúâÂ∫óÂëòÂØπ‰Ω†ÁöÑÊ¨¢ËøéÔºåÁÅ´ËΩ¶ÊÄªÊòØÂáÜÁÇπÔºåÊãâÈù¢Â∫óÈáåÁ©∫Èó¥Áã≠Â∞è‰ΩÜ‰Ω†ÊóÅËæπÁöÑÂ§ßÂèîÊÄª‰ºöÊï¥ÈΩêÊîæÂ•Ω‰ªñÁöÑËÉåÂåÖ‰ª•ÂÖçÁªô‰Ω†ÈÄ†Êàê‰∏ç‰æø„ÄÇË∫´Âú®ÂÖ∂‰∏≠ÔºåÊàëÊó∂‰∏çÊó∂‰ºöÊúâÁßçÈîôËßâÔºå‰ºö‰ª•‰∏∫ËøôÊòØ‰∏™Á≤æÂØÜÊó†ÊØîÁöÑÊú∫Âô®ÔºåËÄåÊØè‰∏™Èõ∂‰ª∂ÈÉΩÂú®ÂÖ®ÂäõÂ±•Ë°åÁùÄ‰ªñ‰ª¨ÁöÑËÅåË¥£ÔºåÂÉèËê®ÁâπÊâÄÊèèËø∞ÁöÑÂíñÂï°Â∫óÈáåÈÇ£‰∏™Â§™ÊÉ≥ÊääËá™Â∑±ÂèòÊàêÊúçÂä°ÂëòÁöÑ&lt;a href=&quot;https://existentialcomics.com/comic/101&quot;&gt;ÊúçÂä°Âëò&lt;/a&gt;„ÄÇ‰∏çËøáÔºåÊàëÁ°ÆÂÆûÊÑüËßâÂæóÂà∞ÂÆâÂÖ®ÊÑüÔºå‰∏áÁâ©‰ª•Â∑≤Áü•ÁöÑËßÑÂæãÂú®Â±ïÂºÄÔºõËÄåÂΩìÊàë‰πüÂºÄÂßãÈÅµÂÆàËøô‰∫õÁªÜÂàôÊó∂ÔºåÊàëÊÑüËßâÊàê‰∫ÜËøôÁ§æ‰ºöÁöÑ‰∏ÄÂëò„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;5-ÊòéÊ≤ªÊîøÂ∫ú&quot;&gt;5. ÊòéÊ≤ªÊîøÂ∫ú&lt;/h3&gt;

&lt;p&gt;ÂÄíÂπïÊàêÂäü‰πãÂêéÁªÑÂª∫Ëµ∑Êù•ÁöÑÊòéÊ≤ªÊîøÂ∫úÂÅöÂá∫ÁöÑÂêÑË°åÂêÑ‰∏öÁöÑÊîπÈù©ÂÖÖÂàÜ‰ΩìÁé∞‰∫ÜÊó•Êú¨‰∫∫ÂØπ‚ÄúÂêÑÂÆâÂÖ∂‰Ωç‚ÄùÁöÑÁ≠âÁ∫ßËßÇÂøµ„ÄÇÊñ∞ÊîøÂ∫úÂΩìÊó∂Ê¥æÂá∫‰ºäËó§ÂçöÊñáÂâçÂæÄÊ¨ßÊ¥≤ÂêÑÂõΩÂèñÁªèÔºåÊÉ≥Ë¶Å‰ªéÁ´ãÊ≥ïÁ≠âÂ±ÇÈù¢ÁùÄÊâãÂª∫Á´ãÊñ∞Êó•Êú¨„ÄÇÂú®ÂΩìÊó∂ÁöÑËã±ÂõΩÂçöÂ≠¶ÂÆ∂ÊñØÂÆæÂ°û(Herbert Spencer)‰∏é‰ºäËó§ÂçöÊñáÁöÑ‰ø°‰ª∂Êù•ÂæÄ‰∏≠Ôºå‰ªñÊèêÂà∞&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;‚Ä¶Êó•Êú¨‰∫∫ÂØπÈïøËÄÖÁöÑ‰º†Áªü‰πâÂä°‰ª•ÂèäÂØπÂ§©ÁöáÁöÑ‰πâÂä°ÈÉΩÊòØÊó•Êú¨Áã¨ÁâπÁöÑÊñáÂåñ„ÄÇÊó•Êú¨Â∞ÜÂú®‚ÄúÈïøËÄÖ‚ÄùÁöÑÂ∏¶È¢Ü‰∏ãÈÅøÂºÄÈÇ£‰∫õÂ¥áÂ∞ö‰∏™‰∫∫‰∏ª‰πâÂõΩÂÆ∂ÊâÄÈù¢‰∏¥ÁöÑ‰∏Ä‰∫õ‰∏çÂèØÈÅøÂÖçÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÂØπÈïøËÄÖÁöÑ‰πâÂä°ÊàñËÆ∏‰∏çÊòØÊó•Êú¨Áã¨ÊúâÔºå‰ΩÜÂØπÂ§©ÁöáÁöÑ‰πâÂä°Á°ÆÂÆûÂ¶ÇÊ≠§„ÄÇSpencerÊòØ19‰∏ñÁ∫™Êú´ÊúÄÊúâÂΩ±ÂìçÂäõÁöÑËã±ÂõΩÂçöÂ≠¶ÂÆ∂Ôºå‰ªñÈ¶ñÂÖàÊèêÂá∫‰∫Ü‚ÄúÈÄÇËÄÖÁîüÂ≠ò(Survival of the fittest)‚ÄùÁöÑÊ¶ÇÂøµÔºåÂêéÊúü‰πü‰ª•ÊîØÊåÅÁ§æ‰ºöËææÂ∞îÊñá‰∏ª‰πâ‚ÄúÂá∫Âêç‚Äù„ÄÇËôΩÁÑ∂Âú®20‰∏ñÁ∫™‰ª•Êù•‰ªñÂ∑≤ÁªèÂ§±Âéª‰∫ÜÂú®Ë•øÊñπ‰∏ñÁïåÊõæÁªèÁöÑÂ≠¶ÊúØÂú∞‰ΩçÔºå‰ΩÜ‰ªñÂú®Êó•Êú¨‰∏é‰∏≠ÂõΩ‰ªçÁÑ∂ÊúâÁùÄÂæàÂ§ßÁöÑÂΩ±Âìç„ÄÇÊòéÊ≤ªÊñ∞ÊîøÂ∫ú‰πüÂæàÊª°ÊÑèSpencerÁöÑÂõû‰ø°ÔºåËøôÊ†∑‰ªñ‰ª¨Â∞±ËÉΩÂ∞ÜÂ§©ÁöáÁ≠âÂèçÊò†Á≠âÁ∫ßÂà∂ÁöÑÁ§æ‰ºöÂÖÉÁ¥†‰øùÁïô‰∏ãÊù•„ÄÇÂú®Êñ∞ËÆÆ‰ºöÊîøÊ≤ª‰∏≠ÔºåÂ§©ÁöáÂèØ‰ª•‰ªªÂëΩÈ´òÂ±Ç‰∫∫Áâ©Ôºå‰∏äËÆÆÈô¢ÁöÑÂΩ±ÂìçÂäõÂàôËøúÈ´ò‰∫é‰∏ãËÆÆÈô¢„ÄÇÂú®ÂÆóÊïô‰∏äÔºåÁ•ûÈÅì‰Ωú‰∏∫ÂõΩÊ∞ë‰ø°‰ª∞Â∑≤ÁÑ∂ÊòØË∂ÖË∂äÂÆóÊïôÁöÑÂ≠òÂú®ÔºåÂÖ∂Á•ûËÅå‰∫∫ÂëòÊúâÁùÄ‚ÄúË¥µÊóè‚ÄùÁöÑÂú∞‰ΩçÔºåËÄåÂÖ∂‰ªñÂÆóÊïô‰∫∫ÂëòÂàôÊ≤°Êúâ„ÄÇ&lt;/p&gt;

&lt;p&gt;ËÄåÂè¶Â§ñ‰∏Ä‰∏™Êñ∞ÊîøÂ∫ú‰∏ãÂá∫Áé∞ÁöÑÂèòÈù©ÊòØÂÜõÈÉ®ÁöÑÁã¨Á´ã„ÄÇÂΩìÊó∂ÁöÑÂÆ™Ê≥ïËôΩÁÑ∂Ê≤°ÊúâÊòéÊñáËßÑÂÆöÔºå‰ΩÜÊòØÂÜõÈÉ®ÂèØ‰ª•Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÁã¨Á´ã‰∫éÂÜÖÈòÅÂÅöÂÜ≥ÂÆöÁîöËá≥Áõ¥Êé•ÂΩ±ÂìçÂÜÖÈòÅÔºåÊØîÂ¶Ç‰ªñ‰ª¨ÂèØ‰ª•‚ÄúÊãíÁªùÂßîÊ¥æÈôÜÊµ∑ÂÜõÂ∞ÜÈ¢ÜÊãÖ‰ªªÂÜÖÈòÅ‰∏≠ÁöÑËÅåÂä°‚Äù„ÄÇ&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;‚Ä¶‰∫∫‰ª¨ÂØπÂÜõÈÉ®ÁöÑÂÅöÊ≥ïÈÄöÂ∏∏ÈááÂèñÊé•ÂèóÊúç‰ªéÁöÑÊÄÅÂ∫¶„ÄÇÂπ∂‰∏çÊòØÂõ†‰∏∫‰ªñ‰ª¨ËµûÂêåËøô‰∫õÂÅöÊ≥ïÔºåËÄåÊòØ‰ªñ‰ª¨Êó©Â∑≤‰π†ÊÉØ‰∫ÜËøôÁßçÁ≠âÁ∫ßÂÖ≥Á≥ªÔºå‰πü‰∏çËµûÊàêÈÄæË∂äÁâπÊùÉÁïåÈôê„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ËÄåÊ∞ë‰ºóÂØπÂÜõÈÉ®‰πüÊòØÂ∞äÊï¨ÁöÑÔºåËøôÊàñËÆ∏‰∏éÊó©ÊúüÂÜõ‰∫∫ËÆ∏Â§öÊõæÊòØÊ≠¶Â£´ÊúâÂÖ≥Ôºå‰πü‰∏é‰ªñ‰ª¨ÂÆ£Áß∞ÊòØÂ§©ÁöáÁöÑÈÉ®ÈòüÊúâÂÖ≥„ÄÇÊÄª‰πãÔºåËøôÁßç‰∫∫Ê∞ëÁöÑÊîØÊåÅÂíåÂÜÖÈòÅÂØπÂÜõÈÉ®ÊúâÈôêÁöÑÁâµÂà∂ËÉΩÂäõ‰∏∫‰πãÂêé‰∫åÊàòÊó∂ÊúüÊó•Êú¨ÂèòÊàêÂÜõÂõΩ‰∏ª‰πâÂõΩÂÆ∂Âüã‰∏ã‰∫Ü‰ºèÁ¨î„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;6-ÂèóÊÅ©‰∏éËøòÂÄ∫&quot;&gt;6. ÂèóÊÅ©‰∏éËøòÂÄ∫&lt;/h3&gt;

&lt;p&gt;Êó•Êú¨ÊòØ‰∏™ÈùûÂ∏∏Âº∫Ë∞É‰∏™‰∫∫‰πâÂä°ÂíåË¥£‰ªªÁöÑÁ§æ‰ºöÔºåËøô‰πüÊòØÊàëÂéªÂà∞ÈÇ£Ëæπ‰πãÂêéËÉΩÂæàÂø´ÊÑüÂà∞Ëá™Â¶ÇÁöÑÂéüÂõ†„ÄÇËÄåÂú®ËøôÊ†∑ÁöÑÁéØÂ¢ÉÈáåÔºå‰ΩúËÄÖËßÇÂØüÂà∞Ôºå‚ÄúÁà±‚ÄùË∑ü‰πâÂä°Áîª‰∏ä‰∫ÜÁ≠âÂè∑Ôºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;‰ªéÊüêÁßçÊÑè‰πâ‰∏äËØ¥ÔºåÊó•Êú¨‰∫∫ËÆ§‰∏∫‚ÄúÁà±‚ÄùÂÖ∂ÂÆûÂ∞±ÊòØ‰∏ÄÁßçÂ∫îÂ∞ΩÁöÑ‰πâÂä°Ôºå‰∏ÄÁßçÂØπÊÅ©ÊÉÖÁöÑÂõûÊä•„ÄÇËÄåÁæéÂõΩ‰∫∫Âç¥ËÆ§‰∏∫‚ÄúÁà±‚ÄùÊòØ‰∏ÄÁßçÁúüÊÉÖÁöÑÊµÅÈú≤ÔºåÊòØËá™ÊÑøÁªô‰∫àÁöÑ„ÄÅ‰∏çÊ±ÇÂõûÊä•ÁöÑ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ËôΩÁÑ∂Ëøô‰∏§ÁßçÁêÜËß£Áúã‰ººÂ§ÑÂú®Â§©Âπ≥ÁöÑ‰∏§Á´ØÔºå‰ΩÜÊàëËÆ§‰∏∫‚ÄúÁà±‚ÄùÊòØÂåÖÊã¨‰∫ÜË¥£‰ªª‰∏éÊÉÖÊÑü„ÄÇ‰∏Ä‰∏™Êõ¥ÂáÜÁ°ÆÁöÑËØ¥Ê≥ïÂèØËÉΩÊòØÔºåÊó•Êú¨Á§æ‰ºöÊõ¥Âº∫Ë∞É‚ÄúÁà±‚ÄùÂΩì‰∏≠ÁöÑË¥£‰ªªÔºåËÄåÁæéÂõΩÂàôÊõ¥Âº∫Ë∞ÉÊÉÖÊÑü„ÄÇÊú¨‰π¶ÁöÑÁ†îÁ©∂Êù•Ëá™‰∫é‰ΩúËÄÖÂú®ÂÖ´ÂçÅÂπ¥ÂâçÂØπÂΩìÊó∂ÁöÑËµÑÊñô‰ª•ÂèäÊõ¥Êó©ÁöÑÂéÜÂè≤ÁöÑËÄÉÂØüÔºåËøô‰∫õËßÇÂØü‰∏ç‰∏ÄÂÆö‰πüÈÄÇÁî®‰∫éÁé∞Âú®ÁöÑÊó•Êú¨„ÄÇÊÄªÁöÑÊù•ËØ¥Ôºå‰∫∫‰ºöÂèóË∫´Â§ÑÂ§ßÁéØÂ¢ÉÁöÑÂΩ±ÂìçÔºåÂ∞§ÂÖ∂Âú®ÊàêÈïøÊúüÔºåÊó•Êú¨Á§æ‰ºöÂÖªËÇ≤Âá∫ÁúãÈáç‰∫≤ÂØÜÂÖ≥Á≥ª‰∏≠Ë¥£‰ªªÁöÑ‰∫∫Ôºå‰ΩÜÂêåÊó∂‰πü‰ºöËÆ©ÂÖ∂‰ªñ‰∏çÈÄÇÂ∫îËÄÖÊÉ≥Ë¶ÅÈÄÉÁ¶ªÂêß„ÄÇ&lt;/p&gt;

&lt;p&gt;Â¶ÇÊûúËØ¥Êù•Ëá™‰∫é‚ÄúÁà±‚ÄùÁöÑ‰πâÂä°Ê≤âÈáç‰ΩÜÊ≤°ÈÇ£‰πàÈ¢ëÁπÅÔºåÈÇ£‚ÄúÈôåÁîü‰∫∫ÁöÑÊÅ©ÊÉ†‚ÄùÂàôÁõ∏Âèç„ÄÇËøôÊ†∑ÁöÑÊÅ©ÊÉ†‰ºöÊó∂Â∏∏ÂèëÁîüÔºåËÄåÂú®ÂæàÂ§öÊó•Êú¨‰∫∫ÂøÉ‰∏≠ÔºåÊé•Âèó‰∫ÜÂà´‰∫∫ÁöÑÊÅ©ÊÉ†Â∞±ÂøÖÈ°ªË¶ÅÂõûÊä•ÔºåÊâÄ‰ª•‰ΩúËÄÖÂèëÁé∞‰ªñ‰ª¨‰ºöÂ∏∏‰∏∫Ê≠§Á±ªÊÅ©ÊÉ†ÊÑüÂà∞ÁÉ¶ÊÅº„ÄÇÊØîÂ¶ÇËØ¥ÔºåÊó•ËØ≠ÈáåÊúâËÆ∏Â§öË°®Á§∫ÊÑüË∞¢ÁöÑËØçËØ≠ÔºåÂΩì‰∏≠ÂæàÂ§öÊòØÁî®Êù•Ë°®ËææÊé•ÂèóÂ∏ÆÂä©Êó∂ÁöÑ‚Äú‰∏çÂÆâÂøÉÊÉÖ‚Äù„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂàöÂéªÊó•Êú¨Ê≤°Â§ö‰πÖÁöÑÊó∂ÂÄôÔºåÊúâ‰∏™ËØçËÆ©ÊàëÊúâÁÇπÊë∏‰∏çÁùÄÂ§¥ËÑë - „Åô„Åø„Åæ„Åõ„ÇìÔºåÂ≠óÈù¢ÊÑèÊÄùÊòØ‚ÄúÊä±Ê≠â‚ÄùÔºåÂÉèÊòØËã±ÊñáÁöÑ‚Äúexecus me‚ÄùÔºåËÄå‚ÄúË∞¢Ë∞¢‚ÄùÁöÑÊó•ËØ≠Áõ¥ËØëÊòØ‚Äú„ÅÇ„Çä„Åå„Å®„ÅÜ‚Äù„ÄÇÊàëÂèëÁé∞Â§ßÂÆ∂Áî®Ëøô‰∏§‰∏™ËØçÁöÑ‰π†ÊÉØË∑üÊàëÂæà‰∏ç‰∏ÄÊ†∑„ÄÇÊØîÂ¶ÇËØ¥ÔºåÂú®Ë∑Ø‰∏äÊç°‰∫ÜÂà´‰∫∫ÊéâÁöÑÊØõÂ∑æËøòÁªô‰ªñÔºå‰ªñ‰∏ç‰ºöËØ¥‚Äú„ÅÇ„Çä„Åå„Å®„ÅÜÔºàË∞¢Ë∞¢Ôºâ‚ÄùÔºåËÄåÊòØËØ¥‚Äú„Åô„Åø„Åæ„Åõ„ÇìÔºàÊä±Ê≠âÔºâ‚ÄùÔºõÂÖ¨‰∫§ËΩ¶ÁâπÊÑèÂ§öÁ≠â‰∫Ü‰ºöÂÑø‰πãÂêé‰∏äÊù•ÁöÑ‰πòÂÆ¢ÂØπÂè∏Êú∫ËØ¥ÁöÑ‰∏çÊòØ‚Äú„ÅÇ„Çä„Åå„Å®„ÅÜÔºàË∞¢Ë∞¢Ôºâ‚ÄùËÄåÊòØ‚Äú„Åô„Åø„Åæ„Åõ„ÇìÔºàÊä±Ê≠âÔºâ‚Äù„ÄÇÂêéÊù•ÊàëÊòéÁôΩ‰∫ÜÔºåÊâøÂèó‰∫ÜÂà´‰∫∫ÊÅ©ÊÉÖÔºàÂ∞§ÂÖ∂ÊòØÈôåÁîü‰∫∫ÔºâÁöÑÊó∂ÂÄôÔºåÂ§ßÂÆ∂‰π†ÊÉØÂØπÂà´‰∫∫Ë°®Á§∫Êä±Ê≠âÔºåÊù•Ë°®ËææËá™Â∑±ÂèóÂà∞ÊÅ©ÊÉÖËÄåÂèàÂèØËÉΩÊó†Ê≥ïÂÅøËøòÁöÑ‰∏çÂÆâÂøÉÊÉÖ„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;7-Á§æ‰ºöÁöÑÂéãÂäõ&quot;&gt;7. Á§æ‰ºöÁöÑÂéãÂäõ&lt;/h3&gt;

&lt;p&gt;Â§ßÂÆ∂ÈÉΩÁü•ÈÅìÈ´òËá™ÊùÄÁéáÊòØÊó•Êú¨‰∏ÄÂ§ßÁ§æ‰ºöÈóÆÈ¢òÔºå‰∏çÂ∞ëÊó•Êú¨ÂõΩÂÜÖÂõΩÂ§ñÁöÑÂ≠¶ËÄÖ„ÄÅËÆ∞ËÄÖ„ÄÅÂÖ¨Ê∞ëÂõ¢‰ΩìÁ≠âÈÉΩËá¥Âäõ‰∫éËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇÂú®‰∏Ä‰∏™Âπ≥ÂùáÂØøÂëΩÊúÄÈïø„ÄÅÂåªÁñó‰ΩìÁ≥ªÂèëËææ„ÄÅÂ±±Ê∏ÖÊ∞¥ÁßÄÁöÑÂèëËææÂõΩÂÆ∂Ôºå‰∏∫‰ªÄ‰πàÊúâÈÇ£‰πàÂ§ö‰∫∫ÊÉ≥Ëá™ÊùÄÔºüËøôÊú¨‰π¶ÁöÑÂêéÂá†Á´†ÊàëËßâÂæó‰πüÁªôÂá∫‰∫Ü‰∏Ä‰∫õÁ≠îÊ°à„ÄÇ&lt;/p&gt;

&lt;p&gt;Á¨¨‰∏Ä‰∏™Âõ†Á¥†Â∞±Ë∑üÊä•Á≠îÊÅ©ÊÉÖÊúâÂÖ≥„ÄÇ‰ΩúËÄÖÂèëÁé∞ÔºåÊó•Êú¨‰∫∫ÊääÊÅ©ÊÉÖÂàÜ‰∏∫‰∏§Á±ªÔºåÁ¨¨‰∏ÄÁ±ªÊòØÊó†Ê≥ïËøòÊ∏ÖÁöÑÊÅ©ÊÉÖÔºöÂø†‰∏éÂ≠ùÔºõÁ¨¨‰∫åÁ±ªÊòØÈúÄË¶ÅÂú®‰∏ÄÂÆöÊó∂Èó¥‰πãÂÜÖÂÅøËøòÁöÑÔºöÊ¨†‰∏ñ‰∫∫ÁöÑÊÉÖ‰πâ‰∏éÂØπËá™Â∑±ÂêçË™âÁöÑÁª¥Êä§„ÄÇÊó•Êú¨Á§æ‰ºöÊää‰∫∫‰ª¨ÊòØÂê¶Â∞ΩÂäõÂÅøËøòÊÅ©ÊÉÖÁúãÂæóÂæàÈáçÔºåÊúâÁöÑ‰∫∫ÁîöËá≥Êää‚Äú‰∏çËÉΩÊä•Á≠îÊÉÖ‰πâÁöÑ‰∫∫ËßÜ‰∏∫ÊòØ‰∫∫Ê†º‰∏äÁ†¥‰∫ß‚Äù„ÄÇËôΩÁÑ∂ËøôÁ±ªÁ§æ‰ºöÂÖ≥Á≥ª‰∏éÊúüÂæÖÂ∏¶Êù•ÁöÑÂøÉÁêÜË¥üÊãÖ‰∏çÊòØÊó•Êú¨Á§æ‰ºö‰ªÖÊúâÔºå‰ΩÜ‰ªñ‰ª¨ÁöÑËøáÂàÜÂº∫Ë∞É‰ºö‰ΩøÂæóÊôÆÈÄö‰∫∫Êó†Êó∂Êó†Âàª‰∏çÂ§ÑÂú®Â∑®Â§ßÁöÑÂéãÂäõ‰πã‰∏ãÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ÊÄª‰πãÔºå‰∫∫‰ª¨Âú®Â±•Ë°å‚ÄúÊÉÖ‰πâ‚ÄùÊó∂ÊÄªÊòØÊÑüÂà∞Âæà‰∏∫ÈöæÔºåÊÑüÂà∞‰∏çÊÉÖÊÑøÔºåÂõ†‰∏∫‚ÄúÊÉÖ‰πâ‚ÄùÊàê‰∏∫‰∫∫‰ª¨Êó•Â∏∏‰∫§ÂæÄ‰∏≠ÊâÄËÉåË¥üÁöÑÊ≤âÈáçÁöÑÂøÉÁêÜË¥üÊãÖ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Âè¶Â§ñ‰∏Ä‰∏™Âõ†Á¥†ÂàôÊòØÊó•Êú¨‰∫∫ÂØπÁª¥Êä§Ëá™Â∑±ÂêçË™âÁöÑËøΩÊ±Ç„ÄÇ‰ΩúËÄÖÂú®ËÆ∏Â§öÂéÜÂè≤ÊñáÁåÆ‰∏éÂΩ±ËßÜ‰ΩúÂìÅ‰∏≠ÂèëÁé∞ÔºåÂ∞ΩÂÖ∂ÊâÄËÉΩÂéªÊæÑÊ∏ÖË∞£Ë®Ä„ÄÅÁª¥Êä§ÂêçË™âÁöÑ‰∫ãÊÉÖÊòØÈ´òÂ∞öÁöÑÔºåÊòØ‰∏ÄÁßçË¢´Êé®Â¥áÁöÑÁæéÂæ∑„ÄÇËÄåÂêåÊó∂ÔºåÊó•Êú¨‰∫∫Áúã‰∏äÂéª‰π†ÊÉØÊääÊüê‰∫∫ÊâÄ‰ªé‰∫ãÁöÑÂ∑•‰Ωú‰∏é‰ªñÊú¨‰∫∫Á¥ßÂØÜËÅîÁ≥ªËµ∑Êù•ÔºåÂØπ‰ªñÂ∑•‰ΩúÁöÑËØÑ‰ª∑Êó¢ÊòØÂØπ‰ªñÊú¨‰∫∫ÁöÑËØÑ‰ª∑„ÄÇÊâÄ‰ª•ÂèØ‰ª•ÊÉ≥Ë±°ÔºåÂΩìÊàëÁöÑÂ∑•‰ΩúÂèóÂà∞ÊâπËØÑÔºåÊàë‰ºöËßâÂæóËøôÊòØÂØπÊàëËøô‰∏™‰∫∫Êú¨Ë∫´ÁöÑÊâπËØÑÔºå‰ªéËÄå‰∏Ä‰∏™Ê≠£Â∏∏ÁöÑÂ§±ËØØÂèØËÉΩÂ∞±ÂèòÊàê‰∫ÜÊàëË¥®ÁñëËá™Â∑±ÁîüÂëΩ‰ª∑ÂÄºÁöÑÁÅµÈ≠ÇÊã∑ÈóÆÔºåËøôÊ†∑ÁöÑÁîüÊ¥ªËÉΩ‰∏çÂéãÂäõÂ§ßÂêóÔºü&lt;/p&gt;

&lt;p&gt;ËÄåËøô‰∫õÂéãÂäõ‰ºöË¢´Âè¶Â§ñ‰∏Ä‰∏™ÂÖÉÁ¥†Âä†ÂâßÔºöÁæûËÄªÊÑü„ÄÇÂú®Êó•Êú¨Á§æ‰ºöÈïøÂ§ßÁöÑÂ≠©Â≠êÂæÄÂæÄÁæûËÄªÂøÉ‰ºöÊØîËæÉÈáçÔºå‰ΩúËÄÖÂèëÁé∞&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Êó•Êú¨‰∫∫‰πãÊâÄ‰ª•ÊääË∞®ÊÖé‰∏éËá™ÈáçÂàíÁ≠âÂè∑ÔºåÊòØÂõ†‰∏∫‰ªñ‰ª¨ÊÄªÊòØÊÑüËßâÂà´‰∫∫Âú®Ê≥®ËßÜËØÑ‰ª∑Ëá™Â∑±„ÄÇ‚Ä¶Âõ†Ê≠§ÔºåËøôÁßç‰ª•ÁæûËÄªÊÑü‰∏∫Âü∫Á°ÄÁöÑÊñáÂåñÊõ¥ÁúãÈáçÂà´‰∫∫ÁöÑËØÑ‰ª∑„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÈöèÊó∂ÈÉΩÂú®ÊÉ≥ÁùÄËá™Â∑±Âú®Âà´‰∫∫Áúº‰∏≠ÁöÑÊ†∑Â≠êÔºå‰∏ÄÁõ¥ÂÅö‰∏Ä‰∏™‚ÄúË¢´Ê≥®ËßÜ‚ÄùÁöÑ‰∫∫ÔºåËôΩÁÑ∂ËøôÁßçÊù•Ëá™‰ªñ‰∫∫ÁöÑÂéüÂàù‰ΩìÈ™åÂú®Ëê®ÁâπÁúãÊù•ÊòØÊàë‰ª¨‰∏éËøô‰∏™‰∏ñÁïå‰∫§ÊµÅÊâÄÂøÖ‰∏çÂèØÂ∞ëÁöÑÔºå‰ΩÜÂ¶ÇÊûú‰ΩìÈ™åÂè™Êúâ‚ÄúË¢´Ê≥®ËßÜ‚ÄùÔºåÊÄªÊòØÊãÖÂøÉËá™Â∑±ÁöÑË°å‰∏∫ÊòØÂê¶‰ºöÁªôËá™Â∑±Â∏¶Êù•Ë¥üÈù¢ËØÑ‰ª∑ÔºåËøôÊ†∑ÁöÑÁîüÊ¥ªËÇØÂÆöÂæàÁ¥ØÂêß„ÄÇ‰∏Ä‰∏™‰æãÂ≠êÂ∞±ÊòØÊòéÊòüÊâÄÈù¢ÂØπÁöÑ24Â∞èÊó∂360Â∫¶ÂÖ®Ê∞ëËØÑ‰ª∑ÔºåÂ∞±Âú®‰ªäÂπ¥‰∫îÊúàÔºåÊó•Êú¨ÁöÑÊëîË∑§ÊâãÊú®ÊùëËä±Âú®ÂèÇÂä†‰∫Ü‰∏ÄÊ°£ÈùûÂ∏∏Âá∫ÂêçÁöÑÁúü‰∫∫ÁßÄ(Terrace House)‰πãÂêéÔºåÂèóÂà∞‰∫ÜËøûÁª≠Âá†‰∏™ÊúàÁöÑÁΩëÁªúÊö¥ÂäõÊó†Ê≥ïÊâøÂèóËÄå&lt;a href=&quot;https://www.nytimes.com/2020/07/17/arts/television/terrace-house-suicide.html&quot;&gt;ÁªìÊùü‰∫ÜËá™Â∑±ÁöÑÁîüÂëΩ&lt;/a&gt;„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;8-ÁªìËØ≠&quot;&gt;8. ÁªìËØ≠&lt;/h3&gt;

&lt;p&gt;‰∫åÊàòÊàòË¥•‰πãÂêéÔºåÊó•Êú¨‰∫∫ÂºÄÂßã‰∫ÜÂØπËá™Â∑±Ë∫´‰ªΩÁöÑÊêúÂØªÔºåËÄå„ÄäËèä‰∏éÂàÄ„Äã‰Ωú‰∏∫‰∏ÄÊú¨ÁæéÂõΩÂ≠¶ËÄÖÁöÑËëó‰ΩúÂàöÂ•ΩËÉΩËÆ©Êó•Êú¨‰∫∫‰ªé‰∏≠ÂéªÊêúÂØªËá™Â∑±‰∏éÁæéÂõΩ‰∫∫ÁöÑÂå∫Âà´ÔºåÂ¶Ç‰ΩïÂú®ÁæéÂõΩÊîøÂ∫úÁöÑ‰ªãÂÖ•‰∏ãÈáçÂª∫Êó•Êú¨„ÄÇÂ≠¶ÁïåÂØπÊú¨‰π¶ÁöÑÊâπËØÑ‰ªéÊù•Ê≤°ÊúâÊ∂àÂ§±ËøáÔºå‰∏ªË¶ÅÊâπËØÑÁöÑÁÇπÊúâ‰∏â‰∏™Ôºö‰π¶‰∏≠ÂæàÂ§öË¢´ÂÜ†‰ª•Êó•Êú¨Ê∞ëÊóèÊÄßÁöÑÁâπÂæÅÂÆûÈôÖ‰∏äÊòØÂü∫‰∫éÂÜõÂõΩ‰∏ª‰πâÊó∂ÊúüÁöÑÂÜõÈòüÔºåËÄåÂü∫‰∫éÂÜõ‰∫∫ÁöÑÁ†îÁ©∂Âπ∂‰∏çËÉΩ‰ª£Ë°®ÊôÆÈÄöÊó•Êú¨‰∫∫Ôºõ‰∏çËøáËøôÊ∫êËá™‰∫éÊú¨‰π¶ÈááÁî®ÁöÑÊñπÊ≥ïÁöÑÈóÆÈ¢òÔºåÈ≤ÅÊÄù‰ªéÂ§¥Âà∞Â∞æÈÉΩÊääÊó•Êú¨Ê∞ëÊóèÂΩìÂÅö‰∫ÜÊï¥ÈΩêÁöÑ‰∏Ä‰ΩìÊù•ÊèèËø∞ÔºåÁÑ∂ËÄåÂêå‰∏ÄÊ∞ëÊóè‰πü‰ºöÊúâÂ§öÊ†∑ÊÄßÔºåÊú¨‰π¶Ê≤°ÊúâÁÖßÈ°æÂà∞Ëøô‰∏ÄÊñπÈù¢ÔºõÊúÄÂêé‰∏Ä‰∏™ÊòØÈ≤ÅÊÄù‰π¶‰∏≠ÁöÑÂçï‰∏Ä‰∏çÂèòÁöÑÊ∞ëÊóèÊÄßÔºåÂØπÊó∂Èó¥ÁöÑÂÖÉÁ¥†‰∫éÊ∞ëÊóèÁâπÂæÅÁöÑÂΩ±ÂìçÊ≤°ÊúâËÆ®ËÆ∫„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏çËøáÔºåÂ∞±ÂÉèÂºÄÂ§¥ÊâÄËØ¥ÔºåÊú¨‰π¶ÊúâËÆæÁ´ãÂÅáÂÆöÔºåÊúâ‰ΩúËÄÖ‰Ωú‰∏∫ÁæéÂõΩ‰∫∫Á±ªÂ≠¶ÂÆ∂ÁöÑËßÜËßíÈôêÂà∂Ôºå‰πüÊúâÂ•π‰∏∫‰∫ÜÂÜôÊàê‰∏ÄÊú¨‰π¶ËÄåÂØπÂÜÖÂÆπÊâÄÂÅöÂá∫ÁöÑÂèñËàçÔºå„ÄäËèä‰∏éÂàÄ„ÄãÂ∏¶Ëµ∑Êù•ÁöÑÂØπÊó•Êú¨Á§æ‰ºö„ÄÅÊó•Êú¨‰∫∫ÁöÑÁ†îÁ©∂ÁÉ≠ÊΩÆÊòØËøôÊú¨‰π¶ÂÆûÂÆûÂú®Âú®ÁöÑË¥°ÁåÆÔºåÂêéÊù•ÁöÑÁ†îÁ©∂‰∏≠ÊúâÂéªÂØπÂ•πÁöÑËßÇÁÇπÂÅöËøõ‰∏ÄÊ≠•ÈòêËø∞ÁöÑÔºå‰πüÊúâÂÅöÊúâÁêÜÊúâÊçÆÂèçÈ©≥ÁöÑÔºå‰ΩÜÊú¨‰π¶‰∏çÊñ≠Ë¢´ÂºïÁî®Ôºå‰πüÂú®‰∏≠ÂõΩÂíåÊó•Êú¨ÁªßÁª≠ÁÉ≠ÈîÄÔºåÂèØËÉΩÊúâÂæàÂ§öË∑üÊàë‰∏ÄÊ†∑ÁöÑÂπ¥ËΩªÊúãÂèãÔºåËØªÂà∞ÁöÑÁ¨¨‰∏ÄÊú¨Èùû‰∏≠ÂõΩËßÜËßíÁöÑÂÖ≥‰∫éÊó•Êú¨ÁöÑ‰π¶Á±çÂ∞±ÊòØ„ÄäËèä‰∏éÂàÄ„ÄãÔºåËøôÁßçÁªè‰πÖ‰∏çË°∞ÁöÑÂΩ±ÂìçÂäõÁùÄÂÆûÂæàËÆ©‰∫∫ÊÉäÂèπ‰∫Ü„ÄÇ&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>2020Âπ¥6ÊúàÔºöÂÉèÂÅötalk show‰∏ÄÊ†∑ÂéªÂÅöÂ≠¶ÊúØÊä•Âëä</title>
   <link href="http://localhost:4000/journal/2020/07/10/journal-2020-06.html"/>
   <updated>2020-07-10T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/07/10/journal-2020-06</id>
   <content type="html">&lt;p&gt;6ÊúàÁöÑÊñ∞Âä†Âù°ÁªìÊùü‰∫ÜÂ∞ÅÂüéÔºåËÄåÊàë‰πüÂú®Ëøô‰∏™ÊúàÁöÑÊúÄÂêé‰∏ÄÂ§©Êî∂Âà∞‰∫Ü&lt;a href=&quot;https://www.researchgate.net/publication/342652506_A_Control_Chart_Approach_to_Power_System_Line_Outage_Detection_Under_Transient_Dynamics&quot;&gt;ÊñáÁ´†&lt;/a&gt;Ë¢´Êé•Êî∂ÁöÑÈÇÆ‰ª∂„ÄÇ&lt;/p&gt;

&lt;p&gt;Âçö‰∏ÄÂºÄÂßãÂ∞±Âú®‰∏é‰∏çÂêåËÄÅÂ∏àÁöÑ‰∫§ÊµÅ‰∏≠‰∫ÜËß£Âà∞ÔºåÁßëÁ†î‰∏éÂèëË°®‰∏çÊòØÁÆÄÂçïÁöÑÂõ†ÊûúÂÖ≥Á≥ªÔºöÂÅö‰∫ÜÂéâÂÆ≥ÁöÑÁßëÁ†îÂ∑•‰ΩúÂ∞±ËÉΩÂú®È°∂Á∫ßÊúüÂàä‰∏äÂèëË°®Á†îÁ©∂ÁªìÊûúÔºåËøô‰ø©Êõ¥ÂÉèÊòØÊúâ‰∏ÄÂÆöÁõ∏ÂÖ≥ÊÄß‰ΩÜ‰πüÂàÜÂà´Áã¨Á´ã‰∫éÂΩºÊ≠§ÁöÑ‰∫ã‰ª∂ÔºåÊúÄÁÆÄÂçïÁöÑ‰æãÂ≠êÂ∞±ÊòØÊàëÂèØËÉΩÂèØ‰ª•ÊéßÂà∂Êüê‰∏™Â∑•‰ΩúÊäïÂÖ•ÁöÑÊó∂Èó¥Ôºå‰ΩÜÊó†Ê≥ïÊéßÂà∂Ë∞ÅÊòØÂÆ°Á®ø‰∫∫„ÄÅÂÆ°Á®ø‰∫∫ÁöÑÂ≠¶ÊúØÂìÅÂë≥Á≠âÁ≠â„ÄÇÂú®ÊàëÂæàÂ∞èÁöÑÂçöÂ£´ÁîüÂúàÂ≠êÈáåÔºå‰∏ÄÁØáÊñáÁ´†Ë¢´‰øÆÊîπ‰∏§‰∏âÊ¨°„ÄÅÂõûÂ§çÂÆ°Á®ø‰∫∫ÊÑèËßÅÁöÑ‰ø°‰ª∂ÊØîÂéüÊñáÁ´†ËøòË¶ÅÈïø„ÄÅ‰øÆÊîπÊúüÈó¥ÊâÄÂÅöÁöÑÂ∑•‰ΩúÈáè‰∏çÊØîÂéüÊñáÂ∞ëÁ≠âËøô‰∏ÄÁ±ªÁöÑÁªèÂéÜ‰πüÊòØÊ≤°Â∞ëÂê¨ËØ¥ÔºåÊâÄ‰ª•ÂΩìÊàëÁöÑÁ¨¨‰∏ÄÁØáÊñáÁ´†Âú®ÁªèËøá‰∏ÄËΩÆ‰øÆÊîπ‰πãÂêé‰æøË¢´Êé•Êî∂Êó∂ÔºåÂøÉÈáåËôΩÁÑ∂ÂæàÈ´òÂÖ¥Ôºå‰ΩÜÊÄªËßâÂæóËøôÊ¨°Â§öÂ∞ë‰πüËµ∞‰∫Ü‰∫õËøêÂêß„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏çËøáËØùËØ¥ÂõûÊù•Ôºågive credit when it‚Äôs dueÔºåÊñáÁ´†Ë¢´Êé•Êî∂ÊòØ‰∏Ä‰ª∂ÂÄºÂæóÂ∫ÜÁ•ùÁöÑ‰∫ãÊÉÖÔºå‰∏ªË¶ÅÂæóÁõä‰∫éËÄÅÊùøÂíåË∂ÖÂì•Âú®Á†îÁ©∂‰∏éÂÜô‰ΩúÊñπÈù¢ÁªôÊàëÁöÑÊåáÂØº„ÄÇËøô‰πüÊòØ‰∏ÄÁßçÊù•Ëá™ÂêåË°åÁöÑËÆ§ÂèØÔºåËØ¥ÊòéÊàë‰ª¨ÂÅöÁöÑ‰∏úË•øÊúâ‰ª∑ÂÄºÔºåÊØïÁ´ü‰πãÂâçËØªÂà∞‰∏ÄÁØáÂÖ≥‰∫éresearcherÁöÑÂøÉÁêÜÁä∂ÊÄÅÁöÑÊñáÁ´†‰πüËØ¥Ëøá‰∫ÜÔºåresearchers tend to be their own worst crticsÔºöÊàë‰ª¨ÊÄªÊòØËÉΩÂú®Ëá™Â∑±ÁöÑÁ†îÁ©∂ÈáåÊâæÂà∞‰∏çË∂≥ÔºåÊàñËÆ∏ÊòØÊàëÂØπËøô‰∏™ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂÖ∂ÂÆûÂπ∂Ê≤°ÊúâÈÇ£‰πàËá™‰ø°Ôºå‰∫¶ÊàñÊòØËøô‰∏™ÊñπÊ≥ïÁöÑÊïàÊûúÂú®Êüê‰∫õÊó∂ÂÄôÂÖ∂ÂÆûÂπ∂‰∏çÈÇ£‰πàÁ®≥ÂÆöÁ≠âÁ≠â„ÄÇ&lt;/p&gt;

&lt;h4 id=&quot;ÂπøÂëäÂπøËÄåÂëä‰πã&quot;&gt;ÂπøÂëäÔºåÂπøËÄåÂëä‰πã&lt;/h4&gt;
&lt;p&gt;Ëøô‰∏™ÊúàÁöÑÊúàÂøóÔºå‰∏ªË¶ÅÊòØÊÉ≥ËÆ∞ÂΩï‰∏Ä‰∏ãÁªôÊàë‰ª¨&lt;a href=&quot;https://frs.ethz.ch/&quot;&gt;FRSÁ†îÁ©∂ÁªÑ&lt;/a&gt;ÊâÄÂÅöÁöÑ‰∏ÄÊ¨°Â≠¶ÊúØÊä•Âëä(presentation)ÁöÑÁªèÂéÜ„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏ä‰∏ÄÊ¨°Âú®FRSÂÅöÊä•ÂëäËøòÊòØ‰∏ÄÂπ¥ÂâçÔºåÈÇ£‰∏™Êó∂ÂÄôÂÖ∂ÂÆûËá™Â∑±Âπ∂Ê≤°ÊúâÂ§™Â§öÂÖ∑‰ΩìÁöÑ‰∏úË•øÂèØ‰ª•ËÆ≤ÔºåÂπ∂‰∏îÂΩìÊó∂ÁöÑÂΩ¢ÂºèÊòØÊØè‰∏™‰∫∫three minutes, three slidesÔºåÊó†Ê≥ïÊ∑±ÂÖ•ËÅä„ÄÇ‰ªäÂπ¥Á†îÁ©∂ÁªÑÊç¢‰∫ÜleadershipÔºåÂºÄÂßã‰∫ÜÂçäÂ∞èÊó∂Âë®‰ºöÊä•ÂëäÁöÑÂàÜ‰∫´ÂΩ¢ÂºèÔºåÂÜçÂä†‰∏äÊñ∞È°πÁõÆÊù•‰∫ÜÂæàÂ§öÊñ∞ÁöÑÈù¢Â≠îÔºå6Êúà‰∏≠Êó¨ËΩÆÂà∞ÊàëÂàÜ‰∫´Êó∂ÔºåÊàëÂ∞±ÊÉ≥ÁùÄË∂ÅËøô‰∏™Êú∫‰ºöÊääÊàëÁöÑÁ†îÁ©∂Êï¥‰∏™‰ªãÁªç‰∏Ä‰∏ã„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊàëÁöÑÁ†îÁ©∂ÈõÜ‰∏≠‰∫éÁªüËÆ°ÊñπÊ≥ïÂú®ÁîµÂäõÁ≥ªÁªüÁöÑÂ∫îÁî®ÔºåÊù•ÂèÇÂä†ÂàÜ‰∫´‰ºöÁöÑ‰∫∫ÂåÖÊã¨ÂçöÂ£´Áîü„ÄÅÂçöÂêé„ÄÅÊïôÊéàÁ≠âÊ∂âË∂≥‰∏çÂêåÈ¢ÜÂüü‰πüÊúâÁùÄ‰∏çÂêåÁöÑËÉåÊôØÁü•ËØÜÔºåÂè™ÊúâÂ∞ëÊï∞‰∫∫Âú®ÁîµÂäõÊñπÈù¢ÂÅöÁ†îÁ©∂ÔºåÂèØËÉΩÊõ¥Â∞ëÁöÑ‰∫∫‰∫ÜËß£Â∑•‰∏öÁªüËÆ°„ÄÇÈÇ£‰πàÈóÆÈ¢òÊù•‰∫ÜÔºåÊàëÂ∫îËØ•Â¶Ç‰ΩïÁÖßÈ°æÂà∞ËßÇ‰ºóÂéªËÆæËÆ°ÂàÜ‰∫´Êä•ÂëäÁöÑÂÜÖÂÆπÔºüÂ¶ÇÊûúÊàë‰∏Ä‰∏äÊù•Â∞±‰∏Ä‰∏™Âä≤ÂÑøÂú∞‰ªãÁªçÁîµÂäõÁ≥ªÁªüÂíåÁªüËÆ°ÊñπÊ≥ïÔºåÂèØËÉΩ‰∏çÂà∞‰∏âÂàÜÈíüÂ∞±‰ºöÂ§±ÂéªÂ§ßÈÉ®ÂàÜËßÇ‰ºóÁöÑÂÖ¥Ë∂£„ÄÇËÆ§Ê∏ÖËøôÊ¨°ÂàÜ‰∫´ÁöÑ&lt;strong&gt;ËßÇ‰ºóÁæ§‰Ωì&lt;/strong&gt;‰∏é‰ªñ‰ª¨Â∑≤ÊúâÁöÑ&lt;strong&gt;Áü•ËØÜËÉåÊôØ&lt;/strong&gt;‰πãÂêéÔºåÊàëÂ∞±ÂèØ‰ª•ÂÆö‰∏Ä‰∏™ÊÄªÁöÑÁõÆÁöÑ‰∫ÜÔºöËÆ©ËßÇ‰ºóÂØπÊàëÁ†îÁ©∂ÁöÑÈóÆÈ¢ò„ÄÅÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄß‰ª•ÂèäÊâÄÊ∂âÂèäÁöÑÊñπÊ≥ïÊúâ‰∏Ä‰∏™Áõ¥ËßÇÁöÑÁêÜËß£(intuitive understanding)„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏ã‰∏Ä‰∏™Êàë‰ºöÊÉ≥Âà∞ÁöÑÈóÆÈ¢òÂ∞±ÊòØÔºåÂ¶ÇÊûúËßÇ‰ºó‰ªéÊàëÁöÑÂàÜ‰∫´‰∏≠Âè™ËÆ∞‰Ωè‰∫Ü‰∏Ä‰ª∂‰∫ãÂÑøÔºåËøô‰∫ãÂÑøÊòØ‰ªÄ‰πàÔºüÊç¢Âè•ËØùËØ¥ÔºåÊàëÊï¥‰∏™ÂàÜ‰∫´ÁöÑ&lt;strong&gt;‰∏≠ÂøÉÊÄùÊÉ≥&lt;/strong&gt;ÊòØ‰ªÄ‰πà„ÄÇÂÆöÂ•ΩËøôÊ†∑‰∏Ä‰∏™key takeaway‰πãÂêéÔºåÂ∞±ÂèØ‰ª•Ê†πÊçÆÂÆÉÂéªËÆæËÆ°ÂàÜ‰∫´ÁöÑÂÜÖÂÆπ‰∫ÜÔºöÊâÄÊúâÂÜÖÂÆπÈÉΩË¶ÅÊúçÂä°‰∫éËøô‰∏™‰∏≠ÂøÉÊÄùÊÉ≥ÔºåÂ¶ÇÊûú‰∏çÁõ∏ÂÖ≥ÔºåÈÇ£Â∞±Ë¶ÅÊûúÊñ≠Âà†Âéª„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂõûÂà∞ÁÖßÈ°æËßÇ‰ºóÁöÑÈóÆÈ¢òÔºåÊàëÂú®ÊÄùËÄÉË¶ÅÂ¶Ç‰ΩïËß£ÂÜ≥ËßÇ‰ºóÁöÑÁü•ËØÜËÉåÊôØ‰∏éÊàëÁöÑÁ†îÁ©∂‰πãÈó¥ÁöÑÂ∑ÆÂºÇÈóÆÈ¢òÊó∂Ôºå‰∏ÄÁõ¥ÊÉ≥ÊâæÂà∞‰∏Ä‰∏™ÂêàÈÄÇÁöÑÁ±ªÊØî(analogy)„ÄÇ‰ªãÁªçÈôåÁîüÁöÑÊ¶ÇÂøµÊó∂ÔºåÂ¶ÇÊûúÁî®ËßÇ‰ºóÁÜüÊÇâÁöÑ‰∫ãÊÉÖÂÅöÁ±ªÊØîÔºåÂæÄÂæÄËÉΩ‰∏Ä‰∏ãÂ≠êËÆ©Â§ßÂÆ∂Êúâ‰∏Ä‰∏™Áõ¥ËßÇÁöÑÁêÜËß£„ÄÇÂÆûÈôÖ‰∏äÔºåËøôÊ†∑ÁöÑ‚ÄúÂ•óË∑Ø‚ÄùÂú®ÂêÑÁ±ªtalk showÈáåÁªèÂ∏∏Ë¢´Áî®Âà∞ÔºåÂ∞ΩÁÆ°Â§ßÂ§öÊï∞Êó∂ÂÄôÊòØÂ∏¶ÁùÄÊàèË∞ëÊàêÂàÜÁöÑÁ±ªÊØî„ÄÇ&lt;/p&gt;

&lt;p&gt;Ê≤øÁùÄËøô‰∏™ÊÄùË∑ØÔºåÊàëË∂äÂèëÊ∏ÖÊô∞Âú∞ËÆ§ËØÜÂà∞ÔºåÊàëÊâÄÂÅöÁöÑÁîµÁΩëÊñ≠Á∫øÁõëÊµãÁ†îÁ©∂‰∏é‰º†ÊüìÁóÖÂ§ßÊµÅË°åÊúâÂá†‰∏™ÈùûÂ∏∏Áõ∏‰ººÁöÑÂú∞ÊñπÔºö‰ªñ‰ª¨ÈÉΩËÉΩËøÖÈÄü‰ªé‰∏Ä‰∏™caseÂèòÊàê‰∏Ä‰∏™clusterÔºõ‰ªñ‰ª¨Âá∫Áé∞Êó∂ÈÉΩ‰ºöÊúâ‰∫õÂºÇÂ∏∏ÁöÑ‚Äú‰ø°Âè∑‚ÄùÔºõËÄå‰∏îËøô‰∫õ‚Äú‰ø°Âè∑‚Äù‰∏ÄÂºÄÂßã‰ºöÂæàÂæÆÂº±Ôºå‰ª•Âèä‰∏çÂÆåÊï¥„ÄÇGood!ËôΩÁÑ∂‰∏çÊòØÊØè‰∏™‰∫∫ÈÉΩÁÜüÊÇâÁîµÁΩëÔºå‰ΩÜÊòØÊàëÁõ∏‰ø°Â§ßÂÆ∂Â∫îËØ•ÂØπÊñ∞ÂÜ†Áñ´ÊÉÖ‰∏ç‰ºöÈôåÁîü‰∫Ü„ÄÇÊâÄ‰ª•ÂêéÊù•Âú®ÊàëÁöÑÂàÜ‰∫´ÈáåÔºåÂá†‰πéÊØè‰∏Ä‰∏™ÂÖ≥ÈîÆÂú∞ÊñπÔºåÊàëÈÉΩ‰ºöÁî®Êñ∞ÂÜ†Áñ´ÊÉÖ‰Ωú‰∏∫Á±ªÊØîÊù•Ëß£Èáä„ÄÇ‰ªéÊàëÂæóÂà∞ÁöÑ‰∏Ä‰∫õÂèçÈ¶àÊù•ÁúãÔºåËøô‰∏™Á±ªÊØîÂ∫îËØ•ËøòÊòØÂèëÊå•‰∫Ü‰∏Ä‰∫õ‰ΩúÁî®‰∫Ü„ÄÇ&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/assets/month-journal/presentation-2.png&quot; alt=&quot;presentation-2&quot; /&gt;
  &lt;figcaption&gt;ËøôÈáåÊàëÊääÁ†îÁ©∂ÈóÆÈ¢ò‰∏≠ÁöÑÊåëÊàò‰∏éÂØπÊäóÊñ∞ÂÜ†Áñ´ÊÉÖÁöÑÊåëÊàòÂÅö‰∫ÜÁ±ªÊØî„ÄÇ&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Êï¥‰∏™ÂàÜ‰∫´ÊàëËä±‰∫ÜÊúÄÂ§öÁöÑÊó∂Èó¥ÂéªËÅäÈóÆÈ¢òÁöÑÊÑè‰πâÔºåÁõÆÂâçÂ§ßÂÆ∂Èù¢‰∏¥ÁöÑÊåëÊàòÔºå‰ª•ÂèäÊàë‰ª¨ÁöÑÁ†îÁ©∂ËÉΩËß£ÂÜ≥Âì™ÈÉ®ÂàÜÁöÑÈóÆÈ¢òÔºå‰∏≠Èó¥Á©øÊèíÁùÄÂêÑÁßçË∑üÊñ∞ÂÜ†Áñ´ÊÉÖÁöÑÁ±ªÊØîÔºåÁ°Æ‰øùËßÇ‰ºóËÉΩÂØπÊàëËØ¥ÁöÑ‰∏úË•øÊúâ‰∏Ä‰∏™Áõ¥ËßÇÁöÑÁêÜËß£„ÄÇËÅäÁöÑÊó∂Èó¥ÊúÄÂ∞ëÁöÑÊòØÂÆûÈôÖÂ∑•‰Ωú‰∏≠Ë¥πÊó∂ÊúÄ‰πÖÁöÑÈÉ®ÂàÜÔºåÊØîÂ¶ÇÊñπÊ≥ï„ÄÅ‰ªøÁúüÂíå‰ª£Á†ÅÁ≠âÁ≠âÔºåÊØïÁ´üÊàëÁöÑÁõÆÁöÑÊòØËÆ©Â§ßÂÆ∂ÂØπËøôÈóÆÈ¢òÊúâÊâÄÂÖ¥Ë∂£ÔºåËÄå‰∏çÊòØÂè™ÊääÊàë‰ª¨ÂÜôÁöÑÊñáÁ´†ËØªÂá∫Êù•„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÊàëÂú®ÂÜôÂàÜ‰∫´‰ºöÁöÑÁ®øÂ≠êÊó∂ÔºåÁâπÊÑèÂú®Êüê‰∏ÄÈ°µÁöÑÂÜÖÂÆπÈáåËÆæËÆ°‰∫Ü‰∏Ä‰∏™jokeÔºåÊÉ≥Â∞ùËØï‰∏Ä‰∏ãÂÉètalk show‰∏ÄÊ†∑Ëê•ÈÄ†‰∏Ä‰∏ãËΩªÊùæÁöÑÊ∞îÊ∞õ„ÄÇÁÑ∂ËÄåÔºåÂú®ÂÆûÈôÖÂàÜ‰∫´ËøáÁ®ã‰∏≠Ôºå‰∏çÁü•ÈÅìÊòØÂõ†‰∏∫ËÑ±Á®øËøòÊòØÂõ†‰∏∫Ëøô‰∫ãÂÑøÂØπÊàëÊù•ËØ¥ÂÆûÂú®Â§™Êñ∞ÔºåÁ´üÁÑ∂ÂÆåÂÆåÂÖ®ÂÖ®ÂøòËÆ∞‰∫ÜËØ¥Ëøô‰∏™jokeÔºåÊúÄÂêéËÆ≤ÂÆåÊâçÊÉ≥Ëµ∑Êù•ÔºåÂÆûÂú®ÊúâÁÇπÂ∞èÂ∞èÁöÑÈÅóÊÜæ„ÄÇË°åÂêßÔºå‰∏ãÊ¨°ÂÜçËØï‰∏ÄÊ¨°„ÄÇ&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>‰∏Ä‰ª∂‚ÄúÂ∞è‰∫ã‚ÄùÔºöDiscriminationÂÖ∂ÂÆûÁ¶ªÊàë‰ª¨ÂæàËøë</title>
   <link href="http://localhost:4000/journal/2020/06/14/journal-discrimination.html"/>
   <updated>2020-06-14T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/06/14/journal-discrimination</id>
   <content type="html">&lt;p&gt;ËÆ∞ÂΩï‰∏Ä‰ª∂‚ÄúÂ∞è‰∫ã‚Äù„ÄÇ&lt;/p&gt;

&lt;p&gt;Êò®Â§©Ë∑üÂ•≥ÊúãÂèãÂùêËΩ¶ÂéªË∂ÖÂ∏ÇÔºåÂùêÂú®‰∫ÜÈù†Ëøë‰∏ãËΩ¶Â§ÑÁöÑÁ¨¨‰∏ÄÊéí‰∏§‰∫∫Â∫ß‰Ωç„ÄÇ‰∏ÄË∑Ø‰∏äË∑üÂæÄÂ∏∏‰∏ÄÊ†∑ËÅäÂ§©ÔºåËØ¥ËøôËØ¥ÈÇ£ÁöÑÔºåÂùê‰∫ÜÂ§ßÊ¶Ç‰∫åÂçÅÂàÜÈíüÂø´Ë¶ÅÂà∞Á´ôÁöÑÊó∂ÂÄôÔºå‰∏Ä‰ΩçÂ∏¶ÁùÄÊñ∞Âä†Âù°Âè£Èü≥ÁöÑÁü≠ÂèëÈòøÂß®Á™ÅÁÑ∂‰ªéÊàë‰ª¨ÂêéÈù¢Ëµ∞Âá∫Êù•ÔºåÁ´ôÂú®ÊâìÂç°‰∏ãËΩ¶Â§ÑÔºåÂàöËêΩËÑöÂ∞±ÂºÄÂßãÂØπÊàë‰ª¨ËÆ≤ËØùÔºåÈòøÂß®ËØ¥Ôºö‚ÄúAre you from China? Don‚Äôt you watch our news? You don‚Äôt talk (Refrain from talking)  on public transport!‚Äù Ôºà‰∏≠ÊñáÔºö‰Ω†‰ª¨ÊòØ‰∏≠ÂõΩÊù•ÁöÑÂêóÔºü‰Ω†‰ª¨‰∏çÁúãÊàë‰ª¨ÁöÑÊñ∞ÈóªÂêóÔºü‰∏çË¶ÅÂú®ÂÖ¨ÂÖ±‰∫§ÈÄö‰∏äËÆ≤ËØùÔºÅÔºâ  Âõ†‰∏∫ÈÉΩÊà¥ÁùÄÂè£ÁΩ©ÔºåÂêé‰∏ÄÂè•ÊòØÊàë‰∏çÂ§ßÁ°ÆÂÆöÁöÑÈÉ®ÂàÜÔºå‰ΩÜÊòØÂâç‰∏§Âè•ÊàëÂê¨ÂæóÂæàÊ∏ÖÊ•ö„ÄÇËøôÈòøÂß®Âá∫Áé∞ÂæóÁ™ÅÁÑ∂ÔºåÊàë‰ª¨‰∏ÄÊó∂Èó¥Ê≤°ÂèçÂ∫îËøáÊù•ÔºåËΩ¶Èó®ÊâìÂºÄÂêéÔºåÂ•πÁ´ãÈ©¨Â∞±Êâì‰∫ÜÂç°‰∏ã‰∫ÜËΩ¶ÔºåËµ∞Âá∫ËΩ¶Èó®ÁöÑÂêåÊó∂ÔºåËøò‰∏ÄËæπÁúãÁùÄÊàë‰ª¨‰∏ÄËæπÁî®ÊâãÂú®Ëá™Â∑±ÁöÑÂ§™Èò≥Á©¥ÊóÅÁîªÂúàÔºåÊÑèÊÄùÊòØÊàë‰ª¨ËÑëÂ≠êÊúâÈóÆÈ¢ò„ÄÇÁâáÂàª‰πãÂêéÔºåÁ≠âÊàë‰ª¨ÂèçÂ∫îËøáÊù•Ôºå‰∏ãËΩ¶ÂéªÁúãÔºåÂ•πÂ∑≤ÁªèËµ∞Âá∫ÂæàËøú„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏ÄÊó∂Èó¥Êàë‰ª¨‰ø©ÈÉΩË¢´Â§çÊùÇÁöÑÊÉÖÁª™ÂåÖË£πÁùÄÔºåËØ¥‰∏çÂá∫ËØùÊù•„ÄÇÊàë‰ª¨ÂøÉÈáåÊÑüÂà∞Êúâ‰∫õÊä±Ê≠âÔºåÂõ†‰∏∫ÊîøÂ∫úÁ°ÆÂÆûÊúâÂÆ£‰º†ÔºåÁªìÊùüÂ∞ÅÂüé‰πãÂêéÔºåÂ§ßÂÆ∂Âú®ÂÖ¨ÂÖ±‰∫§ÈÄö‰∏ä‰∏çË¶ÅËÆ≤ËØùÔºåËøôÁ°ÆÂÆûÊòØÊàë‰ª¨ÁñèÂøΩ‰∫ÜÔºõ‰ΩÜÂêåÊó∂ÔºåÊàëÊÉ≥Êàë‰ª¨‰ø©ÈÉΩÊÑüÂà∞‰∫Ü‰∏ÄÁßçË¢´‰æÆËæ±ÁöÑÂøÉÊÉÖÔºåÂõ†‰∏∫Â•πÁöÑÁ¨¨‰∏ÄÂè•ËØùÂíå‰∏ãËΩ¶Êó∂ÁöÑÊâãÂäø„ÄÇÂ¶ÇÊûúÂ•πÂè™ÊòØ‰∏∫‰∫ÜÂ§ßÂÆ∂ÁöÑÂÆâÂÖ®ÁùÄÊÉ≥ÔºåÂÆåÂÖ®ÂèØ‰ª•Âú®Êàë‰ª¨ËØ¥ËØùÊó∂ËøáÊù•ÊèêÈÜíÊàë‰ª¨ÔºåÁîöËá≥Â§ßÂ£∞Êñ•Ë¥£Ôºå‰πüÊÅêÊÄïÂèØ‰ª•ÁêÜËß£„ÄÇ‰ΩÜÊòØÔºå‰∏∫‰ªÄ‰πàÂ•πÈÄâÊã©Âú®‰∏ãËΩ¶Ââç‰∏ÄÁßíË∑üÊàë‰ª¨ËØ¥ÔºåÁÑ∂ÂêéÂø´Ê≠•Ëµ∞ÂºÄÔºü‰∏∫‰ªÄ‰πàÁ¨¨‰∏ÄÂè•ËØùÊòØÈóÆÂá∫‚Äú‰Ω†‰ª¨ÊòØ‰∏çÊòØ‰∏≠ÂõΩÊù•ÁöÑÔºü‚ÄùËøôÊ†∑ÁöÑÈóÆÈ¢òÔºüËøôËÉåÂêéÊòØÂê¶Â∏¶ÁùÄ‰∏ÄÁßçÂØπ‰∏≠ÂõΩ‰∫∫ÁöÑÈöêÂΩ¢ÁöÑÊ≠ßËßÜÊàñÂÅèËßÅÂë¢ÔºüÂ¶ÇÊûúÊàë‰ª¨ÊòØÊó•Êú¨‰∫∫ÔºåÊàñËÄÖËã±ÂõΩ‰∫∫ÔºåÂ•π‰πü‰ºöÈóÆÂá∫‰∏ÄÊ†∑ÁöÑÂÖ≥‰∫éÂõΩÁ±çÁöÑÈóÆÈ¢òÂêóÔºü&lt;/p&gt;

&lt;p&gt;Êñ∞ÂÜ†Áñ´ÊÉÖÂàöÁàÜÂèëÊó∂Ôºå‰∫öË£îÂú®ÂæàÂ§öÂõΩÂÆ∂ÈÅ≠ÂèóÂà∞‰∫ÜÈùûÊö¥ÂäõÂíåÊö¥ÂäõÁöÑÊîªÂáªÔºõÊúÄËøëÂõ†‰∏∫ÂºóÊ¥õ‰ºäÂæ∑ÁöÑ‰∫ã‰ª∂ÔºåÂèçÊäóÁßçÊóèÊ≠ßËßÜ„ÄÅË≠¶ÂØüÊö¥ÂäõÁöÑÊäóËÆÆÊ¥ªÂä®Â∏≠Âç∑ÂÖ®ÁæéÔºå‰ª•ÂèäÂÖ∂‰ªñË¢´ÊÆñÊ∞ëËøáÊàñÊ∑±ÂèóÊÆñÊ∞ëÊó∂ÊúüÂΩ±ÂìçÁöÑÂõΩÂÆ∂„ÄÇÁæéÂõΩÁöÑÁßçÊóèÊ≠ßËßÜÈóÆÈ¢òÊúâÈïø‰πÖÁöÑÂéÜÂè≤ÂéüÂõ†ÔºåÊòØÂà∂Â∫¶ÊÄßÁöÑ„ÄÅËÆ©‰∫∫ÂëºÂê∏‰∏çËøáÊù•ÁöÑÔºå‰ΩÜÊàëÂØπËøô‰∫õ‰∫ÜËß£ÊúÄÂ§öÊù•Ëá™‰∏Ä‰∫õÊñáÂ≠ó„ÄÅÂΩ±ÂÉèÂíåÂ™í‰ΩìËÆ∞ÂΩïÔºåÂÆûÈôÖ‰∏äÊó†Ê≥ïÁúüÊ≠£‰Ωì‰ºöÂà∞‰Ωú‰∏∫‰∏Ä‰∏™ÈùûË£îÁæéÂõΩ‰∫∫ÔºåÂú®ÈÇ£Ê†∑ÁöÑÁéØÂ¢É‰∏≠ÈïøÂ§ß„ÄÅÁîüÂ≠òÊòØÊÄéÊ†∑ÁöÑÁªèÂéÜ„ÄÇ&lt;/p&gt;

&lt;p&gt;Êñ∞Âä†Âù°‰πüÊõæÁªèÊòØËã±ÂõΩÁöÑÊÆñÊ∞ëÂú∞Ôºå‰ªéÂè§Ëá≥‰ªäÁöÑÈáçË¶ÅË¥∏Êòì‰∫§Ê±áÁÇπËÅöÈõÜ‰∫ÜÊù•Ëá™‰∏çÂêåÂú∞Âå∫ÁöÑ‰∫∫ÔºåÁßçÊóè‰πãÈó¥ÁöÑÂØπÊäóÂíåÊë©Êì¶‰πü‰∏ÄÁõ¥Â≠òÂú®„ÄÇÊúÄËøëÁöÑ‰∏ÄÊ¨°Â§ßËßÑÊ®°ÂÜ≤Á™Å‰∫ã‰ª∂ÂèëÁîüÂú®1969Âπ¥ÔºåÂú®ÈÇ£‰πãÂêéÊîøÂ∫úÂíåÊ∞ë‰ºóÂæàÂä™ÂäõÂéªÁª¥ÊåÅÂ§öÁßçÊóè„ÄÅÂ§öÂõΩÁ±ç„ÄÅÂ§öÂÆóÊïôÁöÑÁ§æ‰ºöÁöÑÂíåË∞êÔºå‰πüÊ≤°ÂÜçÂèëÁîüËøáË∑üÁßçÊóèÊàñÂÆóÊïôÁõ∏ÂÖ≥ÁöÑ‰∏•ÈáçÁöÑÊäóËÆÆÊ∏∏Ë°å‰∫ã‰ª∂„ÄÇ‰ΩÜÔºåËøô‰∏ç‰ª£Ë°®ÈöêÊÄßÁöÑÊ≠ßËßÜ‰∏çÂ≠òÂú®‰∫ÜÔºåÊò®Â§©ÁöÑ‰∫ãÊÉÖÊÅ∞ÊÅ∞ËØ¥ÊòéËøôÊ†∑ÁöÑÂÅèËßÅÂÖ∂ÂÆûÁ¶ªÊàë‰ª¨ÂæàËøë„ÄÇÊàëËßâÂæóÊàëÊúâ‰πâÂä°ÊääÁªèÂéÜÁöÑËøô‰∏Ä‰ª∂‚ÄúÂ∞è‰∫ã‚ÄùËØ¥Âá∫Êù•ÔºåÂõ†‰∏∫Â§™Â§öÁöÑÊ≤âÈªòËÆ©Á±ª‰ººÁöÑ‰∫ãÊÉÖ‰∏ÄÂÜçÂèëÁîü„ÄÇÂè¶Â§ñÔºå‰∏Ä‰∏™‰∫∫ÊÅêÊÄïÂæàÈöæÊîπÂèòÂà´‰∫∫ÁöÑÊó¢ÊúâÂÅèËßÅÔºåÁîöËá≥ÂæàÂ§öÊó∂ÂÄô‰ºöË∂äÊèèË∂äÈªëÔºå‰ΩÜÊàë‰ª¨ÂèØ‰ª•Â∞ΩÈáè‰∏çÁî®ÊúâËâ≤ÁúºÈïúÂéªÁúãÂæÖÂÖ∂‰ªñÁßçÊóè„ÄÅ‰ø°‰ª∞„ÄÅÂá∫Ë∫´Âú∞„ÄÅÁîöËá≥ÊòØ‰ªª‰Ωï‰∏éÊàë‰ª¨ËÉåÊôØ‰∏çÂêåÁöÑ‰∫∫„ÄÇ&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>2020Âπ¥5ÊúàÔºöÊµÖÂ∞ùSnack Writing</title>
   <link href="http://localhost:4000/journal/2020/06/06/journal-2020-05.html"/>
   <updated>2020-06-06T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/06/06/journal-2020-05</id>
   <content type="html">&lt;p&gt;‰∫îÊúàÂõ†‰∏∫Áñ´ÊÉÖÁöÑÂéüÂõ†ÁªßÁª≠Âú®ÂÆ∂ÈáåÂ∑•‰Ωú„ÄÇËøôÊÆµÊó∂Èó¥Â∑•‰ΩúÈáçÂøÉ‰∏ªË¶ÅÂú®ÂÜô‰∏úË•ø‰∏äÈù¢Ôºå‰∏çÂ§™ÈúÄË¶ÅË∑ëÂ§ßÈáèÁöÑÊï∞ÊçÆ„ÄÅ‰πü‰∏çÈúÄË¶ÅÂ§™Â§öË∑üÂêå‰∫ãËÄÅÊùøÁöÑ‰∫§ÊµÅÔºåËôΩÁÑ∂ÊàëËõÆÊÉ≥ÂøµÂäûÂÖ¨ÂÆ§ÁöÑespressoÂíñÂï°ÂíåÊó∂‰∏çÊó∂‰∏çÂêåÈ¢ÜÂüüÁöÑ‰∫∫ËøáÊù•ÂÅöÁöÑseminarÁöÑÔºåËøô‰∏§‰ª∂‰∫ãÂÑøÂÆ∂ÈáåÊ≤°Ê≥ïÂ§çÂà∂„ÄÇ&lt;/p&gt;

&lt;p&gt;Êèê‰∫§‰∫Ü‰∏ä‰∏ÄÁØáÊñáÁ´†ÁöÑÂÆ°Á®øÊÑèËßÅÂõûÂ§çÔºå‰∏§‰∏™ÂÆ°Á®ø‰∫∫Êèê‰∫Ü‰∏çÂ∞ëÈóÆÈ¢ò‰ΩÜÈÉΩÊòØÂª∫ËÆæÊÄßÊàñËÄÖÊæÑÊ∏ÖÁ±ªÁöÑÔºåÊàëÂÜôÁöÑÂõûÂ§çÂàùÁ®øË¢´ËÄÅÊùø‰øÆÊîπ‰∏çÂ∞ëÔºàFirst version is always subpar.)„ÄÇÊàëÂèëÁé∞ÊàëÁöÑÂõûÂ§çÂæÄÂæÄÊõ¥Â©âËΩ¨„ÄÅËøÇÂõûÔºåÁî®ËØç‰πü‰π†ÊÉØÊÄßÂú∞Áî®ÊàëÁÜüÊÇâÁöÑËøô‰∏™Â∑•‰ΩúÁöÑËØ≠Ë®ÄÔºõËÄåÂØπÊØîËÄÅÊùø‰øÆÊîπËøáÁöÑÁâàÊú¨ÔºåË°®ËææÁöÑÁÇπÊòØ‰∏ÄÊ†∑ÁöÑÔºå‰ΩÜËøô‰∏™ÁÇπÂú®‰ªñÁöÑÂõûÂ§çÈáåÂ∞±ÂæàÊòéÁ°Æ„ÄÅÊõ¥assertiveÔºåÂêåÊó∂‰πüÊääÈÇ£‰∫õÊõ¥ÊäÄÊúØÊÄßÁöÑËØçËØ≠Êç¢Êàê‰∫ÜÊõ¥ÊòìÁêÜËß£ÁöÑ„ÄÇËøô‰πà‰∏ÄÊÉ≥ÔºåÊØèÊ¨°ÂÅöÊä•ÂëäÁöÑÈóÆÁ≠îÁéØËäÇÊàë‰πüÊòØËøôÊ†∑„ÄÇÊúÄÂêéÊèê‰∫§‰∏äÂéªÁöÑÂõûÂ§çÔºåÁªôÊàëÁöÑÊÑüËßâÊòØÔºöÂêåÊÑèÁöÑÁÇπ‰ºöÊòéÁ°ÆËØ¥„ÄÅ‰∏çÊ∏ÖÊ•öÁöÑÁÇπ‰ºöÁî®ÊµÖÊòæÁöÑËØ≠Ë®ÄËß£Èáä„ÄÅËØØËß£‰∫ÜÁöÑÁÇπ‰ºöÁî®‰∏çÂçë‰∏ç‰∫¢ÁöÑÂßøÊÄÅÊæÑÊ∏Ö„ÄÇÊúÄÂêéÊ†πÊçÆÂÆ°Á®ø‰∫∫ÊÑèËßÅÂÅöËøá‰øÆÊîπÁöÑÊñáÁ´†ÔºåÁ°ÆÂÆûÊòØË¶ÅÊØî‰πãÂâçÁöÑÁâàÊú¨Êõ¥ÂÆåÊï¥„ÄÅÂèØËÉΩÈÄ†ÊàêËØØËß£ÁöÑÂú∞ÊñπÊõ¥Â∞ë‰∫Ü„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏ãÂçäÊúàÊää‰πãÂâçÂ•Ω‰∏çÂÆπÊòìÊêûÊáÇÁöÑÊñπÊ≥ïÁî®$\LaTeX$‰∏Ä‰∏ÄÊâìÂá∫Êù•ÂÜôËøõ‰∫ÜËá™Â∑±ÁöÑÁ¨îËÆ∞ÈáåÔºåÁÑ∂ÂêéÂèàÂü∫‰∫éËøô‰∏™Á¨îËÆ∞ÂÜô‰∫ÜÁ¨¨‰∫å‰∏™Â∑•‰ΩúÁöÑÊñáÁ´†ÁöÑÂàùÁ®øÔºà‰∏ªË¶ÅÊòØÊñπÊ≥ïÈÉ®ÂàÜÔºâÔºåÂåÖÊã¨ÁîµÂäõÁ≥ªÁªüÁöÑÂä®ÊÄÅÊ®°Âûã„ÄÅÁî®particle filterÂÅöÁä∂ÊÄÅ‰º∞ËÆ°‰ª•ÂèäÂ¶Ç‰ΩïÁî®EMÂÅöÂèÇÊï∞‰º∞ËÆ°„ÄÇÊñπÊ≥ïÈáåÈù¢Ëá™Â∑±Ëä±Êó∂Èó¥ÊúÄ‰πÖÁöÑÂ∫îËØ•ÊòØÊúÄÂêéÈÇ£‰∏™ÂèÇÊï∞‰º∞ËÆ°ÁöÑÊñπÊ≥ïÔºå‰∏ä‰∏ÄÁØáÊúàÂøó‰πüÊèêÂà∞ËøáÔºåÂú®Á¨îËÆ∞ÈáåÁöÑÁØáÂπÖ‰πüÊòØÊúÄÈïøÁöÑÔºåÂõ†‰∏∫ÂêÑÁßçÊ≠•È™§„ÄÇ‰∏çËøáÔºåÂõ†‰∏∫‰∏çÊòØËøô‰∏™Â∑•‰ΩúÁöÑÈáçÁÇπÔºåÊúÄÁªàÂÜôÂà∞ÊñáÁ´†ÈáåÂ∞±Âè™ÊúâÂá†ÊÆµËØù„ÄÇËøôÈÅìÁêÜÔºåÂçö‰∏ÄÁöÑÊó∂ÂÄô&lt;a href=&quot;https://www.eng.nus.edu.sg/isem/staff/ye-zhisheng/&quot;&gt;Âè∂ËÄÅÂ∏à&lt;/a&gt;ËØæ‰∏äÂ∞±Â∑≤ÁªèË∑üÊàë‰ª¨ÂàÜ‰∫´Ëøá‰∫ÜÔºåÂÜôÊñáÁ´†ÁöÑÊó∂ÂÄôË¶Å&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Write around your contributions, not about what you have learned.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ëøô‰∏ÄÊ¨°ÂÜôÔºåÂ∞ùËØï‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂÜô‰ΩúÊ®°ÂºèÔºöÊØèÂ§©Êó©‰∏äËµ∑Â∫äÂêéÂÜô‰∏Ä‰∏™Â∞èÊó∂‰ªª‰ΩïÊàëÁé∞Âú®Âú®ÂÜôÁöÑ„ÄÅÂú®Â≠¶ÁöÑ‰∏úË•ø„ÄÇÁºòÁî±ÊòØÊàëÂõûÂéªÂÜçËØª‰∫Ü‰∏Ä‰∏ã‰∏ä‰∏™Êó•ÂøóËØ¥Âà∞ÁöÑÈÇ£Êú¨‰π¶ÔºåÈáåÈù¢Êúâ‰∏ìÈó®ÂÜôÂà∞Á†îÁ©∂ÁîüÂ∫îËØ•Â¶Ç‰ΩïÂÜô‰Ωú„ÄÇËøôÈáåÁöÑ‚ÄúÂ¶Ç‰ΩïÂÜô‰Ωú‚Äù‰∏çÊòØÊåáÁöÑÂÜô‰ΩúÈ£éÊ†º„ÄÅËØ≠Ë®ÄÁªÑÁªá‰πãÁ±ªÔºåËÄåÊòØÊåá‰ªÄ‰πàÊó∂ÂÄôÂÜôÔºåÂÜô‰∫õ‰ªÄ‰πà„ÄÇ‰π¶‰∏≠‰∏ªË¶ÅÊèêÂà∞&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Don‚Äôt write when ready, write to be ready.&lt;/li&gt;
  &lt;li&gt;Don‚Äôt be binge writing, be snack writing.&lt;/li&gt;
  &lt;li&gt;Schedule regular sessions for writing.&lt;/li&gt;
  &lt;li&gt;Writing means writing, not editing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Á°ÆÂÆûÊòØËøôÊ†∑„ÄÇÊàëÁªèÂ∏∏ËßâÂæóÂæóÁ≠âÊàëÊâÄÊúâÊñπÊ≥ïÈÉΩÂÆåÂÖ®ÊêûÊáÇ‰∫ÜÔºåÊØè‰∏Ä‰∏™ÁªÜËäÇÈÉΩËµ∞ÈÄö‰∫ÜÔºåÊâçËÉΩÂä®ÊâãÂºÄÂßãÂÜôÔºõËÄå‰∏îÂõ†‰∏∫ÁªèÂ∏∏ÊòØÂú®Áü≠Êó∂Èó¥ÂÜÖÂÜôÂ§ßÈáèÁöÑÂÜÖÂÆπÔºåÂÜôÂÆåÊÄª‰ºöÊÑüËßâÂæàÁ¥ØÔºõÊúâÁöÑÊó∂ÂÄô‰πü‰ºöÂèëÁé∞Ëä±‰∫ÜÊó∂Èó¥Âç¥Âè™ÂÜô‰∫ÜÂá†Âè•ËØùÔºåÂéüÊù•ÊòØÊó∂Èó¥ÈÉΩÁî®ÂéªÂÅöÁºñËæë$\LaTeX$Á¨¶Âè∑„ÄÅÊèíÂÖ•ÊñáÁåÆËøôÁßç‰∫ã‰∫Ü„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊâÄ‰ª•ÊàëÁªôËá™Â∑±ÂÆö‰∫Ü‰∏™Êñ∞ÁöÑÂÜô‰ΩúÊ®°ÂºèÔºöÊó©‰∏äËµ∑Â∫äÂêé‰∏ìÊ≥®ÂÜô‰∏ÄÂ∞èÊó∂ÁöÑÂÜÖÂÆπÔºåÂÜÖÂÆπÊòØÊé•ÁùÄÊò®Â§©ÂÜôÁöÑÔºåÂπ∂‰∏îÂè™ÊúâÂÜô‰ΩúÔºõÁôΩÂ§©ÁöÑÊó∂ÂÄôÂÜçÊù•ÊÖ¢ÊÖ¢‰øÆÊîπÂ≠óÁ¨¶ÔºåÊ∑ªÂä†ÊñáÁåÆÂíåÊ£ÄÊü•ÈîôÂà´Â≠ó‰πãÁ±ªÁöÑÁºñËæëÂ∑•‰Ωú„ÄÇËøôÊ†∑ÁöÑÊ®°ÂºèÂÜô‰∫ÜÂ§ßÂçäÊúàÔºåÂèëÁé∞ÂÜô‰ΩúËøõÂ∫¶Á°ÆÂÆûÂæàÂø´ÔºàÁõ∏ÂØπ‰ª•ÂâçÔºâÔºå‰∏ÄÁÇπ‰∏ÄÁÇπÊää‰πãÂâçÂ§ßÂ§öÂè™ÊòØÂÅúÁïôÂú®ËçâÁ®øÊú¨Âíå‰ª£Á†Å‰∏äÁöÑÂ∑•‰ΩúÈÉΩÁ≥ªÁªüÂú∞ÂÜôËøõ‰∫ÜÁ¨îËÆ∞ÔºåÁÑ∂ÂêéÂèàÊ†πÊçÆÁ¨îËÆ∞ÂÜô‰∫ÜÊñáÁ´†ÁöÑÂàùÁ®øÔºåÂú®ÂÜôÁöÑËøáÁ®ã‰∏≠‰πüÊääÂæàÂ§öÁªÜËäÇÁöÑ‰∏úË•øÊçãÊ∏Ö‰∫Ü‰∏çÂ∞ë„ÄÇ‰∏çËøáÔºåÊàëËßâÂæóÁî®ËøôÊ†∑ÁöÑÂÜô‰ΩúÊ®°ÂºèÊúâ‰∏Ä‰∏™Ë¶ÅÊ±ÇÔºåÈÇ£Â∞±ÊòØÂæóÊèêÂâçÊâìÂ•ΩËçâÁ®øÔºöÂÖ∑‰ΩìÂÜÖÂÆπ„ÄÅÊèêÁ∫≤„ÄÅÊØè‰∏ÄÈÉ®ÂàÜÁöÑÊ†∏ÂøÉÂÜÖÂÆπÁ≠âËá™Â∑±ÂæóÁü•ÈÅìÔºå‰∏çÁÑ∂ÁöÑËØùÔºåÊó©‰∏ä‰∏ÄÂ∞èÊó∂ÂæàÈöæÂÜôÂá∫‰∏úË•øÔºåÂç≥‰ΩøÂÜôÂá∫Êù•‰∫ÜÔºåÂ§ßÈÉ®ÂàÜÂÜÖÂÆπÊàëÂêéÊù•ÈÉΩÊÉ≥ÊîπÊéâÈáçÂÜô„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏∫‰ªÄ‰πàË¶ÅÊääÂ≠¶ËøáÁöÑ‰∏úË•øÂÉè‰π¶‰∏ÄÊ†∑ÂÜôËøõÁ¨îËÆ∞ÈáåÂë¢ÔºüÂõ†‰∏∫ÂâçÈù¢ÂÜôÂà∞ÁöÑÂÜô‰ΩúÊ®°ÂºèÂíåÈúÄË¶ÅÂÜôÊñáÁ´†ÁöÑÂéüÂõ†ÔºåÂºÄÂßãÂØªÊÄùÁùÄËÉΩ‰∏çËÉΩÊääËä±Êó∂Èó¥ÊêûÊáÇÁöÑ‰∏úË•ø‰ª•Á≥ªÁªüÁöÑ„ÄÅÊàëËá™Â∑±ËÉΩÊòéÁôΩÁöÑÊñπÂºèÂÜôËøõÁ¨îËÆ∞ÔºüÂΩìÁÑ∂ÂèØ‰ª•Ôºå‰ΩÜÈóÆÈ¢òÊòØ‰∏∫‰ªÄ‰πàË¶ÅËøô‰πàÂÅöÔºüÊØïÁ´üÂ≠¶ÊúØÊñáÁ´†ÊâÄÈúÄÁöÑÂÜÖÂÆπÂíåË°åÊñáÈ£éÊ†ºË∑üÁ¨îËÆ∞ËøòÊòØÂæà‰∏ç‰∏ÄÊ†∑ÁöÑÔºåËøôÂ∞±ÊÑèÂë≥ÁùÄÊõ¥Â§öÁöÑÂÜô‰ΩúÂ∑•‰ΩúÈáèÔºå$\LaTeX$ÂÜôËµ∑Êù•‰πüÊ≤°ÈÇ£‰πàËΩªÊùæ„ÄÇÂÖ∂ÂÆûËøô‰∫ãÂÑøÂâç‰∏§Âπ¥‰πüÊÉ≥ËøáÔºå‰∏ªË¶ÅËøòÊòØÊÉ≥Âª∫Á´ã‰∏Ä‰∏™Â±û‰∫éËá™Â∑±ÁöÑÁü•ËØÜ‰ΩìÁ≥ªÂπ∂ËÉΩ‰∏çÊñ≠‰øÆÊîπÂíåÊâ©ÂÖÖÂÆÉÂêß„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂâçÊÆµÊó∂Èó¥Âõ†‰∏∫ÂÜô‚Äú&lt;a href=&quot;https://yangxiaozhou.github.io/data/2020/05/17/francis-galton.html&quot;&gt;Francis Galton: Áª¥Â§öÂà©‰∫öÊó∂‰ª£ÁöÑÂçöÂ≠¶ÂÆ∂‰∏é‰ªñËßÇÂØüÂà∞ÁöÑÂ•áÂ¶ô‰∏ñÁïå&lt;/a&gt;‚ÄùÔºåÂú®Ë±ÜÁì£ÊêúÁ¥¢ÂÖ≥‰∫éGaltonÁöÑÊñáÁ´†ÔºåËØªÂà∞‰∫Ü‰∫éÊ∑ºÁöÑÂçöÂÆ¢ÔºåËµ∑ÂàùÊÉäÂèπ‰∫é‰ªñÁöÑÁ¨îËß¶Èó¥Ê∏óÈÄèÁùÄÁîüÁâ©„ÄÅÊï∞Â≠¶„ÄÅÁªüËÆ°ÂíåÁºñÁ®ãÁ≠âÊñπÈù¢Â§öÂπ¥ÁßØÁ¥ØÁöÑÁü•ËØÜ‰ª•ÂèäÂØπ‰∫éÁßëÁ†îÂíåËøô‰∏™‰∏ñÁïåÁöÑ‰∏Ä‰∫õÁã¨Âà∞ÁöÑËßÅËß£„ÄÇ‰∫ÜËß£‰πãÂêéÊâçÂèëÁé∞Ëøô‰ΩçÊù•Ëá™‰∏≠ÁßëÈô¢ÁöÑÂçöÂ£´Âú®ÁéØÂ¢ÉÁßëÂ≠¶ÊñπÈù¢ÂÅöÁ†îÁ©∂Ôºå16Âπ¥‰ªé‰∏≠ÁßëÈô¢ÊØï‰∏öÁöÑ‰ªñÁÆóÊòØË∂ÖÂì•ÔºàÊàëÁöÑco-authorÔºâÂ∞è‰∏âÂ±äÁöÑÂ∏àÂºü„ÄÇ‰ªéÊó∂Èó¥Á∫ø‰∏äÁúãÂæóÂà∞ÔºåËØªÂçö‰ª•Êù•Ëøô‰πàÂ§öÂπ¥Ôºå‰∏ÄÁõ¥ÊúâÂú®ÂçöÂÆ¢‰∏äÂÜôËÆ®ËÆ∫ÂêÑÁßçÈóÆÈ¢òÁöÑÂçöÊñáÔºåÂ§ßÂ§öÊòØÂü∫‰∫éËßÇÂØü„ÄÅÂ≠¶‰π†‰∏éÁ†îÁ©∂ÁöÑËÆÆÈ¢ò„ÄÇÂêåÊó∂Ëøô‰ΩçÂì•‰ª¨ÁÉ≠Ë°∑‰∫é‰∏äÁΩëËØæËØªÊïôÊùêÔºåÊõ¥Áà±ÂÅöÁ¨îËÆ∞Êï¥ÁêÜÁü•ËØÜÔºåÁõÆÂâç‰ªñÁöÑÂçöÂÆ¢‰∏äÊúâËøô‰πàÂ§öÂπ¥ÁßØÊîí‰∏ãÊù•ÁöÑÁü•ËØÜÁ¨îËÆ∞ÁöÑ&lt;a href=&quot;http://yufree.github.io/notes/index.html&quot;&gt;Ê±áÁºñ&lt;/a&gt;ÔºåÊúÄÂêéÊàëÂèëÁé∞Ôºå‰ªñËøòÊòØÁªüËÆ°‰πãÈÉΩÁöÑÁºñËæëÈÉ®‰∏ªÁºñ„ÄÇÂéâÂÆ≥ÔºÅ&lt;/p&gt;

&lt;p&gt;Â§ßÊ¶ÇÂ∞±ÊòØÊÉ≥Â∞ùËØïÂÅöËøô‰∏™‰∫ãÂÑøÂêßÔºå‰∏Ä‰∏™‰∏Ä‰∏™ÂØπÂ≠¶ËøáÁöÑ‰∏úË•øÂÅöÁ≥ªÁªüÊÄßÁöÑÊï¥ÁêÜÔºåÊ¢≥ÁêÜÊ°ÜÊû∂ÁÑ∂ÂêéÂÜôÊàêÁõ∏ÂØπÂÆåÊï¥ÁöÑÁ¨îËÆ∞ÔºåÂØπÊ°ÜÊû∂ÈáåÁöÑÊØè‰∏™ÈÉ®ÂàÜÈÄêÊ∏êÂΩ¢ÊàêËá™Â∑±ÁöÑÁêÜËß£ÔºõÂú®‰πãÂêéÁöÑÊâ©ÂÖÖËøáÁ®ã‰∏≠Êó¢ËÉΩÊâæÂà∞Êñ∞Áü•ËØÜÂú®Ê°ÜÊû∂‰∏≠ÁöÑ‰ΩçÁΩÆÔºå‰πüËÉΩ‰∏çÊñ≠Êõ¥Êñ∞Ëá™Â∑±ÁöÑÊ°ÜÊû∂ÂíåÁêÜËß£„ÄÇ&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Francis Galton: Áª¥Â§öÂà©‰∫öÊó∂‰ª£ÁöÑÂçöÂ≠¶ÂÆ∂‰∏é‰ªñËßÇÂØüÂà∞ÁöÑÂ•áÂ¶ô‰∏ñÁïå</title>
   <link href="http://localhost:4000/data/2020/05/17/francis-galton.html"/>
   <updated>2020-05-17T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/05/17/francis-galton</id>
   <content type="html">&lt;p&gt;Âë®Êú´ËØªAeonÁöÑ‰∏ÄÁØáÊñáÁ´†Ôºö&lt;a href=&quot;https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;amp;utm_medium=email&amp;amp;utm_term=0_411a82e59d-f7c118f081-69607277&quot;&gt;Algorithms associating appearance and criminality have a dark past&lt;/a&gt;ÔºåËÆ≤Áé∞Âú®ÊúâÁ†îÁ©∂‰∫∫ÂëòÁî®Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÈÄöËøá‰∫∫ËÑ∏Êù•Âà§Êñ≠Êüê‰∫∫ÁäØÁΩ™ÁöÑÂá†Áéá„ÄÇÊñá‰∏≠ËÆ≤Âà∞ËøôÁßç‰ªé‰∫∫Â§ñË°®ÊèêÂèñÈ¢ÑËßÅÊÄßÁâπÂæÅÁöÑÂ∞ùËØïÔºåÂú®ÁäØÁΩ™Â≠¶ÂéÜÂè≤‰∏äÂπ∂‰∏çÊñ∞Â•áÔºå19‰∏ñÁ∫™ÁöÑÊÑèÂ§ßÂà©ÁäØÁΩ™Â≠¶ÂÆ∂Cesare LombrosoËÆ§‰∏∫ÁΩ™ÁäØÁöÑËÑ∏ÈÉ®ÊúâÁã¨ÁâπÁöÑÊ†∑Ë≤åÔºöÁ™ÅÂá∫ÁöÑÂâçÈ¢ù„ÄÅÈπ∞ÂûãÈºªÊ¢ÅÔºõËÄå18‰∏ñÁ∫™ÁöÑFrancis GaltonÂàôÂ∞ùËØïÂõûÁ≠î‰∏Ä‰∏™Êõ¥ÂπøÊ≥õÁöÑÈóÆÈ¢òÔºö‰∫∫ÁöÑÂ§ñË°®Ë∑ü‰ªñÊàñÂ•πÁöÑÂÅ•Â∫∑Áä∂ÂÜµ„ÄÅÁäØÁΩ™ÂÄæÂêë„ÄÅÊô∫ÂäõÁ≠âÁ≠âÊúâÂÖ≥Á≥ªÂêóÔºüÊàñËÄÖËØ¥Ôºå‰∫∫ÁöÑÂü∫Âõ†ÊòØÂê¶ÂÜ≥ÂÆö‰∫ÜÂÅ•Â∫∑„ÄÅË°å‰∏∫„ÄÅÊô∫ÂäõÂíåÁ´û‰∫âÂäõÔºü&lt;/p&gt;

&lt;h3 id=&quot;francis-galtonÊòØË∞Å&quot;&gt;Francis GaltonÊòØË∞ÅÔºü&lt;/h3&gt;
&lt;p&gt;ËøôÂêçÂ≠óÁúãËµ∑Êù•ÊúâÁÇπÁúºÁÜüÔºåÊàëÈöêÁ∫¶ËÆ∞ÂæóÂú®ËÄÅÊùøÁöÑ‰∏ÄÈó®ForecastingÁªüËÆ°ËØæ‰∏äÂê¨Âà∞Ëøá„ÄÇ‰ªîÁªÜ‰∏ÄÊÉ≥ÔºåÂØπÔºåÂú®Á∫øÊÄßÂõûÂΩíÁöÑÈÉ®ÂàÜÔºåËÄÅÊùø‰∏äËØæ‰∏ìÈó®‰ªãÁªç‰∫Ü‰ªñ„ÄÇSir Francis GaltonÔºåÂßìGaltonÔºåÂêçFrancisÔºå‰ΩÜÂΩìÊèêÂà∞‰ªñÊó∂ÔºåÂá∫‰∫éÁ§º‰ª™Ôºå‰Ω†ÂæóÂä†‰∏™SirÔºåÂõ†‰∏∫‰ªñÂú®1909Âπ¥Ë¢´Ëã±ÂõΩÂ•≥ÁéãÊéà‰∫à‰∫ÜÈ™ëÂ£´Áàµ‰Ωç„ÄÇ‰∏∫‰ªÄ‰πàÂú®ËÆ≤Á∫øÊÄßÂõûÂΩíÁöÑÊó∂ÂÄôË¶Å‰ªãÁªç‰ªñÂë¢ÔºüÂõ†‰∏∫‰ªñ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™‰∫∫ÔºåËßÇÂØüÂπ∂ËÆ∞ÂΩï‰∫ÜËøôÊ†∑‰∏ÄÁßçÁé∞Ë±°&lt;sup id=&quot;fnref:Galton_heights&quot;&gt;&lt;a href=&quot;#fn:Galton_heights&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;ÔºöÂπ≥ÂùáË∫´È´òÂæàÈ´òÁöÑÁà∂ÊØçÔºåÂæÄÂæÄ‰ºöÊúâË∫´È´òÊõ¥Êé•ËøëÊôÆÈÄöÁöÑÂ≠©Â≠êÔºõËÄåÂπ≥ÂùáË∫´È´òÂÅè‰ΩéÁöÑÁà∂ÊØçÁöÑÂ≠©Â≠êÔºåÊàêÂπ¥ÂêéÈÄöÂ∏∏ÊúâÁùÄÊõ¥Êé•ËøëÊôÆÈÄö‰∫∫ÁöÑË∫´È´ò„ÄÇ
‰∏ãÂõæÊòØ&lt;a href=&quot;https://www.ams.org/journals/bull/2013-50-01/S0273-0979-2012-01374-5/S0273-0979-2012-01374-5.pdf&quot;&gt;Bradley Efron&lt;/a&gt;Ê†πÊçÆGaltonÂΩìÊó∂Êî∂ÈõÜÂà∞ÁöÑÁà∂ÊØçÂíåÂ≠©Â≠êÁöÑË∫´È´òÊï∞ÊçÆÈáçÊñ∞Âà∂ÁöÑÂõæÔºåÂÆåÁæéÂú∞Â±ïÁé∞‰∫ÜÊàë‰ª¨Áé∞Âú®ÊâÄÁü•ÈÅìÁöÑBivariate normal distribution„ÄÇ
&lt;img src=&quot;/assets/francis-galton/regression_to_mean.png&quot; alt=&quot;regression_to_mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‰ªñÊääËøôÁßçÁé∞Ë±°Áß∞‰∏∫&lt;a href=&quot;https://www.jstor.org/stable/2841583&quot;&gt;regression towards mediocrity&lt;/a&gt;ÔºåÁé∞Âú®ÈÄöÂ∏∏Âè´ÂÅöregression toward the meanÔºå‰∏≠ÊñáË≤å‰ººÂè´‚ÄúÂêëÂùáÊï∞ÂõûÂΩí‚Äù„ÄÇÂêåÊ†∑ÁöÑÁé∞Ë±°ÔºåÊàë‰ª¨Âú®ÁîüÊ¥ª‰∏≠ÂæàÂ§öÂú∞ÊñπÈÉΩËÉΩËßÇÂØüÂà∞ÔºöÂõ†‰∏∫ËøêÊ∞îËÄåÊäº‰∏≠È¢òÁõÆÁöÑÂ≠¶ÁîüËÄÉÂá∫‰∫ÜÈ´òÂàÜÔºå‰∏ã‰∏ÄÊ¨°ËÄÉËØïÁöÑÊàêÁª©Âç¥Ê≤°ÈÇ£‰πàÁ™ÅÂá∫ÔºõËøûÁª≠Êäï‰∏≠‰∏â‰∏™‰∏âÂàÜÁêÉÁöÑÊúãÂèãÔºå‰∏ã‰∏™ÁêÉÂæÄÂæÄ‚ÄúÂÆπÊòì‚ÄùÂ§±ÊâãÔºõÊàë‰∏äÂë®ÂÅö&lt;a href=&quot;https://yangxiaozhou.github.io/learning/2019/01/01/recipe.html#%E6%B2%B9%E6%B3%BC%E7%8C%AA%E6%89%8B&quot;&gt;Ê≤πÊ≥ºÁå™Êâã&lt;/a&gt;Êó∂ÂêÑÁßçË∞ÉÊñôÊãøÊçèÂæóÂæàÂ•ΩÔºåÂë≥ÈÅìË∂ÖÊ£íÔºåËøôÂë®ÂÜçÂÅö‰∏ÄÊ¨°ÔºåÂ§ßÊ¶ÇÁéáÂë≥ÈÅì‰ºöÊØîËæÉÊôÆÈÄöü§∑‚Äç‚ôÇÔ∏è„ÄÇ&lt;/p&gt;

&lt;p&gt;Á¨¶ÂêàËøôÂéüÂàôÁöÑÁé∞Ë±°Ôºå‰ªñ‰ª¨Êúâ‰∏Ä‰∏™ÂÖ±ÈÄöÁÇπÔºö‰ªñ‰ª¨ÁöÑÁªìÊûúÂæÄÂæÄÂÆåÂÖ®ÊàñÈÉ®ÂàÜÁî±ÈöèÊú∫Âõ†Á¥†ÂÜ≥ÂÆöÔºåËÄåÈöèÊú∫Âõ†Á¥†ÁöÑÂΩ±ÂìçÂæÄÂæÄÁ¨¶Âêà‰ª•0‰∏∫‰∏≠ÂøÉÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÔºàÊó∂Â•ΩÊó∂ÂùèÔºâ„ÄÇÊØîÂ¶ÇËØ¥Ôºå‰∏âÂàÜÁêÉËøõÊàñ‰∏çËøõÔºåÊúâÊäïÊâãËÉΩÂäõÁöÑÂΩ±Âìç‰ΩÜ‰πüÊúâËøêÊ∞îÁöÑÊàêÂàÜÔºõÊàëÂÅöÁöÑÊüêÈÅìËèúÁöÑÂë≥ÈÅìÔºåÂèñÂÜ≥‰∫é‰∏ãÂé®ËÉΩÂäõÔºå‰ΩÜÊàëÁöÑ‰∏ìÂøÉÁ®ãÂ∫¶„ÄÅÊâãÊäñÁ®ãÂ∫¶‰ª•ÂèäÂøÉÊÉÖÁ≠âÂá†‰πéÈöèÊú∫ÁöÑÂõ†Á¥†‰πü‰ºöÊúâÊâÄÂΩ±Âìç„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂÅáËÆæÊüê‰∏ÄÂ§©ÊàëË∂ÖÁ∫ßËµ∞ËøêÔºåÂÅöÂá∫‰∫ÜËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂ•ΩÂêÉÁöÑ‰∏ÄÈÅìËèúÔºåËøôÁßç‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéáÊòØÂæàÂ∞èÁöÑÔºàÂæóÂà∞Ê≠£ÊÄÅÂàÜÂ∏É‰∏äÁöÑÊûÅÂ§ßÂÄºÊàñÊûÅÂ∞èÂÄºÁöÑÊ¶ÇÁéáÔºâ„ÄÇ‰∏ã‰∏ÄÊ¨°ÂÅöÔºåÂ§ßÊ¶ÇÁéáÊàë‰ºöÊ≠£Â∏∏ÂèëÊå•ÔºåËèúÁöÑÂë≥ÈÅì‰πüÊ≤°‰∏äÊ¨°Â•ΩÔºàÂèñÂà∞‰∫ÜÊ≠£ÊÄÅÂàÜÂ∏É‰∏ä0Âë®Âõ¥ÁöÑÊüê‰∏™ÂÄºÔºâ„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊÉ≥Ë±°ËøôÊ†∑‰∏ÄÁßçÊÉÖÂÜµÔºöÊúãÂèãÂú®ÊàëÊê¨Êñ∞ÂÆ∂ÁöÑÊó∂ÂÄôÊù•ÂÆ∂ÈáåÂêÉÈ•≠ÔºåÂàöÂ•ΩÁ¢∞Âà∞ÊàëÂâçÈù¢ËØ¥ÁöÑË∂ÖÂ∏∏ÂèëÊå•ÔºåÈÉΩËØ¥ÂÅöÁöÑÁå™ÊâãÂ•ΩÂêÉÔºÅËøá‰∫ÜÂá†‰∏™ÊúàÔºåÂÆ∂ÈáåËÅö‰ºöÔºåÂ∫îÊúãÂèãÂº∫ÁÉàË¶ÅÊ±ÇÔºåÂÜçÊ¨°ÂÅöÂá∫‰∏ÄÁõòÁå™ÊâãÔºå‰∏çËøáËøôÊ¨°ÊòØÊ≠£Â∏∏ÂèëÊå•„ÄÇÊúãÂèãÂêÉÂêéÂõûÂøÜËµ∑‰πãÂâçÔºåËØÑËÆ∫Âà∞Ôºö‚ÄúÊ∞¥Âπ≥‰∏ãÈôç‰∫ÜÂëÄÔºÅ‚Äù ÊàëÂÜ§‰∏çÂÜ§Ôºü ËøôÊ†∑ÁöÑÂÜ§ÊûâÊàë‰ª¨ÁîüÊ¥ª‰∏≠ËøòÁúü‰∏çÂ∞ëÔºå‰ª•Ëá≥‰∫éÂÆÉÊúâ‰∏™‰∏ìÈó®ÁöÑÁß∞ÂëºÔºöRegression fallacyÔºå‰∏≠ÊñáÂè´‚ÄúÂõûÂΩíË∞¨ËØØ‚Äù„ÄÇDaniel KahnemanËÆ≤Ëøá‰∫≤Ë∫´ÁªèÂéÜÁöÑËøôÊ†∑&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292229/&quot;&gt;‰∏Ä‰∏™‰æãÂ≠ê&lt;/a&gt;Ôºö‰ªñÊúâ‰∏ÄÊ¨°ÁªôÈ£ûË°åÂëòÂ≠¶Ê†°ÂÅöÂüπËÆ≠ÔºåÊèêÂà∞‰∫ÜË°®Êâ¨ËÉΩ‰ΩøÂ≠¶ÂëòÂèòÂæóÊõ¥‰ºòÁßÄ„ÄÇ‰∏ãÈù¢ÁöÑ‰∏Ä‰∏™ÊïôÂÆò‰∏çÂêåÊÑè‰∫ÜÔºåËØ¥‰ªñÊØèÊ¨°‰∏ÄÂ§∏ÂÆåÈôçËêΩÂÅöÂæóÁÆÄÁõ¥ÂÆåÁæéÁöÑÂ≠¶ÂëòÔºå‰∏ã‰∏ÄÊ¨°‰∏ÄÂÆöÂÅöÂæóÊ≤°ÈÇ£‰πàÂ•ΩÔºåËÄåÂàöË¢´‰ªñÈ™ÇËøáÁöÑÂ≠¶ÂëòÔºåÈ©¨‰∏äÂ∞±ËÉΩÁúãÂà∞ÊèêÂçá„ÄÇÂê¨‰∫ÜÊïôÂÆòÁöÑÊäóËÆÆÔºåKahnemanÂΩì‰∏ãÊúâ‰∫Ü‰∏Ä‰∏™eureka momentÔºå‰ªñËØ¥ÈÅìÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÂõûÂΩíË∞¨ËØØÂèØ‰ª•Áî®Êï∞Â≠¶ËØÅÊòéÔºåÂÅáËÆæ‰∏§‰∏™ÂèòÈáè‰ª•bivariate normal distributionÂàÜÂ∏ÉÔºåÂè™Ë¶Å‰ªñ‰ª¨ÁöÑcorrelationÂ∞è‰∫é1ÔºåÂ∞±‰ºöÊúâÂõûÂΩíË∞¨ËØØÂá∫Áé∞ÔºåÂØπËØÅÊòéÊÑüÂÖ¥Ë∂£ÁöÑÊúãÂèãÂèØ‰ª•Áúã&lt;a href=&quot;https://en.wikipedia.org/wiki/Regression_toward_the_mean&quot;&gt;Áª¥Âü∫&lt;/a&gt;„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;Âπ¥Â∫¶ÂÖ¨Áâõ‰ΩìÈáçÁ´ûÁåú&quot;&gt;Âπ¥Â∫¶ÂÖ¨Áâõ‰ΩìÈáçÁ´ûÁåú&lt;/h3&gt;
&lt;p&gt;ËÆ©Êàë‰ª¨ÂõûÂà∞ËßÇÂØüÂ∞èÂ§©ÊâçGalton„ÄÇ1907Âπ¥‰∏âÊúàÁöÑËá™ÁÑ∂ÊùÇÂøó‰∏äÂàäÁôª‰∫Ü‰ªñ‰∏ÄÁØáÁØáÂπÖÂè™Êúâ‰∏ÄÈ°µÁöÑ&lt;a href=&quot;https://www.nature.com/articles/075450a0&quot;&gt;Êù•‰ø°&lt;/a&gt;ÔºåÂêç‰∏∫ÔºöVox PopuliÔºåÁõ¥ËØë‰∏∫‚ÄúÊ∞ë‰ºóÁöÑÂ£∞Èü≥‚ÄùÔºåÁé∞Âú®ÊåáÂ§ßÂ§öÊï∞‰∫∫ÁöÑÊÑèËßÅ„ÄÇ‰ΩèÂú®Ëã±ÂõΩPlymouthÁöÑ‰ªñÔºåÊ≥®ÊÑèÂà∞‰∫ÜÂÆ∂ÈôÑËøëÁöÑÈïáÂ≠ê‰∏äÊØèÂπ¥ÈÉΩÊúâ‰∏æÂäûËøôÊ†∑‰∏ÄÁßçÂÆ∂Á¶Ω‰ΩìÈáçÁ´ûÁåúÊ¥ªÂä®Ôºö‰∏ªÂäûÊñπÊãâ‰∏ÄÂ§¥ÁâõÂá∫Êù•ÔºåÂèÇ‰∏éÁ´ûÁåúÁöÑÊú¨Âú∞ÂÜúÂ§´„ÄÅÂ±†Â§´Á≠âÊÑüÂÖ¥Ë∂£‰∏îÊúâÁªèÈ™åËÄÖÂØπÁâõËøõË°åËØÑ‰º∞ÔºåÂπ∂Â∞Ü‰ªñËÆ§‰∏∫ËøôÂ§¥ÁâõË¢´ÂÆ∞ÊùÄÊ¥óÂáÄ‰πãÂêéÁöÑ‰ΩìÈáçÊèê‰∫§‰∏äÂéª„ÄÇÊú¨ÁùÄÂØπÂ§ß‰ºóÊô∫ÊÖßÁöÑÁßëÂ≠¶Á†îÁ©∂ÊÄÅÂ∫¶Ôºå‰ªñÈÄöËøáÊüêÁßçÊñπÂºèËé∑Âæó‰∫Ü‰∏ÄÊ¨°Á´ûÁåúÊØîËµõ‰∏≠ÁöÑÊï∞ÊçÆÔºöÁâõÁöÑÁúüÂÆû‰ΩìÈáç‰ª•Âèä787‰∏™Á´ûÁåúËÄÖÁöÑ‰º∞ËÆ°„ÄÇ&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 100%;&quot;&gt;&lt;img src=&quot;/assets/francis-galton/stock_show.jpg&quot; alt=&quot;stock_show&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‰ªñÊääÊèê‰∫§ÁöÑÊâÄÊúâÁ´ûÁåú‰ΩìÈáç‰ªéÂ∞èÂà∞Â§ßÊéíÂàóÂºÄÔºåÂèëÁé∞‰∏≠‰ΩçÊï∞Ôºà‰∏ÄÂçäÁöÑÊï∞ÊØîÂÆÉ‰ΩéÔºå‰∏ÄÂçäÊØîÂÆÉÈ´òÔºå‚Äùmedian‚Äù‰∏ÄËØçÂ∞±ÊòØ‰ªñÁªôÂèñÁöÑÔºâÊòØ1207Á£ÖÔºåËÄåÈÇ£Â§¥ÁâõÁöÑÁúüÂÆûÂáÄ‰ΩìÈáçÊòØ1198Á£ÖÔºå‰πüÂ∞±ÊòØËØ¥ÔºåÊ∞ë‰ºóÁöÑÂà§Êñ≠Âú®ËøôÈáåË∑üÁúüÂÆûÂÄºÂè™Â∑Æ‰∫Ü0.8%ÔºÅ&lt;sup id=&quot;fnref:correction&quot;&gt;&lt;a href=&quot;#fn:correction&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
Âú®ÈÇ£‰∏™Á∫øÊÄßÂõûÂΩíËøò‰∏çÊòØÊâÄÊúâÊï∞ÊçÆÂàÜÊûêËØæÁ®ãÁöÑÁ¨¨‰∏ÄËäÇËØæÔºåÊï∞ÊçÆÁßëÂ≠¶‰πüËøò‰∏çÊòØ‰∏ÄÁßçËÅå‰∏öÁöÑÊó∂ÂÄôÔºåGalton‰ªé787‰∏™Á´ûÁåú‰ΩìÈáç‰∏≠ÈÄöËøáÁÆÄÂçïÁöÑÊâãÁÆóÁúãÂà∞‰∫Ü‰ª•Âπ≥ÂùáÂÄºÊàñ‰∏≠‰ΩçÊï∞ÂØπÁúüÂÆûÂÄºËøõË°å‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁé∞Âú®Êàë‰ª¨Áü•ÈÅì‰∫ÜÔºåsample mean is an unbiased estimator of the true population mean„ÄÇ&lt;/p&gt;

&lt;p&gt;GaltonÁöÑËßÇÂØüÊ≤°ÊúâÂÅúÂú®‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß‰∏äÔºå‰ªñËøòÊÉ≥Áü•ÈÅìÔºåÊØè‰∏™‰∫∫‰º∞ËÆ°ÁöÑËØØÂ∑ÆÊúâÂ§öÂ§ß„ÄÇ‰ªñÈöèÂç≥ÊääÊâÄÊúâ‰º∞ËÆ°‰∏é‰∏≠‰ΩçÊï∞ÁöÑÂÅèÂ∑ÆÁîª‰∫ÜÂá∫Êù•Ôºå‰ªñÂèëÁé∞ÔºåÊØè‰∏Ä‰∏™ÊúâÁªèÈ™åÁöÑ‚ÄúËÇâÁúºÊµã‰ΩìÈáçËÄÖ‚ÄùÊâÄÂÅöÁöÑ‰º∞ËÆ°Ôºå‰ªé‰Ωé‰º∞ÁöÑÂà∞È´ò‰º∞ÁöÑÔºå‰∏ÄÁ≥ªÂàóÁöÑÂÅèÂ∑Æ‰∏éÊ≠£ÊÄÅÂàÜÂ∏ÉÊûÅ‰∏∫Áõ∏‰ºº„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂ¶ÇÊûúÊääÁúüÂÆûÂáÄ‰ΩìÈáçÁúãÂÅöÊòØËøô‰∏™ÈááÊ†∑ÂàÜÂ∏ÉÁöÑmeanÔºåÈÇ£‰ªªÊÑè‰∏Ä‰∏™ÂèÇËµõËÄÖÔºàÊúâÁªèÈ™åÁöÑÔºâÊù•‰º∞ËÆ°Ôºå‰ªñÁöÑ‰º∞ËÆ°ÂÄºÂ∞ÜÊòØ‰ª•ÁúüÂÆûÂáÄ‰ΩìÈáç‰∏∫‰∏≠ÂøÉÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉËÄåÂàÜÂ∏ÉÁùÄÁöÑÔºàÁªïÂè£ÔºÅÔºâ„ÄÇLet‚Äôs try again. The estimate by any pair of trained eyes is distributed normally around the true dressed weight of the ox. ËøôÈáåÊàë‰ª¨ÂæóÊèê‰∏Ä‰∏™Êó†Êï∞Áé∞‰ª£ÁßëÂ≠¶‰æùËµñÁöÑÁêÜËÆ∫ÔºöCentral limit theorem (CLT)„ÄÇÂØπÔºåÂ∞±ÊòØÈÇ£‰∏™ÂèØ‰ª•Ëß£Èáä‰∏∫‰ªÄ‰πàÊ≠£ÊÄÅÂàÜÂ∏ÉÂú®Áé∞ÂÆûÁîüÊ¥ª‰∏≠Â¶ÇÊ≠§ÊôÆÈÅçÁöÑÁêÜËÆ∫„ÄÇÂõ†‰∏∫CLTÔºåÊàë‰ª¨Áé∞Âú®Á°ÆÂàáÂú∞Áü•ÈÅìÔºåÂΩìÊ†∑Êú¨ÈáèË∂≥Â§üÂ§ßÊó∂ÔºåÊ†∑Êú¨Âπ≥ÂùáÂÄºÂëà‰ª•ÁúüÂÆûÂÄº‰∏∫‰∏≠ÂøÉÁöÑÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇÊâÄ‰ª•Ôºå‰ªéÂπ¥Â∫¶ÂÖ¨Áâõ‰ΩìÈáçÁ´ûÁåúÁöÑÁúüÂÆûÊï∞ÊçÆ‰∏äÔºå‰ªñÔºåSir Francis GaltonÔºåÁúãÂà∞‰∫Ücentral limit theorem„ÄÇ&lt;/p&gt;

&lt;p&gt;ÈôÑ‰∏ä‰ªñÂéüÁ®øÈáåÁöÑË∑üÁêÜËÆ∫Ê≠£ÊÄÅÂàÜÂ∏ÉÂÅöÂØπÊØîÁöÑÂõæÔºåÊ®™ËΩ¥ÊòØÁôæÂàÜ‰ΩçÔºåÁ∫µËΩ¥ÊòØÂÅèÂ∑ÆÔºö
&lt;img src=&quot;/assets/francis-galton/francis-galton-the-wisdom-of-crowds.jpg&quot; alt=&quot;francis-galton-the-wisdom-of-crowds&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;galton-board‰∏éÊüèÈùíÂì•&quot;&gt;Galton Board‰∏éÊüèÈùíÂì•&lt;/h3&gt;
&lt;p&gt;GaltonÂØπ‰∫éËøôÁßçÊ≤°ÊúâÂæÅÂÖÜ‰ΩÜÂèàËøë‰πéÂÆöÂæãËà¨ÂëàÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÂÅèÂ∑ÆÈùûÂ∏∏ÁùÄËø∑Ôºå‰ªñÊääÂÅèÂ∑ÆÂëàÁé∞Âá∫Êù•ÁöÑÂõæÁß∞‰∏∫:The Curve of FrenquencyÔºå‰πüÂ∞±ÊòØÊàë‰ª¨Áé∞Âú®ÁÜüÁü•ÁöÑÊ†∑Êú¨ÂÅèÂ∑ÆÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÂõæ„ÄÇ‰∏∫‰∫ÜÂ±ïÁé∞ËøôÁßçÂÅèÂ∑ÆÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÔºàÂç≥ÔºåCLT)‰ª•ÂèäÂâçÈù¢ÊèêÂà∞ÁöÑÂõûÂΩíË∞¨ËØØÔºåGaltonËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰ª§‰∫∫ÊãçÊ°àÂè´ÁªùÁöÑË£ÖÁΩÆÔºöGalton BoardÔºåÁé∞Âú®‰πüÂè´bean machineÔºåÂ¶Ç‰∏ãÂõæÔºö&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 75%;&quot;&gt;&lt;img src=&quot;/assets/francis-galton/GaltonBoard.png&quot; alt=&quot;GaltonBoard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ÂÉè‰∏çÂÉèÊàë‰ª¨Â∞èÊó∂ÂÄôÂú®Ë°óÂ∑∑ÁöÑÂ∞èÂçñÈÉ®ÈáåÁé©ËøáÁöÑÂºπÁè†Êú∫ÔºüÊ≤°ÈîôÔºå‰ªñ‰ª¨ÁöÑËÉåÂêéÊòØÂêåÊ†∑ÁöÑÂéüÁêÜ„ÄÇÂÆûÈôÖ‰∏äÔºåÈ£éÈù°ÂÖ®Êó•Êú¨ÁöÑÊüèÈùíÂì•‰πüÊòØÁî®ÁöÑËøôÊ†∑ÁöÑËÆæËÆ°ÂéüÁêÜÔºöÂºπÁè†‰ªéÈ°∂ÈÉ®ËêΩ‰∏ãÔºåÁªèËøáË∑üËã•Âπ≤Â±ÇÁöÑÊíûÈíàÁöÑÊíûÂáªÔºåÊúÄÁªàÊéâËøõÊúÄ‰∏ãÈù¢‰ªéÂ∑¶Âà∞Âè≥N‰∏™Ê°∂ÂΩì‰∏≠ÁöÑÊüê‰∏™Ê°∂Èáå„ÄÇÂõ†‰∏∫Êàë‰ª¨Âπ∂‰∏çÁü•ÈÅìÂºπÁè†Âú®Ë∑üÊØè‰∏ÄÊ†πÊíûÈíàÊíûÂáª‰πãÂêéÊòØËµ∞Â∑¶ËøòÊòØËµ∞Âè≥ÔºåÊâÄ‰ª•Êüê‰∏™ÂºπÁè†ÁöÑÊúÄÁªà‰ΩçÁΩÆÂπ∂‰∏çËÉΩÊèêÂâçÁü•ÈÅìÔºài.e. ÈöèÊú∫‰∫ã‰ª∂ÔºåÈöèÊú∫Êº´Ê≠•ÔºåÈöèÊú∫ËøáÁ®ãÔºâ„ÄÇ&lt;/p&gt;

&lt;p&gt;‰ΩÜÔºÅËôΩÁÑ∂ÂçïÁã¨‰∏Ä‰∏™ÂºπÁè†ÁöÑÂéªÂêëÊó†Ê≥ïÊèêÂâçËé∑Áü•Ôºå‰ΩÜÊàë‰ª¨Âç¥ÊúâÂäûÊ≥ïÁü•ÈÅìÊüê‰∏™ÂºπÁè†ËêΩÂÖ•Êüê‰∏™Âå∫Èó¥ÁöÑÊ¶ÇÁéá„ÄÇÁ≤óÁï•Êù•ËØ¥ÔºåÂºπÁè†Âà∞ËææÊüê‰∏Ä‰∏™Ê°∂ÁöÑË∑ØÁ∫øÊï∞ÈáèÈô§‰ª•ÊâÄÊúâÂÆÉÂèØËÉΩËµ∞ÁöÑË∑ØÁ∫øÔºåÂ∞±ÊòØÂÆÉËøõÂÖ•Êüê‰∏™Ê°∂ÁöÑÊ¶ÇÁéá„ÄÇÊØîÂ¶ÇÔºå‰∏ÄÈ¢óÂºπÁè†ÊÉ≥Ë¶ÅÂà∞ËææÊúÄÂ∑¶ËæπÁöÑÂå∫Èó¥ÔºåÂÆÉÂè™Êúâ‰∏ÄÊù°Ë∑ØÂèØ‰ª•Ëµ∞Ôºö‰ªéÁ¨¨‰∏ÄÂ±ÇÂºÄÂßã‰∏ÄÁõ¥ÂæÄÂ∑¶Âºπ„ÄÇÁÆóÂá∫ÂÖ∂‰ªñÂå∫Èó¥ÁöÑË∑ØÁ∫øÊï∞ÂíåÊ¶ÇÁéáÂèØ‰ª•ÊúâÂæàÂ§öÊñπÊ≥ïÔºåÊØîÂ¶ÇÊûö‰∏æÔºàË¥πÂä≤ÔºâÊàñÁî®ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÔºà‰Ω†‰πüÂæàËÉΩËßÇÂØüÔºÅÔºâÔºå‰πüÂèØ‰ª•Ê†πÊçÆBinomial distributionÁöÑprobability mass function (pmf)ÂæóÂà∞Ôºà$n$ÊòØÊíûÈíàÁöÑÂ±ÇÊï∞Ôºå$k$ÊòØÊ°∂ÁöÑÁºñÂè∑Ôºå$p$ÊòØÂºπÁè†ÊíûÂáªÂêéÂºπÂ∑¶ÁöÑÊ¶ÇÁéáÔºâÔºö&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{Pr}(X=k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}&lt;/script&gt;

&lt;p&gt;for $k = 0, \dots, n$.&lt;/p&gt;

&lt;p&gt;ËØªÂà∞ËøôÈáåÔºå‰∫ÜËß£CLTÁöÑÊúãÂèãÊàñËÆ∏Â∑≤ÁªèÊòéÁôΩ‰∏∫‰ªÄ‰πàËøô‰∏™Galton boardÂèØ‰ª•Â±ïÁ§∫ÂëàÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÂÅèÂ∑Æ‰∫Ü„ÄÇCLTÁöÑ‰∏Ä‰∏™ÁâπÊÆäÂ∫îÁî®ÊòØËØÅÊòéÂΩìËØïÈ™åÁöÑÊ¨°Êï∞($n$)Ë∂≥Â§üÂ§ßÁöÑÊó∂ÂÄôÔºåbinomial distributionÁöÑpmf‰ºöË∑üÊ≠£ÊÄÅÂàÜÂ∏ÉÂçÅÂàÜÁõ∏‰ºº„ÄÇÊç¢Âè•ËØùËØ¥ÔºåÂΩìÊàë‰ª¨ÁöÑGalton boardË∂≥Â§üÂ§ßÔºåÂêåÊó∂Êâî‰∏ãÁöÑÂºπÁè†Ë∂≥Â§üÂ§öÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨Â∫îËØ•Â∞±ËÉΩÁúãÂà∞ÁªèÂÖ∏ÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉBell curveÔºÅGenius!&lt;/p&gt;

&lt;center&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/jiWt77xme64&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;hr /&gt;
&lt;p&gt;‰∏∫‰ªÄ‰πàÊàë‰ª¨ËØ¥Ëøô‰∏™ÂºπÁè†Êú∫‰πüËÉΩÂ±ïÁ§∫‰πãÂâçÊèêÂà∞ÁöÑÂõûÂΩíË∞¨ËØØÂë¢ÔºüÈ¶ñÂÖàÔºåËÆ©Êàë‰ª¨ÊääÂàöÊâçÈÇ£Âá†Áôæ‰∏™ÂºπÁè†ËêΩ‰∏ãÊù•‰πãÂêéÂëàÁé∞Âá∫Êù•ÁöÑÂàÜÂ∏ÉËÆ∞Âú®ËÑëÊµ∑‰∏≠„ÄÇËÆ©ÊàëÂÜçÊ¨°‰ΩøÁî®ÂÅöËèúÁöÑ‰æãÂ≠êÔºåÂÅáËÆæËêΩÂà∞ÊúÄÂè≥Á´ØÁöÑÂºπÁè†‰ª£Ë°®ÁùÄÊàëÂÅöÂá∫‰∫ÜËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂ•ΩÂêÉÁöÑ‰∏ÄÈÅìËèúÔºåÂõ†‰∏∫‰∏çÂØªÂ∏∏Âú∞Ëµ∞ËøêÔºàÂºπÁè†ÊéâÂÖ•ÊúÄÂè≥ËæπÁöÑÂá†ÁéáÈùûÂ∏∏Â∞èÔºâ„ÄÇÁé∞Âú®ÔºåÊàëÊääËøôÈ¢óÂºπÁè†ÊãøÂá∫Êù•ÔºåËÆ©ÂÆÉ‰ªéÈ°∂ÈÉ®ÂÜç‰∏ÄÊ¨°‰∏ãËêΩÔºàÂÜçÂÅö‰∏ÄÊ¨°ÂêåÊ†∑ÁöÑËèúÔºâÔºå‰Ω†ËßâÂæóÂ§ßÊ¶ÇÁéá‰ºöÊòØÊéâÂú®Âì™ÁâáÂú∞ÊñπÔºüÊúâÂ§öÂ§ßÊ¶ÇÁéáÂÜçÊ¨°Âà∞ËææÊúÄÂè≥Á´ØÔºàÂÅöÂá∫ÂêåÊ†∑È´òÊ∞¥Âπ≥ÁöÑËèúÔºâÔºü&lt;/p&gt;

&lt;p&gt;ÂëµÔºåLife!&lt;/p&gt;

&lt;p&gt;ÂØπ‰∫é‰ºóÂ§öÂºπÁè†Áúã‰ººÈöèÊú∫„ÄÅÊó†Ê≥ïÈ¢ÑÊµãÂú∞ËêΩ‰∏ãÔºåÊúÄÂêéË¢´ÊüêÁßçÈ≠îÂäõËÅöÊã¢Ôºå‰∏Ä‰∏™Êå®ÁùÄ‰∏Ä‰∏™ÔºåÈÄêÊ∏êÂëàÁé∞Âá∫Áæé‰∏ΩÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÁé∞Ë±°ÔºåGaltonËá™Â∑±ÊòØËøôÊ†∑ÊèèËø∞ÁöÑÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Order in Apparent Chaos: I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Law of Frequency of Error. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;ÁªìËØ≠&quot;&gt;ÁªìËØ≠&lt;/h3&gt;
&lt;p&gt;Francis Galton‰Ωú‰∏∫Ëã±ÂõΩÁª¥Â§öÂà©‰∫öÊó∂ÊúüÁöÑ‰∏Ä‰ΩçÂçöÂ≠¶ÂÆ∂ÔºåÁªèÂéÜÂÆûÂú®ÊòØÂ§™Ëøá‰∏∞ÂØå„ÄÇËá™ÂπºÂá∫ÁîüÂú®ÂØåË∂≥Á≤æËã±ÁöÑÂÆ∂Â∫≠Ôºå‰ªñÊòØËææÂ∞îÊñáÁöÑË°®ÂºüÔºåÂπ¥ËΩªÊó∂ÁªßÊâø‰∫ÜÁà∂‰∫≤ÁöÑÂ§ßÁ¨îÈÅó‰∫ß‰πãÂêéÂéªÈùûÊ¥≤Â§ßÈôÜÊé¢Èô©ÔºåÂõûÂõΩ‰πãÂêéÂÜôÊàêÁöÑÊ∏∏ËÆ∞Êàê‰∫ÜÁïÖÈîÄ‰π¶„ÄÇÁî®‰ªñÊïèÈîêÁöÑËßÇÂØüÂäõÂíåÂ•ΩÂ•áÂøÉÔºåGaltonÁ†îÁ©∂‰∫ÜÂæàÂ§öÈóÆÈ¢òÔºåÊúâ‰∫õÊ≤°Âï•ÂÆûÈôÖÂΩ±ÂìçÔºàÊúÄ‰Ω≥ÂàáËõãÁ≥ïÊ≥ï„ÄÅÊúÄ‰Ω≥Ê≤èËå∂Ê≥ïÔºâÔºåÊúâ‰∫õÂç¥ÊîπÂèò‰∫Ü‰ºóÂ§öÈ¢ÜÂüüÊé•‰∏ãÊù•‰∏ÄÁôæÂ§öÂπ¥ÁöÑÂèëÂ±ï„ÄÇ‰ªñÂÅö‰∫ÜÊó©ÊúüÁöÑÂõûÂΩíÂàÜÊûê„ÄÅÊèêÂá∫‰∫ÜcorrelationÁöÑÊ¶ÇÂøµ„ÄÅÂ∞ÜÁªüËÆ°Â∫îÁî®Âà∞ÈÅó‰º†Â≠¶„ÄÅÂøÉÁêÜÂ≠¶ÔºåÊï∞ÁêÜÁªüËÆ°ÊúÄÈáçË¶ÅÁöÑÂ≠¶ËÄÖ‰πã‰∏ÄKarl PearsonÊòØ‰ªñÁöÑÂ≠¶Áîü„ÄÇÂêåÊó∂Ôºå‰ªñ‰∏∫‰∫ÜÂæóÂà∞Êï∞ÊçÆÔºåÂèëÊòé‰∫ÜÈóÆÂç∑Ë∞ÉÊü•ÔºõÁ†îÁ©∂Â§©Ê∞îÔºåÂèëÊòé‰∫ÜÁ¨¨‰∏ÄÂº†Â§©Ê∞îÂú∞Âõæ„ÄÅÂºÄÂêØ‰∫ÜÂØπÊ∞îÂÄôÁöÑÁßëÂ≠¶Á†îÁ©∂ÔºõÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàËØÜÂà´ÊåáÁ∫πÁöÑÊñπÊ≥ïÔºåÂØπÂΩìÊó∂ÁöÑÊ≥ïÂåªÂ≠¶ÂÅö‰∫ÜÊé®Âä®„ÄÇÂì¶ÔºåÂØπ‰∫ÜÔºåÊ≠£Â¶ÇÊàë‰ª¨ÂºÄÂ§¥ÊâÄËØ¥Ôºå‰ªñ‰πüÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ†πÊçÆ‰∏çÂêå‰∫∫ËÑ∏ÂõæÂÉèÊèêÂèñ‚ÄúÂπ≥ÂùáÁâπÂæÅ‚ÄùÁöÑÊñπÊ≥ï„ÄÇ&lt;/p&gt;

&lt;p&gt;GaltonÊâÄËßÇÂØüÂà∞ÁöÑ‰∏ñÁïåÔºåËÆ©‰ªñÊúâ‰∫ÜÂæàÂ§öÁñëÈóÆÔºå‰ªñÂ∞ùËØïÁî®ÂêÑÁßçÊñπÊ≥ïÂéª‰∏àÈáèËøô‰∏™‰∏ñÁïåÔºåÂπ∂‰ªéÁúã‰ººÊ∑∑Ê≤åÊó†Â∫èÁöÑÁé∞Ë±°‰∏≠ÊâæÂà∞Áß©Â∫èÂíåËßÑÂæã„ÄÇÊàëÊÉäÂèπ‰∫éGaltonÁöÑËßÇÂØüÂäõ„ÄÅË∑üÈöèËá™Â∑±Â•ΩÂ•áÂøÉ‰∏çÊñ≠ÁöÑÊé¢Á¥¢‰∏éÂ∞ùËØï‰ª•ÂèäÂØπËá™Â∑±‰∏ì‰∏ö‰∏çËÆæÈôêÁöÑÊÄÅÂ∫¶„ÄÇÊñáËâ∫Â§çÂÖ¥‰∫∫ÁöÑÁ≤æÁ•ûÂä≤ÂÑøÂèØËßÅ‰∏ÄÊñë„ÄÇÂ•Ω‰∫ÜÔºå‰∏çÂ§öËØ¥‰∫ÜÔºåÊàëË¶ÅÂéªÂÖ•Êâã‰∏Ä‰∏™Galton board‰∫Ü„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊúÄÂêéÈôÑ‰∏ä‰∏Ä‰∏™ÊääGalton boardËß£ÈáäÂæóÊØîÊàëÊ∏ÖÊ•öÂæóÂ§ö„ÄÅËØôË∞êÂèàÂπΩÈªòÁöÑÂì•‰ª¨ÁöÑ&lt;a href=&quot;https://www.youtube.com/embed/UCmPmkHqHXk&quot;&gt;ËßÜÈ¢ë&lt;/a&gt;„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;Ê≥®Èáä&quot;&gt;Ê≥®Èáä&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Galton_heights&quot;&gt;
      &lt;p&gt;ËøôÈáåÊîæ‰∏äGaltonËá™Â∑±Âà∂‰ΩúÁöÑÁà∂ÊØçÂ≠©Â≠êË∫´È´òÂõûÂΩíÂõæÔºö&lt;img src=&quot;/assets/francis-galton/Galton's_correlation_diagram_1875.jpg&quot; alt=&quot;Galton_heights&quot; /&gt;¬†&lt;a href=&quot;#fnref:Galton_heights&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:correction&quot;&gt;
      &lt;p&gt;ÂêéÊù•ÁöÑÁ†îÁ©∂‰øÆÊ≠£‰∫ÜGaltonÂéüÁ®øÁöÑÊï∞ÊçÆÈîôËØØÔºåÂΩìÊó∂ÈÇ£Â§¥ÁâõÁöÑÁúüÂÆûÂáÄ‰ΩìÈáçÂ∫îËØ•ÊòØ1197ÔºåËÄå‰∏≠‰ΩçÊï∞‰º∞ËÆ°Â∫îËØ•ÊòØ1208„ÄÇÂú®ÂéüÁ®ø‰∏≠ÔºåGaltonÁî®‰∏≠‰ΩçÊï∞ËøõË°å‰∫ÜÁúüÂÆûÂÄº‰º∞ËÆ°„ÄÇ‰∏çËøáÔºåÂΩìÊó∂787‰∏™‰º∞ËÆ°ÁöÑÂπ≥ÂùáÊï∞ÊòØ1197„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÂπ≥ÂùáÊï∞ÂÖ∂ÂÆû‰ª•Èõ∂ËØØÂ∑ÆÁöÑË°®Áé∞‰º∞ËÆ°Âà∞‰∫ÜÁúüÂÆûÂÄºÔºÅ¬†&lt;a href=&quot;#fnref:correction&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>2020Âπ¥4ÊúàÔºöÊõ¥ÈöæÁöÑÊñπÊ≥ï‰∏éÊúâÂ•óË∑ØÁöÑ‰∫§ÊµÅ</title>
   <link href="http://localhost:4000/journal/2020/04/30/journal-2020-04.html"/>
   <updated>2020-04-30T08:00:00+08:00</updated>
   <id>http://localhost:4000/journal/2020/04/30/journal-2020-04</id>
   <content type="html">&lt;p&gt;Ëøô‰∏™ÊúàÊî∂Âà∞‰∫ÜÁ®øÂ≠êÁöÑÊ∂àÊÅØÔºåÂú®Â∞ùËØïÊãº‰∏äÊúÄÂêé‰∏ÄÂùóÈóÆÈ¢òÁöÑ‚ÄúÊãºÂõæ‚ÄùÔºå‰ª•ÂèäÊÄùËÄÉÂ¶Ç‰ΩïÂùê‰∏äÂè∏Êú∫ÁöÑ‰ΩçÁΩÆ„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏ÄËΩ¨ÁúºÂèà‰∏§‰∏™ÊòüÊúüÊ≤°Ë∑üËÄÅÊùøÔºàÂçöÂ£´ÂØºÂ∏àÔºâËÆ®ËÆ∫Á†îÁ©∂‰∏äÁöÑÈóÆÈ¢ò‰∫Ü„ÄÇ&lt;/p&gt;

&lt;p&gt;‰∏äÂë®ËÅäËøáÂÖ≥‰∫éÂõûÂ§çÂÆ°Á®ø‰∫∫ÊÑèËßÅÁöÑ‰∫ãÔºåÁ¨¨‰∏ÄÊ¨°ÊäïÁ®øÂíåÊî∂Âà∞‰øÆÊîπÊÑèËßÅÔºåÂ§öÂ∞ëÊúâÁÇπÂøêÂøë„ÄÇÂ≠¶ÊúØÊñáÁ´†ÂèëË°®ÁöÑËøáÁ®ãÂ§ßËá¥ÊòØÊäïÁ®ø„ÄÅÁºñËæëÊäÑÈÄÅÁªôÂåøÂêçÂÆ°Á®ø‰∫∫„ÄÅÂÆ°Á®ø‰∫∫ÂõûÂ§çÔºàÊé®Ëçê‰∏éÂê¶Âä†‰∏äÂÖ∑‰ΩìÊÑèËßÅÔºâ„ÄÅÁºñËæëÂõûÂ§çÔºàÊé•Âèó„ÄÅ‰øÆÊîπ„ÄÅÊãíÁªùÔºâ„ÄÇÂ¶ÇÊûúÂõûÂ§çÊòØ‰øÆÊîπÔºåÈÇ£ÂèØËÉΩ‰ºöÈáçÂ§çÂá†ËΩÆËøô‰∏™Ê≠•È™§„ÄÇÂèØËÉΩÊòØÂõ†‰∏∫Áñ´ÊÉÖÁöÑÂÖ≥Á≥ªÔºåÂéªÂπ¥ÂçÅ‰∏ÄÊúàÂ∞±ÊäïÂá∫ÂéªÁöÑÊñáÁ´†ÔºåÂéÜÊó∂‰∫î‰∏™ÊúàÔºå‰∏ä‰∏äÂë®ÊâçÊî∂Âà∞ÂõûÂ§çÔºåÂπ∂‰∏îÂè™Êúâ‰∏§‰∏™ÂÆ°Á®ø‰∫∫„ÄÇÊàëÊâìÂøÉÈáåÊÑüË∞¢Ëøô‰∏§‰ΩçÂÆ°Á®ø‰∫∫Âú®ËøôÁâπÊÆäÊó∂ÊúüËøòÂÆ°ÊàëÁöÑÁ®øÔºåÂπ∂‰∏îÊèêÁöÑÂ§ßÂ§öÊòØÂØªÊ±ÇËøõ‰∏ÄÊ≠•Ëß£ÈáäÊàñËÄÖËÆ∫Ëø∞ÊÄßË¥®ÁöÑÊÑèËßÅÔºåËÄå‰∏çÊòØ‚ÄúË¥®Áñë‚ÄùÂûãÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊØèÂë®Âë®‰∫åÔºåÊòØÊàë‰ª¨ÂæÄÂ∏∏ÁªÑ‰ºöÂíåËßÅÈù¢ËÆ®ËÆ∫ÁöÑÊó•Â≠ê„ÄÇËá™‰ªéÁñ´ÊÉÖÂú®Êñ∞Âä†Âù°Êâ©Êï£‰ª•Êù•ÔºåÂ≠¶Ê†°ÂÖàÊòØËßÑÂÆö‰ºöËÆÆË¶ÅÊµã‰ΩìÊ∏©ÁôªËÆ∞Êó∂Èó¥Á≠âÁ≠âÔºå‰∏ç‰πÖÂ∞±Ë¶ÅÊ±ÇÂÖ®ÂëòÂú®ÂÆ∂Â∑•‰Ωú‰∫Ü„ÄÇÂÖ®Ê†°‰∫∫ÂëòÊí§Á¶ªÊ†°Âõ≠ÁöÑÂêåÊó∂ÔºåÊâÄÊúâË∑üÊñ∞ÂÜ†ÁóÖÊØíÁõ∏ÂÖ≥ÁöÑÁ†îÁ©∂Ê¥ªÂä®ÂæóÂà∞ÁâπÊÆäÂÖÅËÆ∏ÔºåÂèØ‰ª•ÁªßÁª≠ÂºÄÂ±ï„ÄÇÊàë‰ª¨Ëøô‰∏ÄÁ±ª‰∏çÁî®ÂÅöÂÆûÈ™åÊúâÁîµËÑëÂ∞±ËÉΩÂ∑•‰ΩúÁöÑÁ†îÁ©∂ÔºåÂú®ÂÆ∂Â∑•‰ΩúÊ≤°Â§™Â§ßÂΩ±ÂìçÔºõScience„ÄÅÂúüÊú®‰∏ÄÁ±ªÈúÄË¶ÅÂ≠¶Ê†°ËµÑÊ∫êÂÅöÂÆûÈ™åÁöÑÊúãÂèã‰ª¨Â∞±Ê≤°ÈÇ£‰πàÂπ∏Ëøê‰∫ÜÔºå‰∏çÁü•ÈÅì‰ªñ‰ª¨Áé∞Âú®Âú®ÂÆ∂ÈáåÂ¶Ç‰ΩïÁªßÁª≠ÁßëÁ†îÔºåÂàöÂ•ΩÈõÜ‰∏≠Á≤æÂäõÊï¥ÁêÜÊï∞ÊçÆÂÜôÂÜôÊñáÁ´†Ôºü&lt;/p&gt;

&lt;p&gt;‰∏çÁü•ÈÅìÊòØÂê¶ÊòØÂá∫‰∫é‰π†ÊÉØÔºåÂë®‰∫å‰∏Ä‰∏äÁè≠Â∞±ÊääÊúÄËøëÂõ∞Êâ∞ÊàëÁöÑÈóÆÈ¢òÂ•ΩÂ•ΩÁªÑÁªáÊâìÁ®ø‰∏ÄÁï™ÂèëÁªô‰∫ÜËÄÅÊùø„ÄÇËøôÈóÆÈ¢òÁÆóÊòØÊàëÁõÆÂâçÂ∑•‰ΩúÁöÑÊúÄÂêé‰∏ÄÂùó‚ÄúÊãºÂõæ‚ÄùÂêß„ÄÇÁ≤óÁï•Êù•ËØ¥ÔºåÊàëÊ®°ÂûãÈáåÊúâ‰∏™Êú™Áü•ÁöÑÂèÇÊï∞Ë¶Å‰º∞ËÆ°Ôºå‰ΩÜÈóÆÈ¢òÊòØÁ∫ø‰∏ãÔºàoffline)ËøòÊòØÁ∫ø‰∏ä(online)‰º∞ËÆ°„ÄÇÁ∫ø‰∏ã‰º∞ËÆ°ÈúÄË¶Å‰∏ÄÂÆöÊï∞ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰πüÂæàÈöæ‰øùËØÅÊôÆÈÅçÊÄßÔºå‰∏á‰∏ÄÊúâ‰∏™Ê≤°ËßÅËøáÁöÑÊÉÖÂÜµÔºåÊïàÊûúÂèØËÉΩÂ∞±‰∏çÂ•ΩÔºõÁ∫ø‰∏ä‰º∞ËÆ°ÁöÑÈÄÇÂ∫îÊÄßÂ∞±Âº∫ÂæàÂ§öÔºå‰πü‰∏çÈÇ£‰πà‰æùËµñËÆ≠ÁªÉÊ†∑Êú¨Ë¥®Èáè„ÄÇ‰ΩÜÊòØÔºåÁ∫ø‰∏ä‰º∞ËÆ°ÁöÑÊñπÊ≥ïÊàëÂâçÊÆµÊó∂Èó¥Áúã‰∫Ü‰∏Ä‰∏ãÊñáÁåÆÔºåÊ≤°ÊÄé‰πàÁúãÊáÇ„ÄÇÂΩìÊó∂ÊÄ•ÁùÄÊÉ≥Ë¶ÅÁúãÁúãÊï¥‰ΩìÊñπÊ≥ïÁöÑË°®Áé∞ÔºåÂßë‰∏îÁî®Á∫ø‰∏ãÂ≠¶‰π†‰º∞ËÆ°‰∫ÜÔºåÂàùÊ≠•ÁªìÊûúÁúã‰∏äÂéª‰πü‰∏çÈîô„ÄÇÁÑ∂ËÄåÔºåËøôÂèÇÊï∞ÊôÆÈÅçÊÄßÁöÑÈóÆÈ¢òÂú®ÊàëÂ∞ùËØïÂÅöÂ§ßÈáè‰ªøÁúüÊ£ÄÈ™åÁöÑÊó∂ÂÄô‰∏çÂá∫ÊÑèÂ§ñÂú∞Âá∫Áé∞‰∫ÜÔºÅËôΩÁÑ∂ÂøÉÈáåÁü•ÈÅìËøôÈóÆÈ¢òÂæóËß£ÂÜ≥Ôºå‰ΩÜÁúüÂ§ÑÁêÜËµ∑ËøôÊúÄÂêé‰∏ÄÂùó‚ÄúÊãºÂõæ‚ÄùÊó∂ÔºåÊàëËøòÊòØÁäπË±´ÁöÑ„ÄÇ&lt;/p&gt;

&lt;p&gt;ÈóÆÈ¢òÂèëÁªô‰∫ÜËÄÅÊùøÔºåÂæàÂø´ÂõûÂ§ç‰∫ÜÊàëÔºöÂèØ‰ª•Á∫ø‰∏ä‰º∞ËÆ°Ôºå‰Ω†ÁúãÁúãxxxÔºàÂá†‰∏™ÊÄùÊÉ≥ÁöÑÂÖ≥ÈîÆËØçÔºåÊñπ‰æøÊàëÂéªÊü•ÈòÖÔºâ„ÄÇÂæóÂà∞ÂõûÂ§çÁöÑÊó∂ÂÄôÊ≤°ÊúâÂ§±ÊúõÔºåÂèçËÄåÊòØÂÆâÂøÉ‰∫Ü‰∏Ä‰∫õÔºåË∑üËá™Â∑±ËØ¥ÔºåË∏èË∏èÂÆûÂÆûÂÅöÂêß„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;‰∏Ä‰∏™Â≠¶ÊúüÂè™ËÅîÁ≥ª‰∏ÄÊ¨°Á†î‰∏ÄÈÉΩÊòØÊîæÂÖªÁöÑÂêó&quot;&gt;‰∏Ä‰∏™Â≠¶ÊúüÂè™ËÅîÁ≥ª‰∏ÄÊ¨°ÔºåÁ†î‰∏ÄÈÉΩÊòØÊîæÂÖªÁöÑÂêóÔºü&lt;/h3&gt;

&lt;p&gt;Ââç‰∏§Â§©ÂÅ∂ÁÑ∂ÁúãÂà∞Ë±ÜÁì£Êúâ‰∫∫ÈóÆ‰∫ÜÂ¶Ç‰∏äÁöÑÈóÆÈ¢òÔºå‰ªñ/Â•πÁúã‰∏äÂéªÂæàÂõ∞ÊÉëÔºåËØ¥Â∏åÊúõËÉΩË∑üÂØºÂ∏àÊúâÊõ¥Â§öÁöÑ‰∫§ÊµÅ„ÄÇËõÆÂ§ö‰∫∫ÂõûÂ§çËØ¥Ëøô‰∫ãÂÑøÂæóÁúãËÄÅÂ∏à„ÄÇÁ°ÆÂÆûÔºåÊØè‰∏™ËÄÅÊùøÊåáÂØºÈ£éÊ†º‰∏çÂêå‰ºöÊúâ‰∏çÂ∞ëÂå∫Âà´Ôºå‰∏çËøáËøôÂè™ÊòØÂÖ¨ÂºèÁöÑ‰∏ÄÂçäÂêßÔºåËá™Â∑±‰Ωú‰∏∫Âè¶‰∏ÄÂçäËÉΩÂÅöÁöÑ‰∫ãÂÑø‰πüÂæàÂ§ö„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊàëÁöÑËÄÅÊùø‰πüÊòØ‰∏≠Èó¥ÂÅè‚ÄúÊîæÂÖª‚ÄùÂûãÁöÑÔºåÊØîËæÉÂ∞ë‰ºö‰∏ªÂä®ËøáÈóÆÊàëËøõÂ±ïÂ¶Ç‰ΩïÔºåÊó∂‰∏çÊó∂‰ºöÂèëÁÇπÊñáÁ´†ÁªôÊàëÁúãÔºå‰∏çËøáÊàëÊÉ≥ËÆ®ËÆ∫ÁöÑÊó∂ÂÄôÔºå‰πü‰ºöËÆ§ÁúüÊåáÂØº„ÄÇËÆ∞ÂæóÂçö‰∏ÄÂàöÂºÄÂßãÈÇ£‰ºöÂÑøË∑üËÄÅÊùøËÅäËøáÂÖ≥‰∫éÂ≠¶ÁîüË∑üÂçöÂØºÁöÑÂÖ≥Á≥ªÔºå‰ªñËßâÂæóËØªÂçöÂÅöÁ†îÁ©∂Ëøô‰∫ãÂÑøÂΩíÊ†πÁªìÂ∫ïÊòØÊàë‰ª¨Ëá™Â∑±ÁöÑÔºåÂ≠¶ÁîüÂ∫îËØ•ÊääËá™Â∑±ÊîæÂà∞È©±Âä®ËÄÖÁöÑËßíËâ≤ÔºõËÄåÂØºÂ∏àÁöÑËßíËâ≤Â∫îËØ•ÊòØ‰∏Ä‰∏™advisor/mentorÔºå‰ºöË¥üË¥£ÁªôËØÑ‰ª∑„ÄÅÊèêÂª∫ËÆÆ„ÄÅÊèê‰æõÊàëÈúÄË¶ÅÁöÑÂ∏ÆÂä©Ôºå‰ΩÜÂπ∂‰∏çË¥üË¥£Â∏ÆÊàëÂÅö‰∫ãÂÑø„ÄÅÁõëÁù£ÊàëÂÅö‰∫ãÂÑø„ÄÇÂØπËøôÔºåÊàëÊòØÂÆåÂÖ®ÂêåÊÑèÁöÑÔºåÂπ∂‰∏îÂ∫ÜÂπ∏ËÄÅÊùøÂú®‰∏ÄÂºÄÂßãÂ∞±Ë∑üÊàë‰ª¨ËØ¥Ê∏ÖÊ•ö„ÄÅËÆ©Êàë‰ª¨Êúâ‰∫ÜÂêàÁêÜÁöÑÊúüÂæÖ(manage expectations)„ÄÇ&lt;/p&gt;

&lt;p&gt;ÊàëÂêéÊù•ÊÑèËØÜÂà∞ÔºåÊó¢ÁÑ∂Ë¶ÅÂùêÂà∞Âè∏Êú∫ÁöÑ‰ΩçÁΩÆ‰∏äÔºåÈÇ£Â∞±ÊÑèÂë≥ÁùÄÂæóÂ≠¶‰π†ÊÄé‰πàÂºÄËΩ¶„ÄÇ‰∏çÊòØËØ¥ÁöÑÂ≠¶‰π†Â¶Ç‰ΩïÂÅöÊï∞Â≠¶Êé®ÂØºÔºåËÄåÊòØÂ≠¶‰π†Â¶Ç‰ΩïÂÅöÁ†îÁ©∂ÁîüÔºåÂÖ∂‰∏≠ÂæàÈáçË¶ÅÁöÑ‰∏ÄÈ°πÂ∞±ÊòØÂ¶Ç‰ΩïË∑üËÄÅÊùø‰øùÊåÅÊ≤üÈÄöÔºàÂ∞§ÂÖ∂ÊòØ‚ÄúÊîæÂÖª‚ÄùÂûãÔºâ„ÄÇÊúâ‰∫õÂ≠¶Ê†°‰ºöÁªôÊñ∞ÂÖ•Â≠¶ÁöÑÁ†îÁ©∂ÁîüÊèê‰æõÁ±ª‰ººÂüπËÆ≠ÁöÑËØæÁ®ãÔºå‰ΩÜ‰∏çÊòØÊâÄÊúâÂ≠¶Ê†°ÈÉΩËøôÊ†∑ÂÅö„ÄÇÂú®Êë∏Á¥¢ÁöÑËøáÁ®ã‰∏≠ÔºåÂêåÁ†îÁ©∂‰∏≠ÂøÉÁöÑÂæ∑ÂõΩÂçöÂêéÂêå‰∫ãÊé®Ëçê‰∫ÜÊú¨Â∞èÂÜåÂ≠êÁªôÊàëÔºö‚Äú&lt;a href=&quot;/assets/month-journal/The 7 Secrets.pdf&quot; target=&quot;_blank&quot;&gt;The Seven Secrets of Highly Successful Research Students&lt;/a&gt;‚Äù„ÄÇÊàë‰ª¨Á†îÁ©∂‰∏≠ÂøÉÊúâÁÇπÁâπÊÆäÔºå‰∏≠ÂøÉÂæàÂ§öÂçöÂ£´ÈÉΩÊòØÂú®ÁëûÂ£´Êãõ‰∫ÜÁÑ∂ÂêéÊù•Êñ∞Âä†Âù°ËØªÁöÑÔºåÊ≤°ÂäûÊ≥ïÁî®Âà∞ÁëûÂ£´ÈÇ£ËæπÂ§ßÂ≠¶ÈáåÁöÑ‰ºóÂ§öËµÑÊ∫êÔºå‰ªñ‰ª¨ÁöÑÂçöÂØº‰ª¨‰πüÂ§ßÂ§öbaseÂú®ÁëûÂ£´„ÄÇËøô‰πüÊòØÂ§ßÂÆ∂ÊØîËæÉÂÖ≥ÂøÉ‚ÄúÂ¶Ç‰ΩïË∑üËÄÅÊùø‰øùÊåÅÊ≤üÈÄö‚ÄùÔºå‚ÄúÂ¶Ç‰ΩïÊúâÊïàÁÆ°ÁêÜËá™Â∑±ÁöÑÁ†îÁ©∂ËøõÂ∫¶‚ÄùÁ≠âÈóÆÈ¢òÁöÑÂéüÂõ†Âêß„ÄÇÂ∞èÂÜåÂ≠êÈáåÈù¢ÁªôÂá∫‰∫ÜËõÆÂ§öËØöÊÅ≥ÁöÑÂª∫ËÆÆ„ÄÇÂÖ∂‰∏≠ÂÖ≥‰∫éÂ¶Ç‰ΩïË∑üËÄÅÊùø‰øùÊåÅÊ≤üÈÄöÔºå‰∫§ÊµÅÂ∑•‰ΩúÁöÑÈÉ®ÂàÜËÆ©Êàë‰πüÂèóÁõäÂå™ÊµÖ„ÄÇÂ∞èÂÜåÂ≠êÊé®ËçêË∑üËÄÅÊùø‰∫§ÊµÅÂ∑•‰ΩúÊó∂Áî®ËøôÊ†∑‰∏Ä‰∏™Ê®°ÊùøÔºö&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ëá™‰ªé‰∏äÊ¨°‰ºöËÆÆÊàëÂÅö‰∫ÜÔºö1..2..3..&lt;/li&gt;
  &lt;li&gt;Êñ∞ÈÅáÂà∞ÁöÑÈóÆÈ¢òÔºö1..2..3..&lt;/li&gt;
  &lt;li&gt;ÔºàËÄÅÊùøÁöÑÔºâËØÑ‰ª∑‰∏éÂª∫ËÆÆÔºö1..2..3..&lt;/li&gt;
  &lt;li&gt;Êé•‰∏ãÊù•Ë¶ÅÂÅöÔºö1..2..3..&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ËøôÊ®°ÊùøÁî®Ëµ∑Êù•ÁâπÂà´ÂÆπÊòì‰∏äÊâã„ÄÇÊàëÊòØÊåâËøôÊ†∑ÁöÑÈ°∫Â∫è‰ΩøÁî®ÁöÑÔºöÊ†πÊçÆ‰∏äÊ¨°‰ºöËÆÆËÄÅÊùøÊèêÂá∫ÁöÑËØÑ‰ª∑‰∏éÂª∫ËÆÆÂéªËß£ÂÜ≥ÊàëÊñ∞ÈÅáÂà∞ÁöÑÈóÆÈ¢òÔºåÁÑ∂ÂêéÂéªÂÅö‚ÄúÊé•‰∏ãÊù•Ë¶ÅÂÅö‚ÄùÁöÑ‰∫ãÂÑø„ÄÇÂú®ËøôËøáÁ®ã‰∏≠ÔºåÂºÄÂêØÊñ∞ÁöÑ‰∏ÄÈ°µÔºåÂπ∂ËÆ∞ÂΩïÊñ∞ÁöÑÁ¨¨‰∏ÄÂíåÁ¨¨‰∫åÁÇπÔºåÁÑ∂ÂêéÈáçÂ§ç„ÄÇÁî®ËøôÊ®°ÊùøË∑üËÄÅÊùøÂÅöÊØèÂë®ÁöÑËÆ®ËÆ∫Âø´‰∏ÄÂπ¥‰∫ÜÔºåË∑ü‰ª•ÂâçÂºÄ‰ºöÂâç‰∏ÄÂ§©ÂåÜÂøôÂáÜÂ§áËÆ®ËÆ∫ÁöÑÁÇπÁõ∏ÊØîÔºåËÉΩÊòéÊòæÊÑüËßâÂà∞Áé∞Âú®Ëá™Â∑±Âú®ËÆ®ËÆ∫Êó∂ÊÄùË∑ØÊ∏ÖÊô∞ÂæàÂ§öÔºåÊØîËæÉÂ∞ëÂõ†‰∏∫ËÄÅÊùøÁöÑÂèëÊï£ÊÄßÊèêÈóÆËÄåÂøòËÆ∞ÂéüÊú¨ÊÉ≥Ë¶ÅËÆ®ËÆ∫ÁöÑÁÇπ„ÄÇÂè¶Â§ñ‰∏Ä‰∏™ÊîπÂèòÊòØÔºåÂΩìÊàëËÉΩÂèÇÁÖßËá™Â∑±ÁöÑËÆ∞ÂΩïÔºåÊØîËæÉÂáÜÁ°ÆÂú∞ËÆ≤Âá∫ËøôÈóÆÈ¢òÁöÑÊù•ÈæôÂéªËÑâÊó∂ÔºåÂæÄÂæÄËÉΩÂæóÂà∞ÊØîËæÉÊ∏ÖÊô∞ÁöÑÂª∫ËÆÆ‰∏éËØÑ‰ª∑„ÄÇËøôÊ†∑ËÆ∞ÂΩï‰∏ãÊù•ÁöÑÁ¨îËÆ∞ÔºåÊó•ÂêéÂæÄÂõûÊü•Áúã‰πüÊñπ‰æøÂæàÂ§öÔºåÂ∞§ÂÖ∂ÊòØÊÉ≥Ë¶ÅÊü•ÊâæËÄÅÊùøÊõæÁªèÁªôÂá∫ÁöÑ‚ÄúËá™Áõ∏ÁüõÁõæ‚ÄùÁöÑÂª∫ËÆÆÁöÑÊó∂ÂÄôüòÉ„ÄÇ&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Tracking the COVID-19 outbreak and signals of containment</title>
   <link href="http://localhost:4000/data/2020/03/24/COVID-19.html"/>
   <updated>2020-03-24T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2020/03/24/COVID-19</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.&lt;/strong&gt; 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are &lt;strong&gt;interactive&lt;/strong&gt; and best viewed on a desktop browser.&lt;/p&gt;

&lt;h2 id=&quot;signals-of-containment&quot;&gt;Signals of Containment&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;Confirmed and Death Cases&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-interplay-of-confirmed-and-death-cases&quot;&gt;The Interplay of Confirmed and Death Cases&lt;/h3&gt;
&lt;p&gt;When should the economy reopen? To try to answer this question, we could look at the interplay of new confirmed cases and death cases.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1739652220&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;More cases means more healthcare resource demand, and doctors and nurses have to make tough decisions. Unfortunately, more patients who need intensive care might not get it, leading to higher fatalities. We are probably going to see a peak in daily cases, and after some time, a peak in daily fatalities. This phenomenon is visible in the graph below. Passing the first peak means measures are taking effect; passing the second means our healthcare system is now able to cope. So, where do countries stand as of now?&lt;/p&gt;

&lt;p&gt;Of course, the decision has to also depend on other factors such as the ability of testing and tracking down close contacts of those infected.&lt;/p&gt;

&lt;p&gt;There are actually many questions that we could ask from this graph. For example:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Why does Germany has much higher daily confirms than Switzerland, and yet manages a much flatter death curve?&lt;/li&gt;
  &lt;li&gt;Why do the two peaks for the UK seem to occur at the same time while that‚Äôs not the case for the rest?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Countries with hopes of relaxing some of the lockdown measures: Germany and Switzerland. Both of them have passed the peaks, have low daily cases (&amp;lt;20), and relatively flat and low death case curve (&amp;lt;5).&lt;/li&gt;
  &lt;li&gt;Countries that probably need more time: They are at the edge of passing the first peak and record about 80 daily cases. What‚Äôs more worrying, though, is the evident pressure on the healthcare system. UK sees a drop in daily death cases, but that number is still high at 11; the US‚Äôs death case curve seems not at its peak yet. They probably need more time. - April 23, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Percentage Change&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;daily-case-percentage-change&quot;&gt;Daily Case Percentage Change&lt;/h3&gt;
&lt;p&gt;Look out for the 7-day moving average of the day-on-day percentage change in confirmed cases. It is important to see both the current percentage change and its trend. To easily classify the situation, we can use the following scale&lt;sup id=&quot;fnref:percentage&quot;&gt;&lt;a href=&quot;#fn:percentage&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;r &gt; 10\%&lt;/script&gt;: &lt;strong&gt;Rapidly increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 10\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 5\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Slowly increasing&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
r &lt; 1\% %]]&gt;&lt;/script&gt;: &lt;strong&gt;Under control&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=565833280&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Japan had a turning point on 23rd March where the increase of cases started accelerating. Coincidently (or maybe not), Japan and I.O.C. officially anounced the &lt;a href=&quot;https://www.nytimes.com/2020/03/24/sports/olympics/coronavirus-summer-olympics-postponed.html&quot;&gt;postponement of Tokyo 2020&lt;/a&gt; on the next day.&lt;/li&gt;
  &lt;li&gt;The cases in Japan have been rising at an increasing rate, now at a 10% &lt;a href=&quot;#Percentage Change&quot;&gt;day-on-day growth rate&lt;/a&gt;. Considering the exponential growth of infections, Abe, Japanese prime minister, is declaring emergency state for seven prefectures. - April 7, 2020&lt;/li&gt;
  &lt;li&gt;Japan sees a slowdown of daily new cases. It‚Äôs been two weeks since the first declaration of ‚ÄúEmergency Situation‚Äù by the prime minister. On average, a 50% reduction in the number of people going out in monitored areas &lt;a href=&quot;https://www3.nhk.or.jp/news/special/coronavirus/#infection-status&quot;&gt;are observed&lt;/a&gt;. Meanwhile, mask sales have skyrocketed in Japan. - April 22, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Google Search Interest&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;google-search-interest&quot;&gt;Google Search Interest&lt;/h3&gt;
&lt;p&gt;This figure tells us how many people in the US are searching for keywords such as ‚Äúhand sanitizer‚Äù or ‚Äúsymptom‚Äù. I suspect that as the community spread of the virus is being contained, we can expect to see a drop in searches for words like ‚Äúsymptom‚Äù and ‚Äúinfluenza‚Äù, similar to the trends shown in Singapore.&lt;/p&gt;

&lt;p&gt;There are drastic differences in terms of the US and Singapore google search interests during this pandemic. When signs of community infection emerged in early March, people in the US were searching for ‚Äúsymptom‚Äù at a record-high frequency, similarly for ‚Äúinfluenza‚Äù and ‚Äúhand sanitizer‚Äù. Searches for ‚Äúmask‚Äù, however, were not so heightened. The picture in Singapore looks very different. When more infections emerged inside the border in late January and early February, the search for ‚Äúmask‚Äù shoot up rapidly, and masks went out of stock everywhere in Singapore. There are probably two main reasons for this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A high percentage of Chinese living in Singapore;&lt;/li&gt;
  &lt;li&gt;As a nation that went through SARS, it feels natural for most people to wear masks when a contagion is spreading in the community.&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=783455223&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=196247116&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;US Testing Numbers&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;us-testing-numbers&quot;&gt;US Testing Numbers&lt;/h3&gt;
&lt;p&gt;As the containment takes effect, we expect to see the number of positive and negative tests stabilize, and the number of tests pending result drops. As you can see, we are not there yet.&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=481777218&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;cumulative-case-progression&quot;&gt;Cumulative Case Progression&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;Case progression&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;696.0000000000001&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=967719983&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Japan has a relatively flat curve. However, there are legitimate concerns that Japan has been under-testing its population to know what is really going on. Assuming the true CFR is 1.2%&lt;sup id=&quot;fnref:diamond_princess&quot;&gt;&lt;a href=&quot;#fn:diamond_princess&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, Japan‚Äôs current fatality number, 77, indicates that at least 6,417 people have been infected. However, only 3,139 cases are officially confirmed as of now. Also, Japan has conducted 486 tests &lt;a href=&quot;#https://www.worldometers.info/coronavirus/&quot;&gt;per one million population&lt;/a&gt;. In Singapore, that number is 11,110. - April 5, 2020.&lt;/li&gt;
  &lt;li&gt;For the first time, Singapore is going into a national ‚ÄúShelter in Place‚Äù mode. The timeing is not surprising as some degree of wide-spread community infection is going on. The number of unlinked cases, those yet to find the source of infection, spiked over the last few days; Singapore also recorded 12 new clusters of infection just over the past five days (One of them is right across the river from my house). - April 5, 2020.&lt;/li&gt;
  &lt;li&gt;Singapore sees a steady increase in confirmed cases, mainly in foreign worker dormitory clusters. However, if we look at the &lt;a href=&quot;#Case progression&quot;&gt;progression of confirmed cases&lt;/a&gt; in Singapore, it‚Äôs an almost perfect example of what ‚Äúflatten the curve‚Äù looks like. For the most part, the cases double every ten days, whereas cases in some of the worst-hit countries double every one to three days.  - April 8, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;death-cases&quot;&gt;Death Cases&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;case fatality rate&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;Germany and Switzerland fare well in this regard and manage to record comparatively low CFRs. Austria, too, has managed one of the lowerest CFRs among European nations. Austria, Germany, and a large part of Switzerland are German-speaking.ü§î&lt;/li&gt;
  &lt;li&gt;While the CFRs in Switzerland and Germany have been comparatively low, they are steadily climbing. Switzerland is probably the first country in Europe to flatten the curve, which conducted one of the highest number of tests &lt;a href=&quot;#https://www.worldometers.info/coronavirus/&quot;&gt;per one million population&lt;/a&gt;. - April 10, 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&quot;696&quot; height=&quot;432&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a name=&quot;cfr bias&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How does selection bias affect CFR?
    &lt;blockquote&gt;
      &lt;p&gt;[In Italy], a change in strategy on Feb 25 limited testing to patients who had severe signs and symptoms also resulted in a 19% positive rate (21,157 of 109,170 tested as of Mar 14) and an apparent increase in the death rate‚Äîfrom 3.1% on Feb 24 to 7.2% on Mar 17‚Äîpatients with milder illness were no longer tested.  In the UK, only patients deemed ill enough to require at least one night in hospital met the criteria for a Covid-19 test.&lt;/p&gt;

      &lt;p&gt;CFR rates are subject to selection bias as more severe cases are tested, generally those in the hospital settings or those with more severe symptoms. The number of currently infected asymptomatics is uncertain: estimates put it at least a half are asymptomatic; the proportion not coming forward for testing is also highly doubtful (i.e. you are symptomatic, but you do not present for testing). Therefore we can assume the IFR is significantly lower than the CFR.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;When is CFR accurate?
    &lt;blockquote&gt;
      &lt;p&gt;Iceland‚Äôs higher rates of testing, the smaller population, and their ability to ascertain all those with Sars-CoV-2  means they can obtain. an accurate estimate of the CFR and the infection fatality rate (IFR) during the pandemic (most countries will only be able to do this after the pandemic). Current data from Iceland suggests their IFR is somewhere between 0.01% and 0.19%.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The bottom line is, CFR is probably &lt;strong&gt;inflated&lt;/strong&gt; in many countries and IFR is &lt;strong&gt;much lower&lt;/strong&gt; than CFR.&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;h4 id=&quot;websites&quot;&gt;Websites&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Bloomberg&lt;/strong&gt;: &lt;a href=&quot;https://www.bloomberg.com/graphics/2020-coronavirus-cases-world-map/?srnd=premium-asia&quot;&gt;Mapping the Coronavirus Outbreak Across the World&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Johns Hopkins University&lt;/strong&gt;: &lt;a href=&quot;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Coronavirus COVID-19 Global Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Global MediXchange&lt;/strong&gt;: &lt;a href=&quot;https://www.alibabacloud.com/universal-service/pdf_reader?spm=a3c0i.14138300.8102420620.dreadnow.646d647fDWbsii&amp;amp;cdnorigin=video-intl&amp;amp;pdf=Read%20Online-Handbook%20of%20COVID-19%20Prevention%20and%20Treatment.pdf&quot;&gt;Handbook of COVID-19 Prevention and Treatment&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;articles&quot;&gt;Articles&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nytimes.com/2020/03/19/us/politics/trump-coronavirus-outbreak.html&quot;&gt;Before Virus Outbreak, a Cascade of Warnings Went Unheeded&lt;/a&gt;, March 19, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.citylab.com/life/2020/03/coronavirus-cases-france-train-hospital-tgv-covid-19-patient/608833/&quot;&gt;To Fight a Fast-Moving Pandemic, Get a Faster Hospital&lt;/a&gt;, March 26, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.businessinsider.sg/coronavirus-spain-says-rapid-tests-sent-from-china-missing-cases-2020-3?_ga=2.212074516.1285585527.1585620210-963085568.1583747541&amp;amp;r=US&amp;amp;IR=T&quot;&gt;Spain, Europe‚Äôs worst-hit country after Italy, says coronavirus tests it bought from China are failing to detect positive cases&lt;/a&gt;, March 26, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://time.com/5812555/germany-coronavirus-deaths/&quot;&gt;Why Is Germany‚Äôs Coronavirus Death Rate So Low?&lt;/a&gt;, March 30, 2020&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nytimes.com/interactive/2020/04/14/science/coronavirus-transmission-cough-6-feet-ar-ul.html&quot;&gt;This 3-D Simulation Shows Why Social Distancing Is So Important&lt;/a&gt;, April 14, 2020&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-sources&quot;&gt;Data sources&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Japan: &lt;a href=&quot;https://www3.nhk.or.jp/news/special/coronavirus/#infection-status&quot;&gt;NHK&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Singapore: &lt;a href=&quot;https://www.moh.gov.sg/covid-19&quot;&gt;Ministry of Health&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Other countries: JHU &lt;a href=&quot;https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6&quot;&gt;Coronavirus COVID-19 Global Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;US testing numbers: &lt;a href=&quot;https://covidtracking.com/&quot;&gt;The COIVD Tracking Project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Search interests: &lt;a href=&quot;https://trends.google.com/trends/explore?date=today%205-y&amp;amp;geo=US&amp;amp;q=%2Fm%2F0b23px,%2Fm%2F01kr41,%2Fm%2F0cycc,%2Fm%2F01b_06&quot;&gt;Google Trends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:percentage&quot;&gt;
      &lt;p&gt;The percentage only indicates a relative change. The actual number of new cases reported in each country may be very different, as it depends on the absolute number of cumulative cases in that country.¬†&lt;a href=&quot;#fnref:percentage&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:diamond_princess&quot;&gt;
      &lt;p&gt;Russell, Timothy W., et al. ‚Äú&lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.03.05.20031773v2&quot;&gt;Estimating the infection and case fatality ratio for COVID-19 using age-adjusted data from the outbreak on the Diamond Princess cruise ship.&lt;/a&gt;‚Äù medRxiv (2020).¬†&lt;a href=&quot;#fnref:diamond_princess&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>ËØª‰π¶Á¨îËÆ∞ÔºöËØª‰π¶‰ΩìÈ™åÊòØ‰ªÄ‰πà</title>
   <link href="http://localhost:4000/reading/2019/11/12/how-to-read.html"/>
   <updated>2019-11-12T08:00:00+08:00</updated>
   <id>http://localhost:4000/reading/2019/11/12/how-to-read</id>
   <content type="html">&lt;p&gt;ÊàëÊúâÈÄâ‰π¶Ë¥≠‰π¶ÁöÑ‰π†ÊÉØÔºå‰πüÊúâÈÄõ‰π¶Â∫óËä±‰∫îÂàÜÈíüÂÜ≥ÂÆöÂ∏¶ÂõûÂì™Êú¨‰π¶ÁöÑÊó∂ÂÄôÔºõÊàëÊó∂‰∏çÊó∂ËØª‰π¶Ôºå‰ΩÜÂæÄÂæÄËØªÂÆå‰πãÂêéÂøòÊéâ‰π¶ÈáåÊúâ‰ªÄ‰πàÁ≤æÂΩ©ÂÜÖÂÆπÔºõÊàëÊúâÂú®ÁïôÁôΩËØÑËÆ∫ÁöÑ‰π†ÊÉØÔºå‰πüÊúâ‰∏çÂÜçÁøªÁúãÂΩìÊó∂ÂÜô‰∏ãÁöÑËØÑËÆ∫„ÄÅËßÇÁÇπÁöÑ‰π†ÊÉØ„ÄÇÂÆ∂ÈáåÊú™ÊãÜÂ∞ÅÁöÑ‰π¶Ë∂äÊù•Ë∂äÂ§öÔºåÊñ≠Êñ≠Áª≠Áª≠Âú®ËØªÁöÑ‰π¶ÂÖ∑‰Ωì‰πüËØ¥‰∏çÂá∫Êù•Âì™ÈáåÂê∏ÂºïÊàëÔºåËØªËøáÁöÑ‰π¶Â•ΩÂÉèË¢´Âø´ÈÄüÊ∂àË¥πËøá‰∏ÄÊ†∑Ê≤°ÂÜçÈú≤ËÑ∏„ÄÇÂù¶ÁôΩËÆ≤ÔºåÊàëÂπ∂‰∏ç‰∫ÜËß£Â∫îËØ•Â¶Ç‰ΩïËØª‰π¶Âêß„ÄÇÂπ∏ËøêÁöÑÊòØÔºåÂ••ÈáéÂÆ£‰πãÂú®‰ªñÁöÑ„ÄäÂ¶Ç‰ΩïÊúâÊïàÈòÖËØª‰∏ÄÊú¨‰π¶„ÄãÈáåÊèê‰æõ‰∫ÜÊ∏ÖÊô∞ÁöÑÊñπÊ≥ï„ÄÇ&lt;/p&gt;

&lt;p style=&quot;display: block; margin-left: auto; margin-right: auto; width: 50%;&quot;&gt;&lt;img src=&quot;/assets/2019-11-12/book_cover.jpg&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‰π¶ÂêçÔºö&lt;a href=&quot;https://book.douban.com/subject/26789567/&quot;&gt;Â¶Ç‰ΩïÊúâÊïàÈòÖËØª‰∏ÄÊú¨‰π¶&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ÂéüÂêçÔºöË™≠Êõ∏„ÅØ1ÂÜä„ÅÆ„Éé„Éº„Éà„Å´„Åæ„Å®„ÇÅ„Å™„Åï„ÅÑ&lt;/li&gt;
  &lt;li&gt;‰ΩúËÄÖÔºö&lt;a href=&quot;http://okuno0904.com/about/index.html&quot;&gt;Â••ÈáéÂÆ£‰πã&lt;/a&gt;Ôºà„Åä„Åè„ÅÆ„Éª„ÅÆ„Å∂„ÇÜ„ÅçÔºâ&lt;/li&gt;
  &lt;li&gt;Ë¥≠ÂÖ•Êó•ÊúüÔºö2019.11.11ÔºàÊâìÊäòÔºÅÔºâ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ËØª‰π¶‰ΩìÈ™å&quot;&gt;ËØª‰π¶‰ΩìÈ™å&lt;/h3&gt;
&lt;p&gt;ËøôÊú¨149È°µÁöÑÂ∞è‰π¶ÔºåÊàëËßâÂæóÂèØ‰ª•ÁúãÂÅöÊòØ‚ÄúÂ¶Ç‰ΩïËØª‰π¶‚ÄùÁöÑ‰∏ÄÊú¨ÂèÇËÄÉ‰π¶ÔºõÈáåÈù¢‰ªãÁªçÁöÑËßÇÁÇπÂíåÊñπÊ≥ïÔºå‰∏çÂêåÁ∫ßÂà´ÁöÑËØªËÄÖÈÉΩÂèØ‰ª•Âú®ÈÄÇÂΩìÁöÑÊó∂ÂÄôËØª‰∏ÄËØªÔºåÂπ∂Â≠¶Âà∞‰∫õ‰∏úË•ø„ÄÇËøôÊú¨Â∞è‰π¶Êó®Âú®ÂõûÁ≠îËøôÊ†∑‰∏Ä‰∏™ÈóÆÈ¢òÔºöÂ¶Ç‰ΩïÊâçËÉΩ‰∏çÂøòËÆ∞ËØªËøáÁöÑ‰π¶‰∏≠ÁöÑÂÜÖÂÆπÔºåÂπ∂‰Ωø‰πãËûçÂÖ•Ë∫´ÂøÉÔºåÁúüÊ≠£‰Ωø‰π¶Á±çÂΩ±ÂìçËá™Â∑±Ôºü&lt;/p&gt;

&lt;p&gt;ËÄå‰ªñÁöÑÂõûÁ≠îÊòØÔºåÊàë‰ª¨Â∫îËØ•ÈáçÊñ∞ÊÄùËÄÉËØª‰π¶Ëøô‰ª∂‰∫ãÂÑø„ÄÇËØª‰π¶‰∏çÂ∫îÂßã‰∫éÁøªÂºÄ‰π¶Á±çÔºå‰πü‰∏çÁªà‰∫éÊúÄÂêé‰∏ÄÈ°µ„ÄÇÊØè‰∏ÄÊ¨°ËØª‰π¶ÔºåÊàë‰ª¨Â∫îËØ•ÂéªÂàõÈÄ†Â±û‰∫éËá™Â∑±ÁöÑ‚ÄúËØª‰π¶‰ΩìÈ™å‚Äù„ÄÇË¶ÅÂ¶Ç‰ΩïÁêÜËß£Ëøô‰∏™‚Äú‰ΩìÈ™å‚ÄùÂë¢ÔºüÊàë‰ª¨ÂèØ‰ª•ÊÉ≥ÊÉ≥ÁîüÊ¥ª‰∏≠ÁöÑÂÖ∂‰ªñ‰ΩìÈ™åÔºåÂ∞§ÂÖ∂ÊòØ‰ΩøÁî®‰ΩìÈ™å„ÄÇÊàë‰∏çÁªèÂ∏∏‰π∞‰∏úË•øÔºå‰ΩÜÂ¶ÇÊûúË¶Å‰π∞‰ªÄ‰πàÔºå‰ºöÂ∞ΩÈáèÂéªÊÉ≥Ê∏ÖÊ•ö‰∏∫‰ªÄ‰πàË¶Å‰π∞Ôºå‰ºöÊèêÂâçÂéª‰∫ÜËß£Ëøô‰∏™‰∏úË•øÁöÑËÉåÊôØÔºåÂäüËÉΩÔºåËØÑ‰ª∑Ôºå‰ΩøÁî®ËøáÁ®ã‰∏≠‰ºö‰∏çÊñ≠Êõ¥Êñ∞ÊúÄÂàùÁöÑËÆæÊÉ≥ÔºåÂπ∂ÊåÅÁª≠ÂΩ±ÂìçÊàë‰∏ã‰∏ÄÊ¨°Ë¥≠Áâ©ÁöÑÂà§Êñ≠„ÄÇËøô‰πüÂ∞±ÊòØ‰∏ÄÊ¨°ÊúâÊÑèËØÜÊúâÁõÆÁöÑËÉΩÂΩ±ÂìçÊú™Êù•ÁöÑË¥≠Áâ©‰ΩìÈ™å„ÄÇ‰πüÂ∞±ÊòØËØ¥Ôºå‰ΩúËÄÖÊé®Â¥áÁöÑÊòØ‰∏ÄÁßçÊúâÁõÆÁöÑËÉΩÈáçÊ∏©„ÄÅÁîöËá≥ÂéÜ‰πÖÂº•Êñ∞ÁöÑËØª‰π¶‰ΩìÈ™å„ÄÇÂÆûÈôÖ‰∏äÔºå‰ΩúËÄÖ‰πüËÆ§‰∏∫ÔºåËøôÊ†∑ÁöÑËØª‰π¶‰ΩìÈ™åÊØî‰π¶Êú¨Ë∫´ÈáçË¶ÅÂ§ö‰∫Ü„ÄÇ&lt;/p&gt;

&lt;p&gt;‰ΩúËÄÖÊÄªÁªì‰∫Ü‰∏ãÈù¢‰∫î‰∏™ÂÖ∑‰ΩìÂèØË°åÁöÑÊ≠•È™§Êù•ÂàõÈÄ†ÊâÄË∞ìÁöÑËØª‰π¶‰ΩìÈ™åÔºö&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;ÈÄâ‰π¶&lt;/strong&gt;ÔºöÊî∂ÈõÜÈöèÊÉ≥ÔºåÂª∫Á´ãÁõÆÁöÑ&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ë¥≠‰π¶&lt;/strong&gt;ÔºöÂÜ∑ÈùôËØÑ‰º∞Ôºå‰π¶Á±çÁ°ÆËÆ§&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ËØª‰π¶&lt;/strong&gt;ÔºöÈÄÇÂΩìÊ†áËÆ∞ÔºåÊèêÁÇºÈáçÁÇπ&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ËÆ∞ÂΩï&lt;/strong&gt;ÔºöÂéüÊñáÊëòÊäÑÔºåÂéüÂàõÊÄùËÄÉ&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ê¥ªÁî®&lt;/strong&gt;ÔºöÈáçËØªÁ¨îËÆ∞ÔºåÊÄùÊÉ≥ËæìÂá∫&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ËÄåÂú®‰ΩúËÄÖÁöÑÂøÉ‰∏≠ÔºåÂàõÈÄ†Ëøô‰∏™ËØª‰π¶‰ΩìÈ™åÂøÖ‰∏çÂèØÂ∞ëÁöÑ‰ºô‰º¥ÊòØ‰∏Ä‰∏™ÊôÆÈÄöÁöÑÁ¨îËÆ∞Êú¨„ÄÇÂõ†‰∏∫ÂÆÉÂú®‰∏äÈù¢‰∫î‰∏™Ê≠•È™§‰∏≠ÊâÄÂèëÊå•ÁöÑÈáçË¶Å‰ΩúÁî®ÔºåËøôÁ¨îËÆ∞Êú¨Â∫îÊó∂Â∏∏‰º¥Êàë‰ª¨Â∑¶Âè≥ÔºåÂπ∂‰∏îÔºåÂ¶ÇÊûúÁõ¥ËØëÊú¨‰π¶ÁöÑÂéü‰π¶ÂêçÔºå‰Ω†‰ºöÂèëÁé∞ÔºåÂÆÉÂÖ∂ÂÆûÊÑèÊÄùÊòØ‚ÄúËØ∑Áî®‰∏Ä‰∏™Á¨îËÆ∞Êú¨Êï¥ÁêÜ‰Ω†ÁöÑËØª‰π¶‚Äù„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;Áî®Ë¥≠‰π¶Ê∏ÖÂçïÊåáÂêçË¥≠‰π¶&quot;&gt;Áî®Ë¥≠‰π¶Ê∏ÖÂçïÊåáÂêçË¥≠‰π¶&lt;/h3&gt;
&lt;p&gt;ÂàõÈÄ†ËØª‰π¶‰ΩìÈ™åÁöÑÁ¨¨‰∏ÄÊ≠•ÔºåÊòØÁªôËá™Â∑±ÂºÄÂá∫‰∏Ä‰∏™Ë¥≠‰π¶Ê∏ÖÂçï„ÄÇÂºÄÊ∏ÖÂçï‰∏çÊòØ‰∏∫‰∫ÜÈÄõ‰π¶Â∫óÁöÑÊó∂ÂÄôÂÆπÊòìÊâæÔºà‰πüÊúâËøôÂ•ΩÂ§ÑÔºâÔºåÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËÆ©Êàë‰ª¨ËÉΩÊ∏ÖÊ•öËÆ§ËØÜÂà∞ËØªÊØè‰∏ÄÊú¨‰π¶ÁöÑÁõÆÁöÑÊòØ‰ªÄ‰πà„ÄÇ‰ΩúËÄÖÊòØËøô‰πàËØ¥ÁöÑÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ÈÇ£‰πàÔºå‰∏∫‰ªÄ‰πàË¶ÅÊääÂàóÊ∏ÖÂçïÁöÑËøáÁ®ã‰πü‰Ωú‰∏∫ËØª‰π¶ÊñπÊ≥ïÁöÑ‰∏ÄÈÉ®ÂàÜÊù•ËØ¥ÊòéÂë¢ÔºüÁêÜÁî±‰πã‰∏ÄÔºåÂ∞±ÊòØË¶ÅÂüπÂÖªÂ∏¶ÁùÄÁõÆÁöÑÂéªËØª‰π¶ÁöÑÁõÆÁöÑÊÑèËØÜ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;‰∏ãÈù¢ÊòØ‰ΩúËÄÖÊé®ËçêÁöÑÈÄâ‰π¶Ë¥≠‰π¶ÁöÑÂÖ∑‰ΩìÊìç‰ΩúÊ≠•È™§Ôºö&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Â•ΩÂ•áÂøÉÊøÄÂèë&lt;/strong&gt; ‚Üí &lt;strong&gt;ÈöèÊÉ≥Á¨îËÆ∞&lt;/strong&gt; ‚Üí &lt;strong&gt;Ë¥≠‰π¶Ê∏ÖÂçï&lt;/strong&gt; ‚Üí &lt;strong&gt;Ë¥≠‰π¶&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Á¨¨‰∏ÄÊ≠•ÊòØÂ∞ÜÊøÄÂèëÂ•ΩÂ•áÂøÉÁöÑÊ∫êÂ§¥ËÆ∞ËøõÁ¨îËÆ∞Êú¨ÈáåÔºåÂèØ‰ª•Âè´ÂÅöÈöèÊÉ≥Á¨îËÆ∞„ÄÇËøôÊ∫êÂ§¥ÁöÑÂèØËÉΩÊÄßÂ∞±ÂæàÂ§ö‰∫ÜÔºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Êä•Âàä‰∏äËØªÂà∞ÊúâÊÑèÊÄùÁöÑ‰π¶ËØÑ&lt;/li&gt;
  &lt;li&gt;Âê¨Âà∞ÊÑüÂÖ¥Ë∂£ÁöÑÊîøÊ≤ªÊó∂‰∫ãËØÑËÆ∫&lt;/li&gt;
  &lt;li&gt;Êù•Ëá™ÊúãÂèãÁöÑ‰π¶Á±çÊé®Ëçê&lt;/li&gt;
  &lt;li&gt;Á≠âÁ≠â&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Âè™Ë¶ÅÊòØÊøÄËµ∑‰∫ÜÊàë‰ª¨Â•ΩÂ•áÂøÉÁöÑ‰∏úË•øÔºåÈÉΩÂ∫îËØ•ËÆ∞ËøõÈöèÊÉ≥Á¨îËÆ∞ÈáåÂéª„ÄÇ‰πãÂêé‰æøËÉΩÊ†πÊçÆÈöèÊÉ≥Á¨îËÆ∞ÔºåÂª∫Á´ãËµ∑ËØª‰π¶ÁöÑÁõÆÁöÑÔºåÂπ∂ÂéªÂØªÊâæÁõ∏ÂÖ≥‰π¶Á±ç„ÄÇÂú®ÁªèËøáÂÜ∑ÈùôÁöÑËØÑ‰º∞‰πãÂêéÔºåÂ∞ÜÊÉ≥Ë¶ÅË¥≠‰π∞ÁöÑ‰π¶Á±çÂàóËøõÊ∏ÖÂçï„ÄÇËøôÊµÅÁ®ãÁöÑÂ•ΩÂ§ÑÊòØÔºåÊàë‰ª¨Áõ∏ÂØπËÉΩÂ§üÂáÜÁ°ÆÂú∞ÈÄâÂá∫Ëá™Â∑±ÁúüÊ≠£ÊÉ≥ËØªÂπ∂‰∏îÊòéÁôΩ‰∏∫‰ªÄ‰πàÊÉ≥ËØªÁöÑ‰π¶ÔºåËøôÊ†∑‰π∞ÂõûÊù•ÊÑüÂÖ¥Ë∂£„ÄÅËØª‰∏ãÂéªÁöÑÂá†ÁéáÈÉΩÊØîËæÉÈ´òÔºàËøô‰∏™ÂæàÈáçË¶ÅüòÇÔºâ„ÄÇÂπ∂‰∏îÔºåÂú®ËØª‰π¶ËøáÁ®ã‰∏≠ÔºåÂèØ‰ª•Â∏¶ÁùÄÊúÄÂàùË¢´ÊøÄÂèëÁöÑÂ•ΩÂ•áÂøÉÂéªËØªÔºåËØªÂÆå‰πãÂêé‰πüËÉΩÂõûÈ°æ‰∏ÄÂºÄÂßãÂ•ΩÂ•áÂøÉË¢´ÊøÄÂèëÁöÑÂ•ëÊú∫ÔºåÂõ†‰∏∫Ëøô‰∫õÈÉΩË¢´‰∏Ä‰∏ÄËÆ∞ÂΩïÂú®Á¨îËÆ∞Êú¨Èáå„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;Áî®Á¨îËÆ∞ÊääËØªËøáÁöÑ‰π¶Âèò‰∏∫Á≤æÁ•ûË¥¢ÂØå&quot;&gt;Áî®Á¨îËÆ∞ÊääËØªËøáÁöÑ‰π¶Âèò‰∏∫Á≤æÁ•ûË¥¢ÂØå&lt;/h3&gt;
&lt;p&gt;ËøôÈáåËÆ≤ÁöÑÂåÖÂê´‰∫ÜÊ≠•È™§‰∏âÂíåÂõõÔºöËØª‰π¶ÔºåËÆ∞ÂΩï„ÄÇ&lt;/p&gt;

&lt;p&gt;ËØª‰π¶ÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÈÅµÂæ™ËøôÊ†∑‰∏Ä‰∏™ÊµÅÁ®ãÔºöÈÄöËØª ‚Üí ÁâáÊÆµÈáçËØª ‚Üí Ê†áËÆ∞„ÄÇÊÑèÊÄùÂ∞±ÊòØÔºåÈÄöËØª‰πãÂêéÔºåÂéªÈáçËØª‰Ω†ÊÑüÂà∞ÊúâÂÖ±È∏£„ÄÅÁñëÊÉë„ÄÅÊÑüÂÖ¥Ë∂£Á≠âÁ≠âÁöÑÁâáÊÆµÔºåÊúâÂøÖË¶ÅÂ∞±Áî®Áªü‰∏ÄÁ¨¶Âè∑Ê†áËÆ∞‰∏ãÊù•ÔºåÊØîÂ¶ÇÔºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‰∏ãÂàíÁõ¥Á∫ø ÔºøÔºøÔºöÂÆ¢ËßÇÈáçË¶Å&lt;/li&gt;
  &lt;li&gt;‰∏ãÂàíÊ≥¢Êµ™Á∫ø À∑À∑À∑À∑À∑À∑ÔºöÊàëËßâÂæóÈáçË¶ÅÔºåÈùûÂ∏∏ÈáçË¶Å&lt;/li&gt;
  &lt;li&gt;ÂúÜÂúà ‚óØÔºöÂÖ≥ÈîÆËØç„ÄÅ‰∏ì‰∏öÂêçÁß∞&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ËøôÊ†∑ËØªËøá‰∏Ä‰∏™Á´†ËäÇ„ÄÅ‰∏ÄÊú¨‰π¶ÔºåÂ∞±ËÉΩËøõÂÖ•Âà∞‰∏ã‰∏Ä‰∏™Ê≠•È™§ÔºöËÆ∞ÂΩï„ÄÇËøô‰∏™Êó∂ÂÄôÂ∞±ÂèØ‰ª•‰∏Ä‰∏ÄÂõûÈ°æ‰∏ä‰∏ÄÊ≠•ÊâÄÊ†áËÆ∞Âá∫Êù•ÁöÑÈÉ®ÂàÜÔºåÂèÇÁÖß‰∏ãÈù¢ÁöÑÊ†ºÂºèÔºåÂú®Á¨îËÆ∞Êú¨‰∏äÂÜô‰∏ãËØªËøáËøôÊú¨‰π¶ÂêéÁöÑËØª‰π¶Á¨îËÆ∞Ôºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‚ö¨ ‚Ä¶Êé•ÂéüÊñáÊëòÊäÑ„ÄÅË¶ÅÁÇπÊ¶ÇÊã¨&lt;/li&gt;
  &lt;li&gt;‚≠ë ‚Ä¶Êé•ËØÑËÆ∫„ÄÅÊÑüÊÉ≥&lt;/li&gt;
  &lt;li&gt;ÈáçÂ§ç‰∏äÈù¢&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÊëòÊäÑÁöÑÊó∂ÂÄôÔºåÂèØ‰ª•ÊëòÊäÑ‰∫õ‰ªÄ‰πàÂë¢Ôºü&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ËÉΩËÆ©Êàë‰∏ªËßÇ‰∫ßÁîüÂÖ±È∏£ÁöÑ&lt;/li&gt;
  &lt;li&gt;‰∏çÊòØËØªÂêéËßâÂæó‚ÄùÁêÜÂ∫îÂ¶ÇÊ≠§‚ÄúÔºåËÄåÊòØ‚ÄúËøô‰πà‰∏ÄËØ¥Á°ÆÂÆûÂ¶ÇÊ≠§‚ÄùÁöÑ&lt;/li&gt;
  &lt;li&gt;ËÉΩÈ¢†Ë¶ÜÊàëÂ∑≤ÊúâÁöÑÊÉ≥Ê≥ï„ÄÅÂä®ÊëáÊàëËÆ§ËØÜÁöÑ&lt;/li&gt;
  &lt;li&gt;Á≠âÁ≠â&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ÊëòÊäÑÊàñË¶ÅÁÇπÊ¶ÇÊã¨ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂéü‰ΩúËÄÖÁöÑÊÄùÊÉ≥ÔºåËÄåÂú®ËØÑËÆ∫ÈáåÔºåÊàë‰ª¨Â∫îËØ•Â∞ΩÈáèÂéªÊåñÊéò‰∫õÂéüÂàõÁöÑÊÄùËÄÉ„ÄÇËøô‰ºöÊòØ‰∏™ËÄóÊó∂Èó¥Ë¥πÁ≤æÂäõÁöÑËøáÁ®ãÔºå‰∏çËøáÂè™ÊúâËØïËøáÁöÑ‰∫∫ÊâçËÉΩÁü•ÈÅìÂà∞Â∫ïÂÄº‰∏çÂÄºÂæó„ÄÇÂè¶Â§ñÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºå‰ΩúËÄÖËøòÊèêÂÄ°Â∞ÜË∑üËøôÊú¨‰π¶ÊúâÂÖ≥ÁöÑÊó∂Èó¥„ÄÅÁ©∫Èó¥Âç∞ËÆ∞‰πü‰∏ÄËµ∑Ë¥¥ËøõÁ¨îËÆ∞Êú¨ÈáåÔºåÊØîÂ¶ÇËØ¥Êñ∞‰π¶‰π∞Êù•Êó∂ÁöÑ‰π¶ËÖ∞„ÄÅÁúãÈÇ£Êú¨‰π¶Êó∂ÊâÄÂùêÁÅ´ËΩ¶ÁöÑÁ•®Ê†πÁ≠âÁ≠â„ÄÇ‰ª•ÂêéÁöÑÊüê‰∏™Êó∂Èó¥ÔºåÂÜçÊ¨°ËØªËµ∑Á¨îËÆ∞Êú¨ÁöÑËøô‰∏ÄÈ°µÔºåÁúãÈÇ£Êú¨‰π¶Êó∂ÊâÄÁªèËøáÁöÑÈ£éÊôØÈóªËøáÁöÑËä±È¶ô‰πü‰ºöË∑ÉÁÑ∂Á∫∏‰∏äÂêß„ÄÇ&lt;/p&gt;

&lt;p&gt;ÂΩìÁÑ∂‰∫ÜÔºå‰∏äÈù¢ËÆ≤ÁöÑËÆ∞ÂΩïÁöÑÂΩ¢ÂºèÈÉΩÊòØ‰ΩúËÄÖÁöÑÊé®ËçêÔºåÊàë‰ª¨Â§ßÂèØ‰∏çÂøÖÂ±Ö‰∫éÊüêÁßçÂΩ¢ÂºèÔºåÂè™ÈúÄÊåâÁÖßËá™Â∑±ËàíÊúçÁöÑÊñπÂºèÂùöÊåÅËÆ∞‰∏ãÂ±û‰∫éËá™Â∑±ÁöÑËØª‰π¶Á¨îËÆ∞Â∞±Â•Ω‰∫ÜÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ËØ¥Âè•ËÄÅÁîüÂ∏∏Ë∞àÁöÑËØùÔºåÂè™ÊúâÊääËØª‰π¶Á¨îËÆ∞ÊéßÂà∂Âú®Ëá™Â∑±ËÉΩÂäõÂÖÅËÆ∏ÁöÑËåÉÂõ¥ÂÜÖÔºåÊâçËÉΩÈïø‰πÖÂú∞ÂùöÊåÅ‰∏ãÂéª„ÄÇÊâÄ‰ª•ÔºåË¶ÅÈÄâÊã©ÂØπËá™Â∑±Êù•ËØ¥ÊØîËæÉÊñπ‰æøÁöÑÁ¨îËÆ∞ÊñπÊ≥ï„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÊàëÊÉ≥ËøôÈÅìÁêÜÂá†‰πéÈÄÇÁî®‰∫éÊâÄÊúâÈúÄË¶ÅÈïø‰πÖÂùöÊåÅÁöÑ‰∫ãÔºåÊé¢Á©∂‰∏ÄÈó®Ê∑±Â••ÁöÑÂ≠¶ÈóÆ„ÄÅÂ≠¶‰π†‰∏ÄÈó®Â§ñËØ≠„ÄÅ‰∫¶ÊàñÊòØÂÅ•Ë∫´ÂáèËÇ•Á≠âÁ≠â„ÄÇÊàëÂèëÁé∞‰∫∫ÂæÄÂæÄËÉΩËΩªÊùæÁúãÂà∞Êº´ÈïøËøáÁ®ã‰πãÂêéÁöÑ‰∏ÄÁßçÁä∂ÊÄÅÔºåËøôÊàñËÆ∏ÊòØÊàë‰ª¨ËøôÊ†∑È´òÁ≠âÁîüÁâ©ÁöÑÁâπÊÆäËÉΩÂäõÔºõ‰ΩÜ‰∫∫ÂèàÂæÄÂæÄÂøç‰∏ç‰Ωè‰ºöÂØπÊúüÂæÖÁöÑÁä∂ÊÄÅËøá‰∫éÁùÄÊÄ•„ÄÇËøôÁöÑÁ°Æ‰∏éÊàë‰ª¨Ë∫´Â§ÑÁöÑÁ§æ‰ºöÁéØÂ¢ÉÊúâÊâÄÂÖ≥Á≥ªÔºå‰ΩÜÂú®Êàë‰ª¨ÁöÑÂü∫Âõ†ÂΩì‰∏≠ÊòØÂê¶‰πüÊúâÁùÄËøôÁßçÊó¢ÊúâËøúËßÅÂèà‰ºÅÊ±ÇËß¶ÊâãÂèØÂæóÁöÑÂõûÊä•ÁöÑÁßçÂ≠êÂë¢Ôºü&lt;/p&gt;

&lt;p&gt;ÊàëÂú®ËØªËøôÊú¨‰π¶ÁöÑÊó∂ÂÄôÊâÄÂÅöÁöÑËØª‰π¶Á¨îËÆ∞Â∞±Ê≤°ÊúâÊåâÁÖß‰ΩúËÄÖÊé®ËçêÁöÑÊ†ºÂºèÔºåËÄåÊòØÈááÁî®‰∫ÜËá™Â∑±‰π†ÊÉØÁöÑÁ±ª‰ºº‰∫éPPTËÆæËÆ°ÁöÑÈ£éÊ†ºÔºö
&lt;img src=&quot;/assets/2019-11-12/how_to_read.jpg&quot; alt=&quot;how_to_read&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‰∏çÈöæÁúãÂá∫ÔºåÁ¨îËÆ∞ÈáåÁöÑÁªìÊûÑÂá†‰πéÂéüÂ∞Å‰∏çÂä®Âú∞ÂèòÊàê‰∫ÜÊàëËøôÁØáÊñáÁ´†ÁöÑÁªìÊûÑÔºåÂÜçÂä†‰∏äÂú®‰π¶ÈáåÁõ∏ÂÖ≥Ê†áËÆ∞ÂÜô‰∏ãÁöÑËØÑËÆ∫ÔºåËøôÁØáÊñáÁ´†ÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÂú®ÊàëÂÅöÂÆåËØª‰π¶Á¨îËÆ∞ÁöÑÂêåÊó∂‰πüÂ∞±ÂÆåÊàê‰∫Ü„ÄÇËÄåËøô‰πüÊòØ‰ΩúËÄÖÊé®Â¥áÁöÑÔºå‰ª•Ëá™Â∑±ÁöÑËØª‰π¶Á¨îËÆ∞‰∏∫Âü∫Á°ÄÔºåËøõ‰∏ÄÊ≠•ÂÜôÂá∫ÂéüÂàõÊñáÁ´†ÔºåÂÅöÂ±û‰∫éËá™Â∑±ÁöÑÊÄùÊÉ≥ÁöÑËæìÂá∫„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;ÈÄöËøáÈáçËØªÁ¨îËÆ∞ÊèêÈ´òËá™Êàë&quot;&gt;ÈÄöËøáÈáçËØªÁ¨îËÆ∞ÊèêÈ´òËá™Êàë&lt;/h3&gt;
&lt;p&gt;ËØª‰π¶‰ΩìÈ™åÁöÑÊúÄÂêé‰∏ÄÊ≠•Ôºå‰πüÊòØÊàë‰ªéÊ≤°ÂÅöËøáÁöÑ‰∏ÄÊ≠•ÔºöÈáçËØªÁ¨îËÆ∞„ÄÇÂ∞±ÂÉèÊúâ‰∫∫‰ºöÂÅ∂Â∞îÈáçËØªÊó•ËÆ∞‰∏ÄÊ†∑ÔºåÊó∂Â∏∏ÈáçËØªËØª‰π¶Á¨îËÆ∞ËÉΩËÆ©Ëá™Â∑±ËØªËøáÁöÑ‰π¶Â•ΩÂÉè‰∏ÄÁõ¥Â≠òÂú®Ëá™Â∑±ÁîüÊ¥ª‰∏≠Ôºå‰∏çÊñ≠ÈÖùÈÖøÔºå‰∏çÊñ≠Ë∑üËá™Â∑±ÁöÑÁªèÂéÜ„ÄÅÁü•ËØÜÂèëÁîüÊñ∞ÁöÑÁ¢∞ÊíûÔºö&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Â¶ÇÊûúÊää‰∏ÄÊú¨‰π¶ÊØî‰Ωú‰∏Ä‰∏™‚ÄúÂú∫ÊâÄ‚ÄùÔºåÈÇ£‰πàËØª‰π¶Á¨îËÆ∞Â∞±ÊòØÂú®Ëøô‰∏™Âú∫ÊâÄÊãçÊëÑÁöÑÁÖßÁâá„ÄÇÂú®‰∏çÂêåÊó∂Èó¥ÂéªÂêå‰∏Ä‰∏™Âú∫ÊâÄÊãçÁÖßÔºåÊãçÂá∫Êù•ÁöÑÁÖßÁâáÈÉΩ‰ºöÊúâÊâÄ‰∏çÂêåÔºåËÄåËøá‰∏ÄÊÆµÊó∂Èó¥ÂÜçÂéªÁúãËøô‰∫õÁÖßÁâáÔºåÂØπÈÇ£‰∏™Âú∫ÊâÄÁöÑÂç∞Ë±°‰πü‰ºöÂèëÁîüÂèòÂåñ„ÄÇ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ÂÖ≥‰∫éÂ¶Ç‰ΩïÈáçËØªÔºå‰ΩúËÄÖÊé®ËçêÔºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ÁÆÄÂçïÂõûÈ°æÔºöËØªÁ¨îËÆ∞&lt;/li&gt;
  &lt;li&gt;ÁªÜËá¥ÂõûÈ°æÔºöËØªÁ¨îËÆ∞ + Âéü‰π¶Ê†áËÆ∞&lt;/li&gt;
  &lt;li&gt;ÁªèÂÖ∏ÈáçÊ∏©ÔºöËØªÁ¨îËÆ∞ + Âéü‰π¶&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ëøô‰πàÁúãÔºåÊàëÁ°ÆÂÆû‰∏Ä‰∏™ÂõûÈ°æÈÉΩÊ≤°ÂÅöËøáüòÇ„ÄÇ‰∏ÄÂºÄÂßãËØªËøôÊú¨‰π¶ÊòØÂõ†‰∏∫‰π∞‰∫ÜÊñ∞ÁöÑÁ¨îËÆ∞Êú¨ÔºåÊàëËÄÅÊòØ‰π∞Êñ∞Á¨îËÆ∞Êú¨ÔºåÊÉ≥ÁùÄË¶ÅÂÜôÁÇπ‰ªÄ‰πàÊñáÂ≠óÔºåÊúÄÂêéÈÉΩÊ≤¶‰∏∫Âπ≥Êó∂Â∑•‰ΩúÁî®ÁöÑËçâÁ®øÁ∫∏Ôºà‰πüÊòØÂæàÈáçË¶ÅÂï¶Ôºâ„ÄÇÁúã‰∫ÜËøôÊú¨‰π¶ÁöÑËØÑËÆ∫ËßâÂæóÂèØËÉΩ‰ºöÂ∏ÆÊàëÁªìÊùüËøô‰∏™Âæ™ÁéØÔºåÁõÆÂâçÁúãÊù•ÂæàÊúâÂ∏åÊúõ„ÄÇËØª‰πãÂâçÔºåÂ¶Ç‰ΩïÂÜôËØª‰π¶Á¨îËÆ∞ÊòØÊàëÊúÄÊÑüÂÖ¥Ë∂£ÁöÑÈÉ®ÂàÜÔºå‰∏çËøáËØªÂÆåÂèëÁé∞Ôºå‰ª•ÈöèÊÉ≥Á¨îËÆ∞Âà∞Á¨îËÆ∞ÂõûÈ°æÁöÑ‰∏ÄÊï¥‰∏™ËØª‰π¶‰ΩìÈ™åÊù•ÁêÜËß£ËØª‰π¶Ëøô‰∫ãÂÑøÔºåÊâçÊòØÂ••ÈáéÂÆ£‰πãËøôÊú¨‰π¶ÁªôÊàëÊúÄÂ§ßÁöÑÊî∂Ëé∑Âêß„ÄÇ&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linear discriminant analysis, explained</title>
   <link href="http://localhost:4000/data/2019/10/02/linear-discriminant-analysis.html"/>
   <updated>2019-10-02T08:00:00+08:00</updated>
   <id>http://localhost:4000/data/2019/10/02/linear-discriminant-analysis</id>
   <content type="html">&lt;p&gt;&lt;em&gt;Intuitions, illustrations, and maths: How it‚Äôs more than a dimension reduction tool and why it‚Äôs robust for real-world applications.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-02/mda.png&quot; alt=&quot;mda&quot; /&gt; This graph shows that boundaries (blue lines) learned by mixture discriminant analysis (MDA) successfully separate three mingled classes. MDA is one of the powerful extensions of LDA.&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Linear discriminant analysis (LDA) is not just a dimension reduction tool, but also a robust classification method.&lt;/li&gt;
  &lt;li&gt;With or without data normality assumption, we can arrive at the same LDA features, which explains its robustness.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;LDA is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed.&lt;/p&gt;

&lt;p&gt;Two prominent examples of using LDA (and it‚Äôs variants) include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Bankruptcy prediction&lt;/em&gt;: Edward Altman‚Äôs &lt;a href=&quot;https://en.wikipedia.org/wiki/Altman_Z-score&quot;&gt;1968 model&lt;/a&gt; predicts the probability of company bankruptcy using trained LDA coefficients. The accuracy is said to be between 80% and 90%, evaluated over 31 years of data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Facial recognition&lt;/em&gt;: While features learned from Principal Component Analysis (PCA) are called Eigenfaces, those learned from LDA are called &lt;a href=&quot;http://www.scholarpedia.org/article/Fisherfaces&quot;&gt;Fisherfaces&lt;/a&gt;, named after the statistician, Sir Ronald Fisher. We explain this connection later.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article starts by introducing the classic LDA and why it‚Äôs deeply rooted as a classification method. Next, we see the inherent dimension reduction in this method and how it leads to the reduced-rank LDA. After that, we see how Fisher masterfully arrived at the same algorithm, without assuming anything on the data. A hand-written digits classification problem is used to illustrate the performance of the LDA. The merits and disadvantages of the method are summarized in the end.&lt;/p&gt;

&lt;p&gt;The second article following this generalizes LDA to handle more complex problems. By the way, you can find a set of &lt;a href=&quot;/assets/2019-10-02/Discriminant_Analysis.pdf&quot; target=&quot;_blank&quot;&gt;corresponding slides&lt;/a&gt; where I present roughly the same materials written in this article.&lt;/p&gt;

&lt;h3 id=&quot;classification-by-discriminant-analysis&quot;&gt;Classification by discriminant analysis&lt;/h3&gt;
&lt;p&gt;Let‚Äôs see how LDA can be derived as a supervised classification method. Consider a generic classification problem: A random variable $X$ comes from one of $K$ classes, with density $f_k(\mathbf{x})$ on $\mathbb{R}^p$. A discriminant rule tries to divide the data space into $K$ disjoint regions $\mathbb{R}_1, \dots, \mathbb{R}_K$ that represent all classes (imagine the boxes on a chessboard). With these regions, classification by discriminant analysis simply means that we allocate $\mathbf{x }$ to class $j$ if $\mathbf{x}$ is in region $j$. The question is then, how do we know which region the data $\mathbf{x }$ falls in? Naturally, We can follow two allocation rules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Maximum likelihood rule&lt;/em&gt;: If we assume that each class could occur with equal probability, then allocate $\mathbf{x }$ to class $j$ if $j = \arg\max_i f_i(\mathbf{x})$.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bayesian rule&lt;/em&gt;: If we know the class prior probabilities, $\pi_1, \dots, \pi_K$, then allocate $\mathbf{x }$ to class $j$ if $j = \arg\max_i \pi_i f_i(\mathbf{x}) $.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-and-quadratic-discriminant-analysis&quot;&gt;Linear and quadratic discriminant analysis&lt;/h4&gt;
&lt;p&gt;If we assume data comes from multivariate Gaussian distribution, i.e. $X \sim N(\mathbf{\mu}, \mathbf{\Sigma})$, explicit forms of the above allocation rules can be obtained. Following the Bayesian rule, we classify $\mathbf{x}$ to class $j$ if $j = \arg\max_i \delta_i(\mathbf{x})$ where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
    \delta_i(\mathbf{x}) = \log f_i(\mathbf{x}) + \log \pi_i
\end{align}&lt;/script&gt;

&lt;p&gt;is called the discriminant function. Note the use of log-likelihood here.  The decision boundary separating any two classes, $k$ and $\ell$, is the set of $\mathbf{x}$ where two discriminant functions have the same value, i.e. &lt;script type=&quot;math/tex&quot;&gt;\{\mathbf{x}: \delta_k(\mathbf{x}) = \delta_{\ell}(\mathbf{x})\}&lt;/script&gt;. Therefore, any data that falls on the decision boundary is equally likely from the two classes.&lt;/p&gt;

&lt;p&gt;LDA arises in the case where we assume equal covariance among $K$ classes, i.e. $\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \dots = \mathbf{\Sigma}_K$. Then we can obtain the following discriminant function:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = \mathbf{x}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}-\frac{1}{2} \mathbf{\mu}_{k}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}+\log \pi_{k} \,,
    \label{eqn_lda}
\end{align}&lt;/script&gt; using the Gaussian distribution likelihood function.&lt;/p&gt;

&lt;p&gt;This is a linear function in $\mathbf{x}$. Thus, the decision boundary between any pair of classes is also a linear function in $\mathbf{x}$, the reason for its name: linear discriminant analysis. Without the equal covariance assumption, the quadratic term in the likelihood does not cancel out, hence the resulting discriminant function is a quadratic function in $\mathbf{x}$:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
    \delta_{k}(\mathbf{x}) = 
    - \frac{1}{2} \log|\mathbf{\Sigma}_k| 
    - \frac{1}{2} (\mathbf{x} - \mathbf{\mu}_{k})^{T} \mathbf{\Sigma}_k^{-1} (\mathbf{x} - \mathbf{\mu}_{k}) + \log \pi_{k} \,.
    \label{eqn_qda}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the decision boundary is quadratic in $\mathbf{x}$. This is known as quadratic discriminant analysis (QDA).&lt;/p&gt;

&lt;h4 id=&quot;which-is-better-lda-or-qda&quot;&gt;Which is better? LDA or QDA?&lt;/h4&gt;
&lt;p&gt;In real problems, population parameters are usually unknown and estimated from training data as $\hat{\pi}_k, \hat{\mathbf{\mu}}_k, \hat{\mathbf{\Sigma}}_k$. While QDA accommodates more flexible decision boundaries compared to LDA, the number of parameters needed to be estimated also increases faster than that of LDA. From (\ref{eqn_lda}), $p+1$ parameters (nonlinear transformation of the original distribution parameters) are needed to construct the discriminant function. For a problem with $K$ classes, we would only need $K-1$ such discriminant functions by arbitrarily choosing one class to be the base class, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{k}'(\mathbf{x}) = \delta_{k}(\mathbf{x}) - \delta_{K}(\mathbf{x})\,,&lt;/script&gt;

&lt;p&gt;for $k = 1, \dots, K-1$. Hence, the total number of estimated parameters for LDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)(p+1)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand, for each QDA discriminant function (\ref{eqn_qda}), mean vector, covariance matrix, and class prior need to be estimated:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mean: $p$&lt;/li&gt;
  &lt;li&gt;Covariance: $p(p+1)/2$&lt;/li&gt;
  &lt;li&gt;Class prior: 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The total number of estimated parameters for QDA is &lt;script type=&quot;math/tex&quot;&gt;(K-1)\{p(p+3)/2+1\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Therefore, the number of parameters estimated in LDA increases linearly with $p$ while that of QDA increases quadratically with $p$.&lt;/em&gt; We would expect QDA to have worse performance than LDA when the dimension $p$ is large.&lt;/p&gt;

&lt;h4 id=&quot;best-of-two-worlds-compromise-between-lda--qda&quot;&gt;Best of two worlds? Compromise between LDA &amp;amp; QDA&lt;/h4&gt;
&lt;p&gt;We can find a compromise between LDA and QDA by regularizing the individual class covariance matrices. Regularization means that we put a certain restriction on the estimated parameters. In this case, we require that individual covariance matrix shrinks toward a common pooled covariance matrix through a penalty parameter $\alpha$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.&lt;/script&gt;

&lt;p&gt;The pooled covariance matrix can also be regularized toward an identity matrix through a penalty parameter $\beta$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}} (\beta) = \beta \hat{\mathbf{\Sigma}} + (1-\beta) \mathbf{I} \,.&lt;/script&gt;

&lt;p&gt;In situations where the number of input variables greatly exceeds the number of samples, the covariance matrix can be poorly estimated. Shrinkage can hopefully improve estimation and classification accuracy. This is illustrated by the figure below.
&lt;img src=&quot;/assets/2019-10-02/lda_shrinkage.png&quot; alt=&quot;lda_shrinkage&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot, credit to &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html&quot;&gt;scikit-learn&lt;/a&gt;.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# samples for testing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# how often to repeat classification
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# maximum number of features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# step size for the calculation
&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Generate random blob-ish data with noisy features.

    This returns an array of input data with shape `(n_samples, n_features)`
    and an array of `n_samples` target labels.

    Only one feature contains discriminative information, the other features
    contain only noise.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add non-discriminative features
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lsqr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score_clf2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_averages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_train&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA with shrinkage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features_samples_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_clf2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LDA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_features / n_samples'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Classification accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'size'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;computation-for-lda&quot;&gt;Computation for LDA&lt;/h4&gt;
&lt;p&gt;We can see from (\ref{eqn_lda}) and (\ref{eqn_qda}) that computations of discriminant functions can be simplified if we diagonalize the covariance matrices first. That is, data are transformed to have an identity covariance matrix (no correlation, variance of 1). In the case of LDA, here‚Äôs how we proceed with the computation:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Perform eigen-decompostion on the pooled covariance matrix: &lt;script type=&quot;math/tex&quot;&gt;\hat{\mathbf{\Sigma}} = \mathbf{U}\mathbf{D}\mathbf{U}^{T} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Sphere the data:
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}^{*} \leftarrow \mathbf{D}^{-\frac{1}{2}} \mathbf{U}^{T} \mathbf{X} \,.&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Obtain class means in the transformed space: &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_1, \dots, \hat{\mu}_{K}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Classify $\mathbf{x}$ according to $\delta_{k}(\mathbf{x}^{*})$:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\delta_{k}(\mathbf{x}^{*})=\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k} \,.
\label{eqn_lda_sphered}
\end{align}&lt;/script&gt;

&lt;p&gt;Step 2 spheres the data to produce an identity covariance matrix in the transformed space. Step 4 is obtained by following (\ref{eqn_lda}). Let‚Äôs take a two-class example to see what LDA is doing. Suppose there are two classes, $k$ and $\ell$. We classify $\mathbf{x}$ to class $k$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &gt; 0.
\end{align}&lt;/script&gt;

&lt;p&gt;Following the four steps outlined above, we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &amp;= 
\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k}
- \mathbf{x^{*}}^{T} \hat{\mu}_{\ell} + \frac{1}{2} \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell} - \log \hat{\pi}_{k} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k}^{T}\hat{\mu}_{k} - \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&amp;&gt; 0 \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That is, we classify $\mathbf{x}$ to class $k$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) &gt; \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \,.&lt;/script&gt;

&lt;p&gt;The derived allocation rule reveals the working of LDA. The left-hand side of the equation is the length of the orthogonal projection of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x^{*}}&lt;/script&gt; onto the line segment joining the two class means. The right-hand side is the location of the center of the segment corrected by class prior probabilities. &lt;em&gt;Essentially, LDA classifies the data to the closest class mean.&lt;/em&gt; We make two observations here.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The decision point deviates from the middle point when the class prior probabilities are not the same, i.e., the boundary is pushed toward the class with a smaller prior probability.&lt;/li&gt;
  &lt;li&gt;Data are projected onto the space spanned by class means, e.g. &lt;script type=&quot;math/tex&quot;&gt;\hat{\mu}_{k} - \hat{\mu}_{\ell}&lt;/script&gt;. Distance comparisons are then done in that space.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reduced-rank-lda&quot;&gt;Reduced-rank LDA&lt;/h3&gt;
&lt;p&gt;What I‚Äôve just described is LDA for classification. LDA is also famous for its ability to find a small number of meaningful dimensions, allowing us to visualize and tackle high-dimensional problems. What do we mean by meaningful, and how does LDA find these dimensions? We will answer these questions shortly. First, take a look at the below plot. For a &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine&quot;&gt;wine classification&lt;/a&gt; problem with three different types of wines and 13 input variables, the plot visualizes the data in two discriminant coordinates found by LDA. In this two-dimensional space, the classes can be well-separated. In comparison, the classes are not as clearly separated using the first two principal components found by PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-02/lda_vs_pca.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script to generate the above plot.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'navy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'turquoise'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'darkorange'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LDA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PCA for Wine dataset'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PC 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h4 id=&quot;dimension-reduction-is-inherent-in-lda&quot;&gt;Dimension reduction is inherent in LDA&lt;/h4&gt;
&lt;p&gt;In the above wine example, a 13-dimensional problem is visualized in a 2d space. Why is this possible? This is possible because there‚Äôs an inherent dimension reduction in LDA. We have observed from the previous section that LDA makes distance comparison in the space spanned by different class means. Two distinct points lie on a 1d line; three distinct points lie on a 2d plane. Similarly, $K$ class means lie on a hyperplane with dimension at most $(K-1)$. In particular, the subspace spanned by the means is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{K-1}=\mu_{1} \oplus \operatorname{span}\left\{\mu_{i}-\mu_{1}, 2 \leq i \leq K\right\} \,.&lt;/script&gt;

&lt;p&gt;When making distance comparison in this space, distances orthogonal to this subspace would add no information since they contribute equally for each class. Hence, by restricting distance comparisons to this subspace only would not lose any information useful for LDA classification. That means, we can safely transform our task from a $p$-dimensional problem to a $(K-1)$-dimensional problem by an orthogonal projection of the data onto this subspace. When $p \gg K$, this is a considerable drop in the number of dimensions. What if we want to reduce the dimension further from $p$ to $L$ where $K \gg L$, e.g. two dimensional with $L = 2$? We can construct an $L$-dimensional subspace, $H_L$, from $H_{K-1}$, and this subspace is optimal, in some sense, for LDA classification.&lt;/p&gt;

&lt;h4 id=&quot;what-would-be-the-optimal-subspace&quot;&gt;What would be the optimal subspace?&lt;/h4&gt;
&lt;p&gt;Fisher proposes that the subspace $H_L$ is optimal when the class means of sphered data have maximum separation in this subspace in terms of variance. Following this definition, optimal subspace coordinates are simply found by doing PCA on sphered class means, since PCA finds the direction of maximal variance. The computation steps are summarized below:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Find class mean matrix, $\mathbf{M}_{(K\times p)}$, and pooled var-cov, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_{(p\times p)}&lt;/script&gt;, where&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
 \mathbf{W} = \sum_{k=1}^{K} \sum_{g_i = k} (\mathbf{x}_i - \hat{\mu}_k)(\mathbf{x}_i - \hat{\mu}_k)^T \,.
 \label{within_w}
 \end{align}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;Sphere the means: $\mathbf{M}^* = \mathbf{M} \mathbf{W}^{-\frac{1}{2}}$, using eigen-decomposition of $\mathbf{W}$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \operatorname{cov}(\mathbf{M}^*)&lt;/script&gt;, the between-class covariance of sphered class means by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{B}^* = \sum_{k=1}^{K} (\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)(\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)^T \,.&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;PCA: Obtain $L$ eigenvectors &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{v}^*_\ell)&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\mathbf{V}^*&lt;/script&gt; of 
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{B}^* = \mathbf{V}^* \mathbf{D_B} \mathbf{V^*}^T&lt;/script&gt; cooresponding to the $L$ largest eigenvalues. These define the coordinates of the optimal subspace.&lt;/li&gt;
  &lt;li&gt;Obtain $L$ new (discriminant) variables $Z_\ell = (\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell)^T X$, for $\ell = 1, \dots, L$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through this procedure, we reduce our data from &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_{(N \times p)}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Z}_{(N \times L)}&lt;/script&gt; and dimension from $p$ to $L$. Discriminant coordinate 1 and 2 in the previous wine plot are found by setting $L = 2$. Repeating the previous LDA procedures for classification using the new data, $\mathbf{Z}$, is called the reduced-rank LDA.&lt;/p&gt;

&lt;h3 id=&quot;fishers-lda&quot;&gt;Fisher‚Äôs LDA&lt;/h3&gt;
&lt;p&gt;If the derivation of the previous reduced-rank LDA looks very different to what you‚Äôve known before, you are not alone! Here comes the revelation. Fisher derived the computation steps according to his optimality definition in a different way&lt;sup id=&quot;fnref:Fisher&quot;&gt;&lt;a href=&quot;#fn:Fisher&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. His steps of performing the reduced-rank LDA would later be known as the Fisher‚Äôs discriminant analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fisher does not make any assumptions about the distribution of the data. Instead, he tries to find a ‚Äúsensible‚Äù rule so that the classification task becomes easier.&lt;/strong&gt; In particular, Fisher finds a linear combination of the original data, &lt;script type=&quot;math/tex&quot;&gt;Z = \mathbf{a}^T X&lt;/script&gt;, where the between-class variance, $\mathbf{B} = \operatorname{cov}(\mathbf{M})$, is maximized relative to the within-class variance, $\mathbf{W}$, as defined in (\ref{within_w}).&lt;/p&gt;

&lt;p&gt;The below plot, taken from ESL&lt;sup id=&quot;fnref:ESL&quot;&gt;&lt;a href=&quot;#fn:ESL&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, shows why this rule makes intuitive sense. The rule sets out to find a direction, $\mathbf{a}$, where, after projecting the data onto that direction, class means have maximum separation between them, and each class has minimum variance within them. The projection direction found under this rule, shown in the right plot, makes classification much easier.
&lt;img src=&quot;/assets/2019-10-02/sensible_rule.png&quot; alt=&quot;sensible_rule&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;finding-the-direction-fishers-way&quot;&gt;Finding the direction: Fisher‚Äôs way&lt;/h4&gt;
&lt;p&gt;Using Fisher‚Äôs sensible rule, finding the optimal projection direction(s) amounts to solving an optimization problem:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\max_{\mathbf{a}} \frac{\mathbf{a}^{T} \mathbf{B} \mathbf{a}}{\mathbf{a}^{T} \mathbf{W} \mathbf{a}} \,.
\end{align}&lt;/script&gt;
Recall that we want to find a direction where the between-class variance is maximized (the numerator) and the within-class variance is minimized (the denominator). This can be recasted as a generalized eigenvalue problem.&lt;/p&gt;

&lt;p&gt;The problem is equivalent to 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen}
\max_{\mathbf{a}} {}&amp;{} \mathbf{a}^{T} \mathbf{B} \mathbf{a} \,,\\ 
\text{s.t. } &amp;{} \mathbf{a}^{T} \mathbf{W} \mathbf{a} = 1 \,, \nonumber
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;since the scaling of $\mathbf{a}$ does not affect the solution.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{W}^{\frac12}$ be the symmetric square root of $\mathbf{W}$, and $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$. We can rewrite the problem (\ref{eqn_g_eigen}) as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_1}
\max_{\mathbf{y}} {}&amp;{} \mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} \,,\\ 
\text{s.t } &amp;{} \mathbf{y}^{T} \mathbf{y} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$ is symmetric, we can find the spectral decomposition of it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} = \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \,.
\label{eqn_fisher_eigen}
\end{align}&lt;/script&gt;

&lt;p&gt;Let $\mathbf{z} = \mathbf{\Gamma}^T \mathbf{y}$. So $\mathbf{z}^T \mathbf{z} = \mathbf{y}^T \mathbf{\Gamma} \mathbf{\Gamma}^T \mathbf{y} = \mathbf{y}^T \mathbf{y}$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} &amp;= \mathbf{y}^{T} \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \mathbf{y} \\
&amp;= \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Problem (\ref{eqn_g_eigen_1}) can then be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\label{eqn_g_eigen_2}
\max_{\mathbf{z}} {}&amp;{} \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} = \sum_i \lambda_i z_i^2 \,,\\ 
\text{s.t } &amp;{} \mathbf{z}^{T} \mathbf{z} = 1 \,. \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;If the eigenvalues are written in descending order, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\mathbf{z}} \sum_i \lambda_i z_i^2 &amp;\le \lambda_1 \sum_i z_i^2 \,,\\
&amp;= \lambda_1 \,,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the upper bound is attained at $\mathbf{z} = (1,0,0,\dots,0)^T$. Since $\mathbf{y} = \mathbf{\Gamma} \mathbf{z}$, the solution is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y} = \pmb \gamma_{(1)}&lt;/script&gt;, the eigenvector corresponding to the largest eigenvalue in (\ref{eqn_fisher_eigen}). Since $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$, the optimal projection direction is &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a} = \mathbf{W}^{-\frac12} \pmb \gamma_{(1)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem A.6.2&lt;/strong&gt; from MA&lt;sup id=&quot;fnref:MA&quot;&gt;&lt;a href=&quot;#fn:MA&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;: For &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}_(n \times p)&lt;/script&gt; and $\mathbf{B}_(p \times n)$, the non-zero eigenvalues of
$\mathbf{AB}$ and $\mathbf{BA}$ are the same and have the same multiplicity. If $\mathbf{x}$ is a non-trivial eigenvector of $\mathbf{AB}$ for an eigenvalue $\lambda \neq 0$, then $\mathbf{y}=\mathbf{Bx}$ is a non-trivial eigenvector of $\mathbf{BA}$.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\pmb \gamma_{(1)}&lt;/script&gt; is an eigenvector of $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$, then, $\mathbf{W}^{-\frac12} \pmb \gamma_{(1)}$ is also the eigenvector of $\mathbf{W}^{-\frac12} \mathbf{W}^{-\frac12} \mathbf{B} = \mathbf{W}^{-1} \mathbf{B}$, using &lt;strong&gt;Theorem A.6.2&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In summary, optimal subspace coordinates, also known as discriminant coordinates, are obtained from the eigenvectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-1}\mathbf{B}&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;\ell = 1, ... , \min\{p,K-1\}&lt;/script&gt;.&lt;/em&gt; It can be shown that the &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}_\ell&lt;/script&gt;s obtained are the same as &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell&lt;/script&gt;s obtained in the reduced-rank LDA formulation.&lt;/p&gt;

&lt;p&gt;Surprisingly, Fisher arrives at this formulation without any Gaussian assumption on the population, unlike the reduced-rank LDA formulation. The hope is that, with this sensible rule, LDA would perform well even when the data do not follow exactly the Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;handwritten-digits-problem&quot;&gt;Handwritten digits problem&lt;/h2&gt;
&lt;p&gt;Here‚Äôs an example to show the visualization and classification ability of Fisher‚Äôs LDA, or simply LDA. We need to recognize ten different digits, i.e., 0 to 9, using 64 variables (pixel values from images). The dataset is taken from &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, we can visualize the training images and they look like these: 
&lt;img src=&quot;/assets/2019-10-02/digits.png&quot; alt=&quot;digits&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.discriminant_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_digits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images_and_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;i'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Next, we train an LDA classifier on the first half of the data. Solving the generalized eigenvalue problem mentioned previously gives us a list of optimal projection directions. In this problem, we keep the top 4 coordinates, and the transformed data are shown below. 
&lt;img src=&quot;/assets/2019-10-02/reduced_lda_digits.png&quot; alt=&quot;lda_vs_pca&quot; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a classifier: a Fisher's LDA classifier
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearDiscriminantAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'eigen'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shrinkage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train lda on the first half of the digits
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Visualize transformed data on learnt discriminant coordinates
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'seaborn-talk'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_r_lda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'$&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.f$'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Discriminant Coordinate 4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The above plot allows us to interpret the trained LDA classifier. For example, coordinate 1 helps to contrast 4‚Äôs and 2/3‚Äôs while coordinate 2 contrasts 0‚Äôs and 1‚Äôs. Subsequently, coordinate 3 and 4 help to discriminate digits not well-separated in coordinate 1 and 2. We test the trained classifier using the other half of the dataset. The report below summarizes the result.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;¬†&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;precision&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;recall&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;f1-score&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.90&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.94&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.91&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.99&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.98&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.96&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.92&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.77&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.95&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.85&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;avg / total&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.93&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;899&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;details&gt;
&lt;summary&gt;Click here for the script.&lt;/summary&gt;
&lt;div&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Predict the value of the digit on the second half:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Classification report:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;The highest precision is 99%, and the lowest is 77%, a decent result knowing that the method was proposed some 70 years ago. Besides, we have not done anything to make the procedure better for this specific problem. For example, there is collinearity in the input variables, and the shrinkage parameter might not be optimal.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-lda&quot;&gt;Summary of LDA&lt;/h2&gt;
&lt;p&gt;Here I summarize the virtues and shortcomings of LDA.&lt;/p&gt;

&lt;p&gt;Virtues of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Simple prototype classifier: Distance to the class mean is used, it‚Äôs simple to interpret.&lt;/li&gt;
  &lt;li&gt;Decision boundary is linear: It‚Äôs simple to implement and the classification is robust.&lt;/li&gt;
  &lt;li&gt;Dimension reduction: It provides informative low-dimensional view on
the data, which is both useful for visualization and feature engineering.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Shortcomings of LDA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired.&lt;/li&gt;
  &lt;li&gt;In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired.&lt;/li&gt;
  &lt;li&gt;Support for more complex prototype classification is desired.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the next article, flexible, penalized, and mixture discriminant analysis will be introduced to address each of the three shortcomings of LDA. With these generalizations, LDA can take on much more difficult and complex problems.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Fisher&quot;&gt;
      &lt;p&gt;Fisher, R. A. (1936). &lt;em&gt;The Use Of Multiple Measurements In Taxonomic Problems. Annals of eugenics&lt;/em&gt;, 7(2), 179-188.¬†&lt;a href=&quot;#fnref:Fisher&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ESL&quot;&gt;
      &lt;p&gt;Friedman, J., Hastie, T., &amp;amp; Tibshirani, R. (2001). &lt;em&gt;The Elements Of Statistical Learning&lt;/em&gt; (Vol. 1, No. 10). New York: Springer series in statistics.¬†&lt;a href=&quot;#fnref:ESL&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:MA&quot;&gt;
      &lt;p&gt;Mardia, K. V., Kent, J. T., &amp;amp; Bibby, J. M. &lt;em&gt;Multivariate Analysis&lt;/em&gt;. 1979. Probability and mathematical statistics. Academic Press Inc.¬†&lt;a href=&quot;#fnref:MA&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>How did I set up my blog using Jekyll, Hyde and GitHub</title>
   <link href="http://localhost:4000/learning/2019/09/25/set-up-blog.html"/>
   <updated>2019-09-25T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2019/09/25/set-up-blog</id>
   <content type="html">&lt;p&gt;What would be a better way to start this blog than writing a post about how I set it up? Because after three days of sifting through all the documents, blogs, StackOverflow answers and GitHub issues, I finally realized that the process is not as straightforward as I thought it would be. Anyway, I got it to work (for now).&lt;/p&gt;

&lt;p&gt;My aim is simple, to set up a blog for myself where I can post stuff about my life. The blog needs to be free, elegant, intuitive and supports math. My current set up, Jekyll + Hyde + Github + MathJax, matches with that. Since there are many resources online about setting up a blog using Jekyll and serve it with GitHub, I am going to skip all the standard procedures by referring to the official documents. Instead, this post specifically documents:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the sequence of setting up different parts,&lt;/li&gt;
  &lt;li&gt;adding support for Tags, Categories and their corresponding pages,&lt;/li&gt;
  &lt;li&gt;adding MathJax to support $\LaTeX$-like math.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am using a macbook, so the steps will be described assuming the system is macOS. When in doubt, just google the relevant steps for other OSs.&lt;/p&gt;

&lt;h2 id=&quot;1-set-up-jekyll&quot;&gt;1. Set up Jekyll&lt;/h2&gt;
&lt;p&gt;Jekyll is the package that is generating all your website pages. First thing you want to do is to make sure that &lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is installed and ready to run.&lt;/p&gt;

&lt;p&gt;Follow the official &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;instructions&lt;/a&gt;. If you successfully made a new site, good!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;But if you ran into a &lt;strong&gt;failed to build native extension error&lt;/strong&gt;, install macOS SDK headers with the following line.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;If you ran into a &lt;strong&gt;file permission error&lt;/strong&gt;, run the following lines to set GEM_HOME to your user directory.
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'# Install Ruby Gems to ~/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export GEM_HOME=$HOME/gems'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'export PATH=$HOME/gems/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ~/.bashrc
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now you can proceed with the following line and the rest of the steps.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gem &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;bundler jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is caused by the macOS Mojave update. The above solution is provided by &lt;a href=&quot;https://talk.jekyllrb.com/t/issues-installing-jekyll-on-macos-mojave/2400/3&quot;&gt;desiredpersona and Frank&lt;/a&gt;. Make sure Jekyll can run normally before proceeding to step 2.&lt;/p&gt;

&lt;h2 id=&quot;2-set-up-github-repo&quot;&gt;2. Set up GitHub repo&lt;/h2&gt;
&lt;p&gt;Jekyll generates web pages locally; we need a GitHub repository to host our pages so that they can be accessed on the internet. For this part, setup can be done by following GitHub‚Äôs official &lt;a href=&quot;https://pages.github.com&quot;&gt;instructions&lt;/a&gt;. In the end, you should have a repo on GitHub called &lt;em&gt;username&lt;/em&gt;.github.io, and the corresponding local folder on your computer. In my case, the name of my repo is yangxiaozhou.github.io.&lt;/p&gt;

&lt;p&gt;By the end of Step 1 and 2, we have set up the local engine for generating web pages and the GitHub repo for hosting and publishing your pages. Now we proceed to the actual website construction.&lt;/p&gt;

&lt;h2 id=&quot;3-use-a-website-template&quot;&gt;3. Use a website template&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://hyde.getpoole.com&quot;&gt;Hyde&lt;/a&gt; is a Jekyll website theme built on &lt;a href=&quot;https://github.com/poole/poole&quot;&gt;Poole&lt;/a&gt;. They provide the template and the theme for the website. There are many themes for Jekyll, but I decided to use Hyde because I like the elegant design and it‚Äôs easy to customize.&lt;/p&gt;

&lt;p&gt;To get Hyde, just download &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;the repo&lt;/a&gt; and move all the files into the local folder that you have just created in Step 2. Remember to clear any existing file in that folder before moving in Hyde files. From here, you just have to edit parts of those files to make the website yours (or use it as it is). I changed the following two lines in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; since redcarpet and pygments are not supported anymore. Other variables can also be changed such as name, GitHub account, etc.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;markdown&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kramdown&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;highlighter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rouge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At this point, it would be a good idea to learn some basics of &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;Jekyll&lt;/a&gt;, e.g. what is a front matter, what is a page, how to create a layout, etc. After learning these, you can go ahead and customize the website as you‚Äôd like.&lt;/p&gt;

&lt;p&gt;One problem that I ran into is that pages look fine in local serve, but when I publish them to the web, all pages other than the home page have suddenly lost all their style elements. After searching through the internet, I realize that this has to do with the &lt;code class=&quot;highlighter-rouge&quot;&gt;url&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;baseurl&lt;/code&gt; usage. If you also have this problem, consider doing the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;change all the &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ site.baseurl }}&lt;/code&gt;
instances in &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sidebar.html&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;{{ '/' | relative_url }}&lt;/code&gt; so that the correct files can be located.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-add-tags--categories&quot;&gt;4. Add tags &amp;amp; categories&lt;/h2&gt;
&lt;p&gt;I want to add tags and categories to my posts and create a dedicated page where posts can be arranged according to &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;tags&lt;/a&gt;/&lt;a href=&quot;https://yangxiaozhou.github.io/categories/&quot;&gt;categories&lt;/a&gt;. This should be easy since tags and categories are default front matter variables that you can define in Jekyll. For example, tags and categories of my LDA post are defined like this:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;post&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;discriminant&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;analysis,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;explained&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;s&quot;&gt;2019-10-2 08:00:00 +0800&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DATA&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LDA supervised-learning classification&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;categories&lt;/strong&gt;, I created one page where posts of different categories are collected and the page is accessible through the sidebar link. To do this, just create a &lt;code class=&quot;highlighter-rouge&quot;&gt;category.html&lt;/code&gt; in the root folder:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: page
permalink: /categories/
title: Categories
---

&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archives&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
{% for category in site.categories %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-group&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% capture category_name %}{{ category | first }}{% endcapture %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&amp;lt;/p&amp;gt;&lt;/span&gt;

    &lt;span class=&quot;nt&quot;&gt;&amp;lt;h3&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;category-head&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ category_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h3&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ category_name | slugize }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    {% for post in site.categories[category_name] %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;article&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;archive-item&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;h4&amp;gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.baseurl }}{{ post.url }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{post.title}}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/article&amp;gt;&lt;/span&gt;
    {% endfor %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
{% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For &lt;strong&gt;tags&lt;/strong&gt;, I did two things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Show the tags of a post at the end of the content.&lt;/li&gt;
  &lt;li&gt;For every tag, create a page where posts are collected, i.e. &lt;a href=&quot;https://yangxiaozhou.github.io/tag/classification&quot;&gt;classification&lt;/a&gt;, &lt;a href=&quot;https://yangxiaozhou.github.io/tag/supervised-learning&quot;&gt;supervised-learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To do 1, include the following lines after the &lt;code class=&quot;highlighter-rouge&quot;&gt;content&lt;/code&gt; section in your &lt;code class=&quot;highlighter-rouge&quot;&gt;post.html&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-tags&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% for tag in page.tags %}
      {% capture tag_name %}{{ tag }}{% endcapture %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;no-underline&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tag/{{ tag_name }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;code&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;highligher-rouge&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;nobr&amp;gt;&lt;/span&gt;{{ tag_name }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/nobr&amp;gt;&amp;lt;/code&amp;gt;&lt;/span&gt;&lt;span class=&quot;ni&quot;&gt;&amp;amp;nbsp;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;    
    {% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To do 2, Long Qian has written a very clear &lt;a href=&quot;https://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;post&lt;/a&gt; about it.&lt;/p&gt;

&lt;h2 id=&quot;5-add-mathjax&quot;&gt;5. Add MathJax&lt;/h2&gt;
&lt;p&gt;The last piece to my website is to add the support of $\LaTeX$-like math. This is done through MathJax. There are two steps to achieve it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a &lt;code class=&quot;highlighter-rouge&quot;&gt;mathjax.html&lt;/code&gt; file and put it in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_includes&lt;/code&gt; folder. Download the file &lt;a href=&quot;https://github.com/YangXiaozhou/yangxiaozhou.github.io/blob/master/_includes/mathjax.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Put the following line before &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/head&amp;gt;&lt;/code&gt; in your &lt;code class=&quot;highlighter-rouge&quot;&gt;head.html&lt;/code&gt;:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; {% include mathjax.html %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;to enbale MathJax on the page.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tips&quot;&gt;Tips&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;To use the normal dollar sign instead of the MathJax command (escape), put &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;span class=&quot;tex2jax_ignore&quot;&amp;gt;...&amp;lt;/span&amp;gt;&lt;/code&gt; around the text you don‚Äôt want MathJax to process.&lt;/li&gt;
  &lt;li&gt;Check out currently supported Tex/LaTeX commands by MathJax &lt;a href=&quot;https://docs.mathjax.org/en/latest/input/tex/macros/index.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;math-rendering-showcase&quot;&gt;Math Rendering Showcase&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Inline math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$...\$&lt;/code&gt;: $\mathbf{x}+\mathbf{y}$.&lt;/li&gt;
  &lt;li&gt;Displayed math using &lt;code class=&quot;highlighter-rouge&quot;&gt;\$\$...\$\$&lt;/code&gt; on a new paragraph:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic numbering and referencing using &lt;span class=&quot;tex2jax_ignore&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\ref{label}&lt;/code&gt;&lt;/span&gt;:
In (\ref{eq:sample}), we find the value of an interesting integral:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
  \int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15} \, .
  \label{eq:sample}
\end{align}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Multiline equations using &lt;code class=&quot;highlighter-rouge&quot;&gt;\begin{align*}&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \nabla \times \vec{\mathbf{B}} -\, \frac1c\, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \,,\newline
  \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \,.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That‚Äôs it for now. Happy blogging.&lt;/p&gt;

&lt;p&gt;Additional resources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set up &lt;a href=&quot;https://blog.webjeda.com/jekyll-categories/&quot;&gt;categories &amp;amp; tags&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Set up &lt;a href=&quot;http://joshualande.com/jekyll-github-pages-poole&quot;&gt;Disqus comments &amp;amp; Google Analytics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Add in social media &lt;a href=&quot;https://jreel.github.io/social-media-icons-on-jekyll/&quot;&gt;icons&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;kramdown &lt;a href=&quot;https://kramdown.gettalong.org/quickref.html&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MathJax &lt;a href=&quot;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&quot;&gt;basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>ËèúË∞±</title>
   <link href="http://localhost:4000/learning/2019/01/01/recipe.html"/>
   <updated>2019-01-01T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2019/01/01/recipe</id>
   <content type="html">&lt;p&gt;ÂÅöËøáÁöÑÂ•ΩÂêÉÁöÑ‰∏úË•øÔºå‰∏∫‰∫Ü‰∏çÂøòËÆ∞ÊÄé‰πàÂÅöÔºåÊääËèúË∞±ÈÉΩÊî∂ÈõÜËµ∑Êù•ÊîæÂú®ËøôÂÑø„ÄÇÊúâÂæàÂ§öÊòØË∑üËÄÅÈ•≠È™®‰ª¨Â≠¶ÁöÑÔºåÊúâ‰∫õÊòØË∑üÊàëËÄÅÂ¶àÂ≠¶ÁöÑÔºåÊúâËßÜÈ¢ëÊïôÁ®ãÁöÑÔºåÊàëÈÉΩÊî∂ÂΩïÂú®&lt;a href=&quot;https://www.youtube.com/playlist?list=PL9_EOuhN2bBxtcNacsP8Qtw_zv0oN2lJu&quot;&gt;‰∏ãÂé®Êó∂Èó¥&lt;/a&gt;Êí≠ÊîæÂàóË°®Èáå„ÄÇÂè™Ë¶ÅÊ≤°ÊúâÂçïÁã¨ËØ¥ÁöÑÔºå‰∏ÄÂæãÊåâÈÄÇÈáèÂéüÂàô„ÄÇ&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#ÂÆ´‰øùÈ∏°‰∏Å&quot;&gt;ÂÆ´‰øùÈ∏°‰∏Å&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Ê≤πÊ≥ºÁå™Êâã&quot;&gt;Ê≤πÊ≥ºÁå™Êâã&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ËÖäËÇâÁ≥ØÁ±≥ÂúÜÂ≠ê&quot;&gt;ËÖäËÇâÁ≥ØÁ±≥ÂúÜÂ≠ê&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ÂÆ´‰øùÈ∏°‰∏Å&quot;&gt;ÂÆ´‰øùÈ∏°‰∏Å&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/kong_pao_chicken.jpg&quot; alt=&quot;ÂÆ´‰øùÈ∏°‰∏Å&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ÂáÜÂ§áÊùêÊñôÔºö&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ËÇâÔºöÈ∏°ËÖøËÇâÂéªÈ™®ÔºåÂàá‰∏Å„ÄÇ&lt;/li&gt;
  &lt;li&gt;‰∏äÊµÜÔºö‰∏ä‰∏ÄÊ¨°Ëë±ÂßúÊ∞¥ÔºåÊäìÂåÄÔºå‰∏äËõãÊ∏ÖÔºåÊäìÂåÄÔºåÂÜç‰∏ä‰∏ÄÊ¨°ÂßúËë±Ê∞¥ÔºåÊäìÂåÄ„ÄÇÊîæÂÖ•ÁîüÊäΩ„ÄÅËÄÅÊäΩ„ÄÅËä±ÈõïÈÖíÔºåÊäìÂåÄÔºåÂÜçÊîæÂÖ•ËÉ°Ê§íÁ≤â„ÄÅÁõê„ÄÅÊ∑ÄÁ≤âÔºåÊäìÂåÄ„ÄÇÊúÄÂêéÁî®&lt;strong&gt;Â∞ëÈáèÊ≤πÂ∞Å‰Ωè&lt;/strong&gt;„ÄÇ&lt;/li&gt;
  &lt;li&gt;Â∞èÊñôÔºöÂ§ßËë±Ëä±ÔºåËíúÁâáÔºåÂßúÁâá„ÄÇ&lt;/li&gt;
  &lt;li&gt;ÊñôÊ±ÅÂÑøÔºöÂ∞ëÈáèÁõêÔºåÂ§öÁÇπÁ≥ñÔºåËÉ°Ê§íÁ≤âÔºåÊñôÈÖíÔºåÈÖ±Ê≤πÔºåËÄÅÊäΩÔºåÈÄÇÈáèÊ∞¥Ê∑ÄÁ≤âÔºåÊúÄÂêéÂä†‰∏äÁÇπ&lt;strong&gt;Ëë±ÂßúËíúÁâáÂ∞ùÂë≥ÈÅì&lt;/strong&gt;„ÄÇ&lt;/li&gt;
  &lt;li&gt;Ëä±ÁîüÔºöÁÉ≠Ê∞¥Ê≥°‰∫ÜÂéªÁöÆ„ÄÇ&lt;/li&gt;
  &lt;li&gt;Âπ≤Ëæ£Ê§íÔºöÂàáÊÆµ„ÄÇ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ÂºÄÂßãÂÅöÔºö&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Â∞èÁÅ´ÁÇ∏Ëä±ÁîüÔºåÊÖ¢ÊÖ¢ÁÇ∏Âà∞È¶ôËÑÜ„ÄÇ&lt;/li&gt;
  &lt;li&gt;Âè¶Ëµ∑ÈîÖÔºåÁÉ≠ÈîÖÂÜ∑Ê≤πÔºåÂ∞èÁÅ´ÊªëÈ∏°‰∏ÅÔºåÊªëÂ•ΩÂÄíÂá∫„ÄÇ&lt;/li&gt;
  &lt;li&gt;Â∞èÁÅ´ÁÖ∏ÁÇíËä±Ê§íÔºåÊçûÂá∫Ëä±Ê§íÔºåÁÖ∏Âπ≤Ëæ£Ê§í„ÄÇ&lt;/li&gt;
  &lt;li&gt;ÁÖ∏Â•ΩÁöÑËæ£Ê§íËä±Ê§íÊ≤π‰∏≠Âä†ÂÖ•È∏°‰∏ÅÔºåÂºÄÂ§ßÁÅ´ÔºåÁøªÁÇíÂá†‰∏ã‰∏ãËë±ÂßúËíúÔºåÁÇíÂá∫È¶ôÂë≥Ôºå‰∏ãÊñôÊ±ÅÂÑøÔºåÁúãÈ¢úËâ≤ÈÄÇÈáèÊîæËÄÅÊäΩ‰∏äËâ≤„ÄÇ&lt;/li&gt;
  &lt;li&gt;Âá∫ÈîÖÂâçÂÄíÂÖ•Â∞ëÈáèËä±Ê§íÊ≤πÂíåËä±Áîü„ÄÇ&lt;/li&gt;
  &lt;li&gt;‰∏äËèúÔºÅ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ê≥®ÊÑèÔºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‰∏äÊµÜ‰∏ÄÂÆöË¶ÅËÄêÂøÉÔºåÊØèÊîæ‰∏ÄÊ¨°Ë∞ÉÊñôÂ∞±Ë¶ÅÊäìÂåÄ„ÄÇ&lt;/li&gt;
  &lt;li&gt;Ë∞ÉÂ•ΩÁöÑÊñôÊ±ÅÂÑøÈúÄË¶ÅÂ∞ù‰∏ÄÂ∞ùÔºåÊ†πÊçÆÂë≥ÈÅìÂÜçÂÅöË∞ÉÊï¥ÔºåÁº∫Âï•Ë°•Âï•„ÄÇ&lt;/li&gt;
  &lt;li&gt;Ëä±ÁîüÈúÄË¶ÅÂ∞èÁÅ´ÊÖ¢ÁÇ∏ÔºåÁÇ∏Âà∞È¶ôËÑÜÔºåÂæÆÈªÑ„ÄÇ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;Ê≤πÊ≥ºÁå™Êâã&quot;&gt;Ê≤πÊ≥ºÁå™Êâã&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/splash_oil_pig_trotters.jpg&quot; alt=&quot;Ê≤πÊ≥ºÁå™Êâã&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ÂáÜÂ§áÊùêÊñôÔºö&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Ê¥óÂáÄÂ∞èÁå™Êâã„ÄÇ&lt;/li&gt;
  &lt;li&gt;Â∞èËë±ÂàáÊÆµÔºåÂßúÂàáÁâáÔºåËíúÂàáÁâá„ÄÇ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ÂºÄÂßãÂÅöÔºö&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ÁÑØÊ∞¥ÔºöÂ∞èÁå™ÊâãÂÜ∑Ê∞¥‰∏ãÈîÖÁÑØÊ∞¥Ôºå&lt;strong&gt;Ê∞¥‰∏≠ÊîæÈÜã&lt;/strong&gt;„ÄÇ&lt;/li&gt;
  &lt;li&gt;Â§áÊ±§ÔºöÈ´òÂéãÈîÖÂÜÖÊîæÊ∞¥ÔºåÁõêÔºåËä±ÈõïÈÖíÔºåËë±Âè∂ÔºåÂßúÁâá„ÄÇ&lt;/li&gt;
  &lt;li&gt;ÁÇñËÇâÔºöÁÑØÂ•ΩÊ∞¥ÁöÑÁå™ËÑöÊãøÂá∫Áî®ÁÉ≠Ê∞¥Ê∏ÖÊ¥óÂπ≤ÂáÄÔºåÊîæÂÖ•È´òÂéãÈîÖÔºåÁÇñÂõõÂçÅÂàÜÈíüÂ∑¶Âè≥„ÄÇ&lt;/li&gt;
  &lt;li&gt;Â∞èÁÅ´ÁÇ∏Ëä±ÁîüÔºåÊääËä±ÁîüÁÇ∏Ëá≥È¶ôËÑÜ„ÄÇ&lt;/li&gt;
  &lt;li&gt;Áå™ÊâãÁÇñÂ•ΩÊãøÂá∫Êù•Ê≥°Âú®ÂÜ∞Ê∞¥ÈáåÈôçÊ∏©ÔºåÁî®ÊâãÂ∞ΩÈáèÊé∞ÊàêÂ∞èÂùóÂÑøÁä∂ÔºåÊñπ‰æøÂÖ•Âë≥„ÄÇ&lt;/li&gt;
  &lt;li&gt;Ê≤•Âπ≤Ê∞¥ÂàÜÁöÑÁå™Êâã‰∏≠ÊîæÂÖ•Ëä±ÁîüÁ±≥ÔºåÂ∞èËë±ÔºåËíúÁâáÔºåÈ¶ôÊ≤πÔºåÁõêÔºåÈÜãÔºåÊêÖÊãåÂùáÂåÄ„ÄÇ&lt;/li&gt;
  &lt;li&gt;Âè¶Ëµ∑ÈîÖÔºåÊîæÂÖ•Ê≤πÂíåËä±Ê§íÔºå&lt;strong&gt;Â∞èÁÅ´ÁÖ∏Âá∫Ëä±Ê§íÊ≤π&lt;/strong&gt;„ÄÇ&lt;/li&gt;
  &lt;li&gt;ÊãåÂ•ΩÁöÑÁå™Êâã‰∏≠ÊîæÂÖ•Ëæ£Ê§íÈù¢ÔºàÈáè‰æùÁÖß‰∏™‰∫∫Âè£Âë≥ÔºâÔºåÊ≥º‰∏äÁÉ≠ÁöÑËä±Ê§íÊ≤πÔºàÊ≤πÊ≥ºÁå™ÊâãÔºâ„ÄÇ&lt;/li&gt;
  &lt;li&gt;‰∏äËèúÔºÅ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ê≥®ÊÑèÔºö&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;È´òÂéãÈîÖÁÇñ‰πÖ‰∏ÄÁÇπËÉΩ‰ΩøÁå™ÊâãÊõ¥ËΩØÁ≥ØÔºåÊ†πÊçÆÂñúÂ•ΩÁöÑÂè£ÊÑüÂèØ‰ª•Ëá™Â∑±ÈÄâÊã©Êó∂Èïø„ÄÇ&lt;/li&gt;
  &lt;li&gt;ÊèêÂâçÂ§áÂ•ΩË∂≥Â§üÁöÑÂÜ∞ÂùóÔºåÁå™ÊâãÂá∫ÈîÖÂêéËÆ©ÂÆÉÂ∞ΩÈáèÂÜ∑Âç¥Ôºå‰∏çÁÑ∂ÊâãÊé∞Ëµ∑Êù•ÈùûÂ∏∏ÁÉ´„ÄÇ&lt;/li&gt;
  &lt;li&gt;Ëæ£Ê§íÈù¢Ê†πÊçÆËá™Â∑±ÂñúÂ•ΩÊîæÂÖ•Ôºå‰∏çËÉΩÂêÉËæ£ÁöÑÊ≥®ÊÑèÂà´ÊîæÂ§™Â§ö„ÄÇ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ËÖäËÇâÁ≥ØÁ±≥ÂúÜÂ≠ê&quot;&gt;ËÖäËÇâÁ≥ØÁ±≥ÂúÜÂ≠ê&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/recipes/sticky_rice_ball.jpg&quot; alt=&quot;ËÖäËÇâÁ≥ØÁ±≥ÂúÜÂ≠ê&quot; height=&quot;500px&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Êó•Êú¨Ë™û„ÅÆÁ∑¥Áøí</title>
   <link href="http://localhost:4000/learning/2018/11/21/japanese-essay.html"/>
   <updated>2018-11-21T08:00:00+08:00</updated>
   <id>http://localhost:4000/learning/2018/11/21/japanese-essay</id>
   <content type="html">&lt;p&gt;Âú®Â≠¶‰π†Êó•ËØ≠ËøáÁ®ã‰∏≠Èõ∂Èõ∂Êï£Êï£ÂÜôËøá‰∏Ä‰∫õÂ∞èÁü≠ÊñáÔºåÊÉ≥ÁùÄ‰∏çÂ¶ÇÂÖ®ÈÉ®ÊîæÂà∞ËøôÂÑøÊù•ÔºåÂΩìÂÅöËá™Â∑±ÁöÑ‰∏Ä‰∏™ËÆ∞ÂΩïÔºå‰πüÂ∏åÊúõÊî∂Âà∞ÂêåÂú®Êó•ËØ≠Â≠¶‰π†Ë∑Ø‰∏äÁöÑÊúãÂèã‰ª¨ÁöÑÂèçÈ¶à„ÄÇ
A collection of Japanese practice writings. All comments and corrections are welcomed.&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥4Êúà5Êó•--ÁßÅ„ÅÆÂ§¢&quot;&gt;2019Âπ¥4Êúà5Êó• ÔΩú ÁßÅ„ÅÆÂ§¢&lt;/h3&gt;

&lt;p&gt;„ÄÄÁßÅ„ÅÆÂ§¢„ÅØÂ•Ω„Åç„Å™‰ªï‰∫ã„ÇíËá™Áî±„Å´„Åó„Å™„Åå„Çâ„ÄÅÁ§æ‰ºö„ÅÆ‰∫∫„Åü„Å°„Å´‰Ωï„ÅãÂΩπ„Å´Á´ã„Å§„Åì„Å®„Åå„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çã„Åì„Å®„Åß„Åô„ÄÇËá™Áî±„ÅØ‰∏ÄÁï™Â§ßÂàá„Å™„Åì„Å®„Å†„Å®Ëã•„ÅÑ„Åã„ÇâÊ±∫„ÇÅ„Åæ„Åó„Åü„ÄÇ„ÇÑ„Å£„Å¶„ÅÑ„Çã‰ªï‰∫ã„ÅØËá™ÂàÜ„ÅåÂ•Ω„Åç„Å™„ÅÆ„Å™„Çâ„ÄÅ„ÅÑ„Åè„ÇâÈõ£„Åó„Åë„Çå„Å∞„ÄÅÁ∂ö„Åë„Çâ„Çå„Åæ„Åô„Åã„Çâ„ÄÇ„Åù„Çå„Å´„ÄÅÂçíÊ•≠„ÅÆÊôÇ„ÄÅÁà∂„Å´„ÄåÂ•Ω„Åç„Å™„Åì„Å®„Çí„ÇÑ„Çç„ÅÜ„Äç„Å®Ë®Ä„Çè„Çå„Å¶„ÄÅÂÆâÂøÉ„Åó„Åæ„Åó„Åü„ÄÇ„Åæ„Åü„ÄÅ‰∫∫„Åü„Å°„ÅÆÁîüÊ¥ª„ÅåËâØ„Åè„Å™„Çã„Åü„ÇÅ„Å´„ÄÅÂ§ßÂ≠¶„ÅßÂãâÂº∑„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂ§ßÂ≠¶Èô¢„ÅÆÁîüÊ¥ª„ÅØ„ÅÑ„Å§„ÇÇÂøô„Åó„ÅÑ„Åß„Åô„Åå„ÄÅÊó•Êú¨Ë™û„ÅåÁøí„Åà„Å¶„ÄÅÂ•Ω„Åç„Å™‰ªï‰∫ã„Åå„Åß„Åç„Å¶ÊØéÊó•„Å®„Å¶„ÇÇÂ¨â„Åó„ÅÑ„Åß„Åô„ÄÇÂ§¢„ÇíÊåÅ„Å£„Å¶„ÅÑ„Çã‰∫∫Áîü„ÅØÂπ∏„Åõ„Å†„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥4Êúà3Êó•&quot;&gt;2019Âπ¥4Êúà3Êó•&lt;/h3&gt;
&lt;p&gt;ÔΩú Á•ñÊØç„Å´„Å§„ÅÑ„Å¶&lt;/p&gt;

&lt;p&gt;„ÄÄËã•„ÅÑÊôÇ„Åã„Çâ„ÄÅÁ•ñÊØç„ÅØÂ®ò„ÅÆÊÅØÂ≠ê„ÅÆÁßÅ„Å´„ÅÑ„Å§„ÇÇË¶™Âàá„Å´„Åó„Å¶„Åè„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÆ∂„ÅßÂÆøÈ°å„Çí„Åô„ÇãÊôÇ„ÄÅ„Çà„Åè‰∏≠ÂõΩË™û„ÅÆÊñáÊ≥ï„ÇÑÊï∞„ÅàÊñπ„Å™„Å©„ÅÆÂïèÈ°å„Åå„ÅÇ„Å£„Åü„Çâ„ÄÅ‰∏ÄÁîüÊá∏ÂëΩ„Å´Êïô„Åà„Å¶„Åè„Çå„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ„ÄåËá™ÂàÜ„ÅßÈñìÈÅï„ÅÑ„ÇíÁõ¥„Åõ„Çã„Çà„ÅÑ„Å´„ÄÅÈ†ëÂºµ„Å£„Å¶„ÄÇ„Äç„Å®Ë®Ä„Çè„Çå„Åü„ÄÅ‰ªä„Åß„ÇÇ„ÄÅË¶ö„Åà„Å¶‰ªï‰∫ã„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÊóÖË°å„Åô„ÇãÊôÇ„ÄÅËâ≤„ÄÖ„Å™ÊâÄ„ÅÆ„ÅäÂúüÁî£„ÇíÊé¢„Åó„Å¶ÊåÅ„Å£„Å¶ÂÆ∂„Å´Â∏∞„Å£„Å¶„ÄÅ„ÅäÁ•ù„ÅÑ„ÇíÁ•ñÊØç„Å´„ÅÇ„Åí„Çå„Å∞„ÄÅÁ•ñÊØç„ÅØÂ¨â„Åó„Åè„Å™„Çã„ÅÆ„Åß„ÄÅÁßÅ„ÇÇÂ¨â„Åó„ÅÑ„Åß„Åô„ÄÇ&lt;/p&gt;

&lt;p&gt;ÔΩú Êñ∞ËÅû„ÅÆË™≠„ÅøÊñπ„Å´„Å§„ÅÑ„Å¶&lt;/p&gt;

&lt;p&gt;„ÄÄÂÖà„Åö„ÄÅË¶ãÂá∫„Åó„ÅØ‰Ωï„ÅãË™≠„Çì„Åß„Åø„Åæ„Åô„ÄÇ„Åì„Çå„ÅØË°®„Åß„ÅÇ„Çã„Éö„Éº„Ç∏„Åß„Åô„ÄÇ„Åì„ÅÆ„Éö„Éº„Ç∏„ÇíË™≠„ÇÅ„Å∞„ÄÅ‰∏ñÁïå‰∏≠„Åß‰∏ÄÁï™Â§ß„Åç„ÅÑ„Éã„É•„Éº„Çπ„Åå„Çè„Åã„Çä„Åæ„Åô„ÄÇ„Çà„ÅèÊîøÊ≤ª„ÇÑ‰∫ãÊïÖ„Å™„Éã„É•„Éº„Çπ„Åå„Åì„ÅÆ„Éö„Éº„Ç∏„Åß„ÅÇ„Çä„Åæ„Åô„Åå„ÄÅÊôÇ„ÄÖÁ§æ‰ºö„Å®ÊñáÂåñ„ÅÆ„Éã„É•„Éº„Çπ„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇË¶ãÂá∫„Åó„ÅÆ„Éã„É•„Éº„Çπ„ÅØÁßÅ„ÅÆÁîüÊ¥ª„Åã„ÇâÈÅ†„ÅÑ„Åß„Åô„Åå„ÄÅÁü•„Çã„ÅÆ„ÅØÂøÖË¶Å„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÊ¨°„Å´„ÄÅ„Çπ„Éù„Éº„ÉÑ„ÅåÂ•Ω„Åç„Åß„ÄÅ„Çπ„Éù„Éº„ÉÑ„Éã„É•„Éº„Çπ„Å∏Ë°å„Åç„Åæ„Åô„ÄÇÂÖ®ÈÉ®Ë™≠„Åæ„Å™„Åè„Å¶„ÄÅ„Éê„Çπ„Ç±„ÉÉ„Éà„Éú„Éº„É´„ÅÆË©¶Âêà„ÅåÊ∞ó„Å´ÂÖ•„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰∏°Ë¶™„ÅØÁî∫„ÅÆÊñ∞ËÅû„Çí‰∏ÄÁï™„Å´Ë™≠„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥3Êúà28Êó•ÁâπÂà•„Å™ÊÄù„ÅÑÂá∫„Åå‰Ωú„Çã„ÅÆ„ÅØ„Å©„ÅÜ„Åô„Çå„Å∞„ÅÑ„ÅÑ„Åß„Åô„Åã&quot;&gt;2019Âπ¥3Êúà28Êó•„ÄÄÔΩú„ÄÄÁâπÂà•„Å™ÊÄù„ÅÑÂá∫„Åå‰Ωú„Çã„ÅÆ„ÅØ„ÄÅ„Å©„ÅÜ„Åô„Çå„Å∞„ÅÑ„ÅÑ„Åß„Åô„Åã„ÄÇ&lt;/h3&gt;

&lt;p&gt;A„Åï„Çì„Å®B„Åï„Çì„ÅØ„Éè„Ç∏„É¨„Éº„É≥„Å∏Ë°å„Å£„Åü„Åì„Å®„Åå„Å™„ÅÑ„Åã„Çâ„ÄÅÂàù„ÇÅ„Å¶„Éè„Ç∏„É¨„Éº„É≥„Çí„ÇÜ„Å£„Åè„Çä‰ΩìÈ®ì„Åß„Åç„Çã„Çà„ÅÜ„Å´„ÄÅ„ÉÑ„Ç¢„Éº„Çí‰Ωú„Çä„Åæ„Åó„Åü„ÄÇ„ÉÑ„Ç¢„Éº„ÅÆÊó•„ÅØÂ§©Ê∞ó„ÅåËâØ„Åè„Å™„Åè„Å¶„ÄÅ„Ç∑„É≥„Ç¨„Éù„Éº„É´„ÅÆÊôÆÈÄö„ÅÆËí∏„ÅóÊöë„ÅÑÂ§©Ê∞ó„Åß„Åó„Åü„ÄÇ„Åù„Çå„Å´„ÄÅÁßÅ„Åü„Å°„ÅÆÂõõ‰∫∫„Ç¨„Ç§„Éâ„ÅØÊôÇ„ÄÖË§áÈõë„Å™ËÄÉ„Åà„ÅåË©±„Åõ„Å™„Åã„Å£„Åü„ÅÆ„Åß„ÄÅËã±Ë™û„Å®Êó•Êú¨Ë™û„ÇÇ‰∏äÊâã„Å™B„Åï„Çì„Å´ÁøªË®≥„Åó„Å¶„ÇÇ„Çâ„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åß„ÇÇ„ÄÅ„ÉÑ„Ç¢„Éº„ÅÆÁµÇ„Çè„Çä„Å´A„Åï„Çì„Å®B„Åï„Çì„ÅØ„Äå‰ªäÊó•„ÅØ„ÄÅ„Éè„Ç∏„ÉÅ„É£„É¨„É≥„Ç∏„Å´ÂèÇÂä†„Åó„Å¶„Çà„Åã„Å£„Åü„Å≠„ÄÇ„Äç„Å®ÊÄù„Å£„Åü„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;p&gt;„ÅÑ„ÅÑÂÜôÁúü„ÅåÊíÆ„Çå„Çã„Å†„Åë„Åò„ÇÉ„Å™„Åè„Å¶„ÄÅ‰Ωï„ÅåÁâπÂà•„Å™ÊÄù„ÅÑÂá∫„Åå‰Ωú„Çå„Çã„Çà„ÅÜ„Å´„ÄÅ„ÉÑ„Ç¢„Éº„ÅÆÂΩ¢„ÇíËÄÉ„Åà„Åæ„Åó„Åü„ÄÇ„Éè„Ç∏„Éº„É¨„É≥„Å´„ÅØ„Åü„Åè„Åï„ÇìÁ∂∫È∫ó„Å™ËêΩÊõ∏„Åç„Åå„ÅÇ„Å£„Å¶„ÄÅ„ÅÇ„ÅÆËêΩÊõ∏„Åç„ÇíË©≥„Åó„ÅèË¶ã„Çå„Å∞„ÄÅÁî∫„ÅÆ„Åì„Å®„Åå„Çà„ÅèÂàÜ„Åã„Çã„Åó„ÄÅÁâπÂà•„Å™ÊÄù„ÅÑÂá∫„ÇÇ„Åß„Åç„Çã„Å®ÊÄù„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åù„Çå„Åß„ÄÅ„Éè„Ç∏„ÉÅ„É£„É¨„É≥„Ç∏„ÅåÁîü„Åæ„Çå„Åæ„Åó„Åü„ÄÇ„Åß„ÇÇ„ÄÅ„Åì„ÅÆ„ÉÑ„Ç¢„Éº„ÅåÊàêÂäü„Åó„Åü‰∏ÄÁï™Â§ßÂàá„Å™ÁêÜÁî±„ÅØ„Éã„Ç≥„É´„Åï„Çì„ÅåËá™ÂàÜ„Åß‰Ωú„Å£„Åü„Éë„É≥„Éï„É¨„ÉÉ„Éà„Åß„Åó„Åü„ÄÇA„Åï„Çì„Å®B„Åï„Çì„ÅØ„Åù„ÅÆÁæé„Åó„ÅÑ„Éë„É≥„Éï„É¨„ÉÉ„Éà„ÇíÊ∫ñÂÇô„Åó„Å¶„ÇÇ„Çâ„Å£„Å¶„ÄÅ„Å≥„Å£„Åè„Çä„Åó„Åæ„Åó„Åü„ÄÇ‰∫å‰∫∫ÈÅî„ÅØÊé¢„Åó„Å¶„ÅÑ„ÇãËêΩÊõ∏„Åç„ÅåË¶ã„Å§„Åã„Å£„ÅüÊôÇ„ÄÅ„Åç„Å£„Å®„Éë„É≥„Éï„É¨„ÉÉ„Éà„Å´„ÅØ„Çì„Åì„ÇíÊäº„Åï„Çå„Çã„ÅÆ„ÅØÂ¨â„Åó„Åã„Å£„Åü„Åß„Åó„Çá„ÅÜ„ÄÇÊâã„Åß‰Ωú„Å£„Åü„Éë„É≥„Éï„É¨„ÉÉ„Éà„Çí‰Ωø„ÅÜ„Åì„Å®„ÇÑ„ÄÅÁ¥ô„ÅßÊõ∏„ÅÑ„Åü„ÇÇ„ÅÆ„Å´„ÄÅ„ÅØ„Çì„Åì„ÇíÊäº„Åô„ÅÆ„ÅØ„ÄÅ‰Ωï„ÅåÁâπÂà•„Å™ÊÑü„Åò„Åå„Åó„Åæ„Åô„ÄÇ‰ªä„ÄÅÊú¨„ÇÑ„Éë„É≥„Éï„É¨„ÉÉ„Éà„Çà„Çä„ÇÇ„Å£„Å®ÊÉÖÂ†±„ÅåÂ§ö„Åè„Å¶„ÄÅÈÄü„Åè„Å¶„ÄÅ‰æøÂà©„Å™Êê∫Â∏ØÈõªË©±„ÅØÂ§ßÂ§â‰∫∫Ê∞ó„Åå„ÅÇ„Çä„Åæ„Åô„Åå„ÄÅÂú∞Âõ≥„ÇÑ„Éë„É≥„Éï„É¨„ÉÉ„Éà„Å™„Å©„Çí„Åæ„Å†‰Ωø„Å£„Å¶„ÅÑ„Çã‰∫∫„ÇÇ„ÅÑ„Åæ„Åô„ÄÇÊÉÖÂ†±„ÅØÂ§â„Çè„Çâ„Å™„ÅÑ„ÅÆ„Åß„ÄÅÁ¥ô„ÅÆ„Éë„É≥„Éï„É¨„ÉÉ„Éà„Çí‰Ωø„Åà„Å∞Âë®„Çä„ÅÆÁâ©„Å´„ÇÇ„Å£„Å®Ê∞ó„Åå„Å§„Åè„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ„Åù„Çå„Åß„ÄÅ„Åù„ÅÆÊôÇ„ÄÅ„Åù„Åì„Åß„ÄÅ‰∫∫„Å®Ëá™ÁÑ∂„ÅÆ„ÄÅ„Åæ„Åü„ÄÅ‰∫∫„Å®‰∫∫„ÅÆÊÑèÂë≥„Åå„ÅÇ„ÇãÈñ¢‰øÇ„Åå„Åß„Åç„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥3Êúà20Êó•&quot;&gt;2019Âπ¥3Êúà20Êó•&lt;/h3&gt;

&lt;p&gt;„ÄÄ„ÇØ„É¨„É°„É≥„ÉÜ„Ç£„ÅØÁßÅ„ÅÆÂ§ßÂ•Ω„Åç„Å™Áî∫„Åß„Åô„ÄÇ„Åì„Åì„ÅØ‰∫∫„ÅåÂ§ßÂã¢„ÅÑ„Å¶„ÄÅ„ÅÑ„Å§„ÇÇ„Å´„Åé„ÇÑ„Åã„Å™„Çì„Åß„Åô„Åå„ÄÅÂõ∞„Çä„Åæ„Åõ„Çì„ÄÇ„Åù„Çå„ÅØ„ÄÅËâ≤„ÄÖ„Å™Â∫ó„ÅåÂìÅÁâ©„ÇíÂ£≤„Å£„Å¶„ÄÅË≤∑„ÅÑÁâ©„ÅÆÈÄî‰∏≠„Åß„ÅäËÖπ„Åå„Å∏„Å£„Åü„Çâ„ÄÅÁã≠„ÅÑÈÅì„ÅßÂÄ§ÊÆµ„Åå„Åù„Çì„Å™„Å´È´ò„Åè„Å™„ÅÑÈ£ü„ÅπÁâ©„ÇÑÈ£≤„ÅøÁâ©„ÅåË≤∑„Åà„Åæ„Åô„ÄÇÊÅ•„Åö„Åã„Åó„Åè„Å™„Åè„Å¶„ÄÅÈ£≤„ÅøÁâ©„ÇíÈ£≤„Åø„Å™„Åå„Çâ„ÄÅË≤∑„ÅÑÁâ©„ÇíÈÅ∏„Å≥„Åæ„Åó„Çá„ÅÜ„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥3Êúà4Êó•&quot;&gt;2019Âπ¥3Êúà4Êó•&lt;/h3&gt;

&lt;p&gt;„ÄÄÊòî„Åã„Çâ„ÄÅ‰∏≠ÂõΩ„ÅØ‰∏ñÁïå‰∏≠„ÅÆËâ≤„ÄÖ„Å™ÂõΩ„ÅÆ‰∫∫„ÇíÊãõÂæÖ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰ªñ„ÅÆÂõΩ„Å®‰ª≤ËâØ„Åè„Åô„Çå„Å∞„ÄÅÂïÜÂìÅ„ÇíËº∏ÂÖ•„Åß„Åç„Åæ„Åô„ÄÇ„Åü„Å®„Çå„Å∞„ÄÅ„Çà„ÅèÁü≥Ê≤π„Å®‰ªñ„ÅÆÂéüÊñô„ÇíËº∏ÂÖ•„Åó„Å¶„ÄÅÁæé„Åó„ÅÑÊ±†„ÅÆÁµµ„Å®„Åã„ÄÅÁ±≥„Å®„Åã„ÄÅ„ÅäËå∂„ÇíËº∏Âá∫„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰ªä„ÇÇ„ÄÅ„Åü„Åè„Åï„Çì„ÅÆÂõΩ„Å´È†º„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;p&gt;„ÄÄÂéªÂπ¥11Êúà„Å´Ë°å„Å£„Åü„Éï„Ç£„É™„Éî„É≥„Å´„ÅÇ„Çã„Éû„É©„Éë„Çπ„ÇØ„Ç¢Â≥∂„ÅØ„Å®„Å¶„ÇÇÁ∂∫È∫ó„Å™Â≥∂„Åß„Åô„ÄÇÊØéÊó•Êµ∑Â≤∏„ÅßÊï£Ê≠©„Åô„ÇãÂâç„Å´ÂçµÂÖ•„Çå„Å¶Êúù„Åî„ÅØ„Çì„ÇíÈ£ü„Åπ„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆÂ≥∂„ÅØÂ∞è„Åï„ÅÑ„Åß„Åô„Åã„Çâ„ÄÅÂ∑•Â†¥„ÅØÂÖ®ÁÑ∂Âª∫„Å¶„Å¶„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÊùë„ÅÆ‰∏≠„Åß„ÄÅË±ö„Å®È≥•„Çí„Åü„Åè„Åï„ÇìËÇ≤„Å¶„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅÈ£≤„Åæ„Çå„ÇãÊ∞¥„ÅØ„ÅÇ„Åæ„ÇäË∂≥„Çâ„Å™„Åè„Å¶„ÄÅ‰∫∫„Åü„Å°„ÅÆÁîüÊ¥ª„ÅØÈõ£„Åó„ÅÑ„Åß„Åô„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥2Êúà18Êó•&quot;&gt;2019Âπ¥2Êúà18Êó•&lt;/h3&gt;

&lt;p&gt;„ÄÄ„Ç∑„É≥„Ç¨„Éù„Éº„É´Â≥∂„ÅØ„Éû„É¨„Éº„Ç∑„Ç¢„ÅÆÂçóÂêë„Åì„ÅÜ„ÅÆÂ≥∂„Åß„Åô„ÄÇ„Åì„ÅÆÂ≥∂„ÅØÂ∞è„Åï„ÅÑ„Åß„Åô„Åå„ÄÅÊú®„ÇÑËä±„ÅåÂ§ö„Åè„Å¶„ÄÅ„Åù„Åó„Å¶„ÄÅÊú®„ÅÆËëâ„ÇÇÈÅì„Åß„Çà„ÅèË¶ã„Åà„Åæ„Åô„ÄÇ„Éû„É¨„Éº„Ç∑„Ç¢„Åæ„Åß‰∫å„Å§Ê©ã„ÇíÂª∫„Å¶„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„ÅÆÂ≥∂„Åß„ÄÅ‰∫∫„Åü„Å°„ÅÆÁîüÊ¥ª„ÅØÊòî„Åã„ÇâÂ§ßÂ§â„Åò„ÇÉ„Å™„Åè„Å¶„ÄÅÊó©„ÅèÁµêÂ©ö„Åô„Çå„Å∞„ÄÅ‰Ωè„ÇÄÂÆ∂„ÅåË≤∑„Åà„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;p&gt;„ÄÄ‰ªäÊó•„ÅØÂúüÊõúÊó•„Åß„Åô„ÄÇÂàù„ÇÅ„Å¶Ëàπ„Åß‰πó„Å£„Å¶„ÄÅÊµ∑„ÇíÊ∏°„Å£„Å¶Ôºå„Ç¶„Éì„É≥Â≥∂„Å´Ë°å„Å£„Åü„ÄÇ‰∏ÄÊôÇÈñì„Åê„Çâ„ÅÑ„ÄÅÊµ∑„ÅßÊ≥≥„ÅÑ„Å†„ÄÇÂ≥∂„Åß„Åü„Åè„Åï„ÇìÈáéËèú„Å®È≠ö„ÇíÈ£ü„Åπ„Çâ„Çå„Åü„ÄÇÂë≥„ÅØ„Ç∑„É≥„Ç¨„Éù„Éº„É´„ÅÆ„Å®ÈÅï„Å£„Å†„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2019Âπ¥1Êúà28Êó•&quot;&gt;2019Âπ¥1Êúà28Êó•&lt;/h3&gt;

&lt;p&gt;„ÄÄÁßÅ„ÅÆÂÆ∂„ÅÆËøë„Åè„Å´Á©∫Ê∏Ø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„Åã„Çâ„ÄÅÈ£õË°åÊ©ü„ÇíË¶ã„Å¶„ÅÑ„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ„Åß„ÇÇ„ÄÅÂÆ∂„ÅÆËøë„Åè„Å´„Åü„Åè„Åï„ÇìÁ∂∫È∫ó„Å™Â±±„ÇÑÂ∑ù„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊò•„Å´„Å™„Çå„Å∞„ÄÅÂ±±„ÅØÁ∑ë„ÅÆË°£„Çí„Å§„Åë„Å¶„ÅÑ„Åæ„Åô„ÄÇÁßÅ„ÅÆÂ≠ê‰æõ„ÅÆÊôÇ„ÄÅ„Çà„ÅèÁà∂„ÅØÂ±±Áôª„Çä„Å´ÈÄ£„Çå„Å¶Ë°å„Å£„Å¶„Åè„Çå„Åæ„Åó„Åü„ÄÇÂÖ¨Âúí„Å∏Ë°å„Åè„ÅÆ„Çà„ÇäÈù¢ÁôΩ„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åù„Çå„Åß„ÄÅÂú∞Ë≥™Â≠¶ËÄÖ„Å´„Å™„Çç„ÅÜ„Å®Ê±∫„ÇÅ„Åæ„Åó„Åü„ÄÇ&lt;/p&gt;

&lt;p&gt;„ÄÄËá™ÂãïËªä„ÇíÈÅãËª¢„Åó„Å¶„ÅÑ„ÇãÊôÇ„ÄÅ„Å®„Å¶„ÇÇÊ∞ó„Çí‰ªò„Åë„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇÁâπ„Å´„ÄÅÊõ≤„Åå„ÇãÊôÇ„ÅØ„ÄÅ„Çà„Åè‰∫§ÈÄö‰∫ãÊïÖ„Åå„ÅÇ„Çã„Åã„Çâ„ÄÅÊú¨ÂΩì„Å´Âç±Èô∫„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÊõ≤„Åå„ÇãÂâç„Å´„ÅØ„ÄÅËªä„ÅÆÂ∏≠„Åã„ÇâÂ∑¶„Å®Âè≥„ÇíË¶ã„Åü„Çâ„ÅÑ„ÅÑ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;

&lt;h3 id=&quot;2018Âπ¥11Êúà21Êó•È†ÜÂ§©Â†ÇÂ§ßÂ≠¶„ÅÆ„Åø„Å™„Åï„Çì„Å®„ÅÆ‰∫§ÊµÅ„Åã„ÇâÂ≠¶„Çì„Å†„Åì„Å®&quot;&gt;2018Âπ¥11Êúà21Êó•„ÄÄÔΩú„ÄÄÈ†ÜÂ§©Â†ÇÂ§ßÂ≠¶„ÅÆ„Åø„Å™„Åï„Çì„Å®„ÅÆ‰∫§ÊµÅ„Åã„ÇâÂ≠¶„Çì„Å†„Åì„Å®&lt;/h3&gt;

&lt;p&gt;„ÄÄÂπ¥„ÇíÂèñ„Çã‰∫∫„ÅåÂ§ö„Åè„Å¶„ÄÅËã•„ÅÑ‰∫∫„Åå„ÅÇ„Åæ„ÇäÂ§ö„Åè„Å™„ÅÑ„ÅÆ„Åß„ÄÅ‰ªãË≠∑Èõ¢ËÅ∑„ÅØ„Åü„Åó„Åã„Å´Êó•Êú¨„Å´„Å®„Å£„Å¶Â§ß„Åç„ÅÑÁ§æ‰ºöÁöÑÂïèÈ°å„Åß„Åô„ÄÇ„Åù„Åó„Å¶„ÄÅ„Åì„ÅÆÂïèÈ°å„ÅÆÁ≠î„Åà„ÅØ„ÄÅÊó©„ÅèËÄÉ„Åà„Å™„Åë„Çå„Å∞„Å™„Çä„Åæ„Åõ„Çì„ÄÇÊó•Êú¨„Åß„ÅØ‰ªãË≠∑„Çµ„Éº„Éì„Çπ„Å´„Åã„Åã„ÇãË≤ªÁî®„Åå„Å®„Å¶„ÇÇÈ´ò„ÅÑ„ÅÆ„Åß„ÄÅÊôÆÈÄö„ÅÆÂÆ∂Êóè„ÅØ„ÄÅ„ÅÇ„Åæ„ÇäÊâï„ÅÜ‰∫ã„Åå„Åß„Åç„Å™„ÅÑ„Åß„Åô„Åã„Çâ„ÄÅ‰∏ª„Å´ÂÆ∂Êóè„Åå‰ªãË≠∑„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åï„Çâ„Å´„ÄÅ‰ªãË≠∑ËÄÖ„ÅÆ‰∫∫Êï∞„ÅØ‰ªä„Çà„Çä„ÇÇ„Å£„Å®„Åü„Åè„Åï„ÇìÂøÖË¶Å„Å´„Å™„Çä„Åæ„Åô„ÄÇ„ÇÇ„Åó„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞„ÄÅ„Ç≥„Çπ„Éà„ÅØÁµ∂ÂØæ„Çà„ÇäÈ´ò„Åè„Å™„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÈ†ÜÂ§©Â†ÇÂ§ßÂ≠¶„ÅÆÂ≠¶Áîü„ÅåÁô∫Ë°®„Åó„Åü‰∫ã„Å´„Å§„ÅÑ„Å¶„ÄÅÁ¨¨‰∏Ä„Å®Á¨¨‰∫å„Å®„ÅÇ„Çì„ÅÆ‰ºöÁ§æÂì°„Åå‰ªãË≠∑‰ºëÊöá„ÇÑ‰ªãË≠∑‰ºëÊ•≠„ÇíÂèñ„Å£„Åü„Çä„ÄÅ„Éï„É¨„ÉÉ„ÇØ„Çπ„Çø„Ç§„É†Âà∂„ÅåÊúâ„Å£„Åü„Çä„Åô„Çã‰ºöÁ§æ„ÅØÂ§ßÂàá„Å†„Å®Ë®Ä„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åù„Çå„ÅØÁßÅ„ÇÇ„Åù„ÅÜÊÄù„ÅÑ„Åæ„Åô„Åå„ÄÅÊó•Êú¨„ÅÆ‰ºöÁ§æ„ÅØÊÆãÊ•≠„ÅåÂ§ö„Åè„Å¶„ÄÅÁ´∂‰∫â„Åå„Å®„Å¶„ÇÇÂé≥„Åó„ÅÑ„Åß„Åô„Åã„Çâ„ÄÅÈõ£„Åó„ÅÑ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ‰∏â„Å§ÁõÆ„ÅÆ„ÅÇ„Çì„ÅØ„ÄÅ„ÅÑ„ÅÑ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÂ§ñÂõΩ‰∫∫„ÅÆÂä¥ÂÉçËÄÖ„ÅØ„ÄÅ„ÇÇ„ÅÜ„Ç∑„É≥„Ç¨„Éù„Éº„É´„ÇÑÈ¶ôÊ∏Ø„Åß„ÅØÈï∑„ÅÑÈñì„ÅÑ„Åæ„Åô„ÄÇÁ¢∫„Åã„Å´„ÄÅ„Åä„Åò„ÅÑ„Åï„Çì„ÇÑ„Åä„Å∞„ÅÇ„Åï„Çì„ÅØ„ÄÅÂ§ñÂõΩ‰∫∫„Å®‰∏ÄÁ∑í„Å´ÊöÆ„Çâ„Åó„Å¶„ÄÅ‰∏ñË©±„Çí„Åó„Å¶„ÇÇ„Çâ„ÅÜ‰∫ã„ÅåÂ´å„ÅÑ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ„Åß„ÇÇ„ÄÅÁßÅ„ÅØÂçäÂπ¥„Åê„Çâ„ÅÑ‰∫¨ÈÉΩ„Åß‰Ωè„Çì„Åß„ÅÑ„Åæ„Åó„Åü„Åã„Çâ„ÄÅÂ§ñÂõΩ‰∫∫„ÅØÊó•Êú¨‰∫∫„Å®‰∏ÄÁ∑í„Å´ÊöÆ„Çâ„Åô„Åì„Å®„Åå„Åß„Åç„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÂ§ñÂõΩ‰∫∫„ÅÆ„Éò„É´„Éë„Éº„Åï„Çì„ÅØÊó•Êú¨Ë™û„ÇÑÊó•Êú¨„ÅÆÊñáÂåñ„ÇíÂãâÂº∑„Åó„Å¶„Åã„Çâ„ÄÅÂÉç„Åè„Åì„Å®„ÅåËß£Ê±∫Á≠ñ„ÅÆ‰∏Ä„Å§„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt;
</content>
 </entry>
 

</feed>
