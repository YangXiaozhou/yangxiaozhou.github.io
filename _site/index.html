<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Xiaozhou's Notes &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">




  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <!-- <a href="/"> -->
          Xiaozhou's Notes
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">Publication</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/yangxiaozhou/yangxiaozhou.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:xiaozhou.yang@u.nus.edu" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/yangxiaozhou" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/YangXiaozhou" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/a315345751" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h2 class="post-title">
      <a href="/journal/2020/05/16/journal-2020-05.html">
        2020年5月：
      </a>
    </h2>

    <span class="post-date">16 May 2020</span>

    <p>2020.02.06 - 安排自己的PhD训练
让自己的PhD training变得structured, how?
Schedule around deep work:</p>
<ol>
  <li>Research work</li>
  <li>Academic training</li>
  <li>Professional development</li>
</ol>

    <!-- <p>2020.02.06 - 安排自己的PhD训练
让自己的PhD training变得structured, how?
Schedule around deep work:</p>
<ol>
  <li>Research work</li>
  <li>Academic training</li>
  <li>Professional development</li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/05/16/francis-galton.html">
        Francis Galton
      </a>
    </h2>

    <span class="post-date">16 May 2020</span>

    <p>周末读Aeon的一篇文章：<a href="https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;utm_medium=email&amp;utm_term=0_411a82e59d-f7c118f081-69607277">Algorithms associating appearance and criminality have a dark past</a>，讲现在有研究人员用机器学习算法通过人脸来判断某人犯罪的几率。文中讲到这种从人外表提取预见性特征的尝试，在犯罪学历史上并不新奇，19世纪的意大利犯罪学家Cesare Lombroso认为罪犯的脸部有独特的样貌：突出的前额、鹰型鼻梁；而18世纪的Francis Galton则尝试回答一个更广泛的问题：人的外表跟他或她的健康状况、犯罪倾向、智力等等有关系吗？或者说，人的基因是否决定了健康、行为、智力和竞争力？</p>

    <!-- <p>周末读Aeon的一篇文章：<a href="https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;utm_medium=email&amp;utm_term=0_411a82e59d-f7c118f081-69607277">Algorithms associating appearance and criminality have a dark past</a>，讲现在有研究人员用机器学习算法通过人脸来判断某人犯罪的几率。文中讲到这种从人外表提取预见性特征的尝试，在犯罪学历史上并不新奇，19世纪的意大利犯罪学家Cesare Lombroso认为罪犯的脸部有独特的样貌：突出的前额、鹰型鼻梁；而18世纪的Francis Galton则尝试回答一个更广泛的问题：人的外表跟他或她的健康状况、犯罪倾向、智力等等有关系吗？或者说，人的基因是否决定了健康、行为、智力和竞争力？</p>

<p>Francis Galton？
这名字看起来很眼熟，我隐约记得在老板的一门Forecasting统计课上听到过。仔细一想，对，在线性回归的部分时，老板上课专门介绍了他。Sir Francis Galton，姓Galton，名Francis，但当提到他时，出于礼仪，你得加个Sir，因为他在1909年被英国女王授予了骑士爵位。为什么在讲线性回归的时候要介绍他呢？因为他作为第一个人，观察并记录了这样一种现象：平均身高很高的父母，往往会有身高更接近普通的孩子；而平均身高偏低的父母的孩子，成年后通常有着更接近普通人的身高。
下图是<a href="https://www.ams.org/journals/bull/2013-50-01/S0273-0979-2012-01374-5/S0273-0979-2012-01374-5.pdf">Bradley Efron</a>根据Galton当时收集到的父母和孩子的身高数据重新制的图，完美地展现了我们现在所知道的Bivariate normal distribution。
<img src="/assets/francis-galton/regression_to_mean.png" alt="regression_to_mean" />
他把这种现象称为regression towards mediocrity[reference]，现在通常叫做regression toward the mean，中文貌似叫“向均数回归”。同样的现象，我们在生活中很多地方都很观察到：因为运气而押中题目的学生考出了高分，下一次考试的成绩却没那么突出；连续投中三个三分球的朋友，下个球往往“容易”失手；我上周做<a href="https://yangxiaozhou.github.io/learning/2019/01/01/recipe.html#%E6%B2%B9%E6%B3%BC%E7%8C%AA%E6%89%8B">油泼猪手</a>时各种调料拿捏得很好，味道超棒，这周再做一次，大概率味道会比较普通🤷‍♂️。</p>

<p>符合这原则的现象，他们有一个共通点：他们的结果往往完全或部分由随机因素决定，而随机因素的影响往往符合以0为中心的正态分布（时好时坏）。比如说，三分球进或不进，有投手能力的影响但也有运气的成分；我做的某道菜的味道，取决于下厨能力，但我的专心程度、手抖程度以及心情等几乎随机的因素也会有所影响。也就是说，假设某一天我超级走运，做出了迄今为止最好吃的一道菜，这种事件发生的概率是很小的（得到正态分布上的极大值（或极小值）的概率）。下一次做，大概率我会正常发挥，菜的味道也没上次好（取到了正态分布上0周围的某个值）。</p>

<p>想象这样一种情况：朋友在我搬新家的时候来家里吃饭，刚好碰到我前面说的超常发挥，都说做的猪手好吃！过了几个月，家里聚会，应朋友强烈要求，再次做出一盘猪手，不过这次是正常发挥。朋友吃后回忆起之前，评论到：“水平下降了呀！” 我冤不冤？ 这样的冤枉我们生活中还真不少，以至于它有个专门的称呼：Regression fallacy，中文叫“回归谬误”。Daniel Kahneman讲过亲身经历的这样<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292229/">一个例子</a>：他有一次给飞行员学校做培训，提到了表扬能使学员变得更优秀。下面的一个教官不同意了，说他每次一夸完降落做得简直完美的学员，下一次一定做得没那么好，而刚被他骂过的学员，马上就能看到提升。听了教官的抗议，Kahneman当下有了一个eureka moment，他说道：</p>
<blockquote>
  <p>because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.</p>
</blockquote>

<p>让我们回到观察小天才Galton。从年度公牛体重竞猜的数据中看到了以平均值与中位数对真实值进行估计的准确性，也看到了这样的估计与真实值之间的偏差呈正态分布。</p>

<p>为了展现这种差值的正态分布以及前面提到的回归谬误，Galton设计了一个令人拍案叫绝的装置：Galton Board。这个board可以展示central limit theorem，对，就是那个可以解释为什么正态分布在现实生活中如此普遍的理论。这个board也可以展示Galton自己发现的regression toward the mean现象。</p>

<p>解释Galton board的视频：</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UCmPmkHqHXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>这里放上Galton自己制作的父母孩子身高回归图：
<img src="/assets/francis-galton/Galton's_correlation_diagram_1875.jpg" alt="Galton_heights" /></p>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/journal/2020/04/30/journal-2020-04.html">
        2020年4月：更难的方法与有套路的交流
      </a>
    </h2>

    <span class="post-date">30 Apr 2020</span>

    <p><em>这个月收到了稿子的消息，在尝试拼上最后一块问题的“拼图”，以及思考如何坐上司机的位置。</em></p>

    <!-- <p><em>这个月收到了稿子的消息，在尝试拼上最后一块问题的“拼图”，以及思考如何坐上司机的位置。</em></p>

<p>一转眼又两个星期没跟老板（博士导师）讨论研究上的问题了。</p>

<p>上周聊过关于回复审稿人意见的事，第一次投稿和收到修改意见，多少有点忐忑。学术文章发表的过程大致是投稿、编辑抄送给匿名审稿人、审稿人回复（推荐与否加上具体意见）、编辑回复（接受、修改、拒绝）。如果回复是修改，那可能会重复几轮这个步骤。可能是因为疫情的关系，去年十一月就投出去的文章，历时五个月，上上周才收到回复，并且只有两个审稿人。我打心里感谢这两位审稿人在这特殊时期还审我的稿，并且提的大多是寻求进一步解释或者论述性质的意见，而不是“质疑”型的问题。</p>

<p>每周周二，是我们往常组会和见面讨论的日子。自从疫情在新加坡扩散以来，学校先是规定会议要测体温登记时间等等，不久就要求全员在家工作了。全校人员撤离校园的同时，所有跟新冠病毒相关的研究活动得到特殊允许，可以继续开展。我们这一类不用做实验有电脑就能工作的研究，在家工作没太大影响；Science、土木一类需要学校资源做实验的朋友们就没那么幸运了，不知道他们现在在家里如何继续科研，刚好集中精力整理数据写写文章？</p>

<p>不知道是否是出于习惯，周二一上班就把最近困扰我的问题好好组织打稿一番发给了老板。这问题算是我目前工作的最后一块“拼图”吧。粗略来说，我模型里有个未知的参数要估计，但问题是线下（offline)还是线上(online)估计。线下估计需要一定数量的训练数据，也很难保证普遍性，万一有个没见过的情况，效果可能就不好；线上估计的适应性就强很多，也不那么依赖训练样本质量。但是，线上估计的方法我前段时间看了一下文献，没怎么看懂。当时急着想要看看整体方法的表现，姑且用线下学习估计了，初步结果看上去也不错。然而，这参数普遍性的问题在我尝试做大量仿真检验的时候不出意外地出现了！虽然心里知道这问题得解决，但真处理起这最后一块“拼图”时，我还是犹豫的。</p>

<p>问题发给了老板，很快回复了我：可以线上估计，你看看xxx（几个思想的关键词，方便我去查阅）。得到回复的时候没有失望，反而是安心了一些，跟自己说，踏踏实实做吧。</p>

<h3 id="一个学期只联系一次研一都是放养的吗">一个学期只联系一次，研一都是放养的吗？</h3>

<p>前两天偶然看到豆瓣有人问了如上的问题，他/她看上去很困惑，说希望能跟导师有更多的交流。蛮多人回复说这事儿得看老师。确实，每个老板指导风格不同会有不少区别，不过这只是公式的一半吧，自己作为另一半能做的事儿也很多。</p>

<p>我的老板也是中间偏“放养”型的，比较少会主动过问我进展如何，时不时会发点文章给我看，不过我想讨论的时候，也会认真指导。记得博一刚开始那会儿跟老板聊过关于学生跟博导的关系，他觉得读博做研究这事儿归根结底是我们自己的，学生应该把自己放到驱动者的角色；而导师的角色应该是一个advisor/mentor，会负责给评价、提建议、提供我需要的帮助，但并不负责帮我做事儿、监督我做事儿。对这，我是完全同意的，并且庆幸老板在一开始就跟我们说清楚、让我们有了合理的期待(manage expectations)。</p>

<p>我后来意识到，既然要坐到司机的位置上，那就意味着得学习怎么开车。不是说的学习如何做数学推导，而是学习如何做研究生，其中很重要的一项就是如何跟老板保持沟通（尤其是“放养”型）。有些学校会给新入学的研究生提供类似培训的课程，但不是所有学校都这样做。在摸索的过程中，同研究中心的德国博后同事推荐了本小册子给我：“<a href="/assets/month-journal/The 7 Secrets.pdf" target="_blank">The Seven Secrets of Highly Successful Research Students</a>”。我们研究中心有点特殊，中心很多博士都是在瑞士招了然后来新加坡读的，没办法用到瑞士那边大学里的众多资源，他们的博导们也大多base在瑞士。这也是大家比较关心“如何跟老板保持沟通”，“如何有效管理自己的研究进度”等问题的原因吧。小册子里面给出了蛮多诚恳的建议。其中关于如何跟老板保持沟通，交流工作的部分让我也受益匪浅。小册子推荐跟老板交流工作时用这样一个模板：</p>

<ol>
  <li>自从上次会议我做了：1..2..3..</li>
  <li>新遇到的问题：1..2..3..</li>
  <li>（老板的）评价与建议：1..2..3..</li>
  <li>接下来要做：1..2..3..</li>
</ol>

<p>这模板用起来特别容易上手。我是按这样的顺序使用的：根据上次会议老板提出的评价与建议去解决我新遇到的问题，然后去做“接下来要做”的事儿。在这过程中，开启新的一页，并记录新的第一和第二点，然后重复。用这模板跟老板做每周的讨论快一年了，跟以前开会前一天匆忙准备讨论的点相比，能明显感觉到现在自己在讨论时思路清晰很多，比较少因为老板的发散性提问而忘记原本想要讨论的点。另外一个改变是，当我能参照自己的记录，比较准确地讲出这问题的来龙去脉时，往往能得到比较清晰的建议与评价。这样记录下来的笔记，日后往回查看也方便很多，尤其是想要查找老板曾经给出的“自相矛盾”的建议的时候😃。</p>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/03/24/COVID-19.html">
        Tracking the COVID-19 outbreak and signals of containment
      </a>
    </h2>

    <span class="post-date">24 Mar 2020</span>

    <p><strong>Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.</strong> 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are <strong>interactive</strong> and best viewed on a desktop browser.</p>

    <!-- <p><strong>Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.</strong> 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are <strong>interactive</strong> and best viewed on a desktop browser.</p>

<h2 id="signals-of-containment">Signals of Containment</h2>

<p><a name="Confirmed and Death Cases"></a></p>
<h3 id="the-interplay-of-confirmed-and-death-cases">The Interplay of Confirmed and Death Cases</h3>
<p>When should the economy reopen? To try to answer this question, we could look at the interplay of new confirmed cases and death cases.</p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1739652220&amp;format=interactive"></iframe>
<p>More cases means more healthcare resource demand, and doctors and nurses have to make tough decisions. Unfortunately, more patients who need intensive care might not get it, leading to higher fatalities. We are probably going to see a peak in daily cases, and after some time, a peak in daily fatalities. This phenomenon is visible in the graph below. Passing the first peak means measures are taking effect; passing the second means our healthcare system is now able to cope. So, where do countries stand as of now?</p>

<p>Of course, the decision has to also depend on other factors such as the ability of testing and tracking down close contacts of those infected.</p>

<p>There are actually many questions that we could ask from this graph. For example:</p>
<ol>
  <li>Why does Germany has much higher daily confirms than Switzerland, and yet manages a much flatter death curve?</li>
  <li>Why do the two peaks for the UK seem to occur at the same time while that’s not the case for the rest?</li>
</ol>

<ul>
  <li>Countries with hopes of relaxing some of the lockdown measures: Germany and Switzerland. Both of them have passed the peaks, have low daily cases (&lt;20), and relatively flat and low death case curve (&lt;5).</li>
  <li>Countries that probably need more time: They are at the edge of passing the first peak and record about 80 daily cases. What’s more worrying, though, is the evident pressure on the healthcare system. UK sees a drop in daily death cases, but that number is still high at 11; the US’s death case curve seems not at its peak yet. They probably need more time. - April 23, 2020</li>
</ul>

<p><a name="Percentage Change"></a></p>
<h3 id="daily-case-percentage-change">Daily Case Percentage Change</h3>
<p>Look out for the 7-day moving average of the day-on-day percentage change in confirmed cases. It is important to see both the current percentage change and its trend. To easily classify the situation, we can use the following scale<sup id="fnref:percentage"><a href="#fn:percentage" class="footnote">1</a></sup>:</p>
<ul>
  <li><script type="math/tex">r > 10\%</script>: <strong>Rapidly increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 10\% %]]></script>: <strong>Increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 5\% %]]></script>: <strong>Slowly increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 1\% %]]></script>: <strong>Under control</strong>.</li>
</ul>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=565833280&amp;format=interactive"></iframe>
<ul>
  <li>Japan had a turning point on 23rd March where the increase of cases started accelerating. Coincidently (or maybe not), Japan and I.O.C. officially anounced the <a href="https://www.nytimes.com/2020/03/24/sports/olympics/coronavirus-summer-olympics-postponed.html">postponement of Tokyo 2020</a> on the next day.</li>
  <li>The cases in Japan have been rising at an increasing rate, now at a 10% <a href="#Percentage Change">day-on-day growth rate</a>. Considering the exponential growth of infections, Abe, Japanese prime minister, is declaring emergency state for seven prefectures. - April 7, 2020</li>
  <li>Japan sees a slowdown of daily new cases. It’s been two weeks since the first declaration of “Emergency Situation” by the prime minister. On average, a 50% reduction in the number of people going out in monitored areas <a href="https://www3.nhk.or.jp/news/special/coronavirus/#infection-status">are observed</a>. Meanwhile, mask sales have skyrocketed in Japan. - April 22, 2020</li>
</ul>

<p><a name="Google Search Interest"></a></p>
<h3 id="google-search-interest">Google Search Interest</h3>
<p>This figure tells us how many people in the US are searching for keywords such as “hand sanitizer” or “symptom”. I suspect that as the community spread of the virus is being contained, we can expect to see a drop in searches for words like “symptom” and “influenza”, similar to the trends shown in Singapore.</p>

<p>There are drastic differences in terms of the US and Singapore google search interests during this pandemic. When signs of community infection emerged in early March, people in the US were searching for “symptom” at a record-high frequency, similarly for “influenza” and “hand sanitizer”. Searches for “mask”, however, were not so heightened. The picture in Singapore looks very different. When more infections emerged inside the border in late January and early February, the search for “mask” shoot up rapidly, and masks went out of stock everywhere in Singapore. There are probably two main reasons for this:</p>
<ol>
  <li>A high percentage of Chinese living in Singapore;</li>
  <li>As a nation that went through SARS, it feels natural for most people to wear masks when a contagion is spreading in the community.</li>
</ol>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=783455223&amp;format=interactive"></iframe>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=196247116&amp;format=interactive"></iframe>

<p><a name="US Testing Numbers"></a></p>
<h3 id="us-testing-numbers">US Testing Numbers</h3>
<p>As the containment takes effect, we expect to see the number of positive and negative tests stabilize, and the number of tests pending result drops. As you can see, we are not there yet.</p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=481777218&amp;format=interactive"></iframe>

<h2 id="cumulative-case-progression">Cumulative Case Progression</h2>
<hr />
<p><a name="Case progression"></a></p>
<iframe width="696.0000000000001" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=967719983&amp;format=interactive"></iframe>
<ul>
  <li>Japan has a relatively flat curve. However, there are legitimate concerns that Japan has been under-testing its population to know what is really going on. Assuming the true CFR is 1.2%<sup id="fnref:diamond_princess"><a href="#fn:diamond_princess" class="footnote">2</a></sup>, Japan’s current fatality number, 77, indicates that at least 6,417 people have been infected. However, only 3,139 cases are officially confirmed as of now. Also, Japan has conducted 486 tests <a href="#https://www.worldometers.info/coronavirus/">per one million population</a>. In Singapore, that number is 11,110. - April 5, 2020.</li>
  <li>For the first time, Singapore is going into a national “Shelter in Place” mode. The timeing is not surprising as some degree of wide-spread community infection is going on. The number of unlinked cases, those yet to find the source of infection, spiked over the last few days; Singapore also recorded 12 new clusters of infection just over the past five days (One of them is right across the river from my house). - April 5, 2020.</li>
  <li>Singapore sees a steady increase in confirmed cases, mainly in foreign worker dormitory clusters. However, if we look at the <a href="#Case progression">progression of confirmed cases</a> in Singapore, it’s an almost perfect example of what “flatten the curve” looks like. For the most part, the cases double every ten days, whereas cases in some of the worst-hit countries double every one to three days.  - April 8, 2020</li>
</ul>

<h2 id="death-cases">Death Cases</h2>
<hr />
<p><a name="case fatality rate"></a></p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;format=interactive"></iframe>
<ul>
  <li>Germany and Switzerland fare well in this regard and manage to record comparatively low CFRs. Austria, too, has managed one of the lowerest CFRs among European nations. Austria, Germany, and a large part of Switzerland are German-speaking.🤔</li>
  <li>While the CFRs in Switzerland and Germany have been comparatively low, they are steadily climbing. Switzerland is probably the first country in Europe to flatten the curve, which conducted one of the highest number of tests <a href="#https://www.worldometers.info/coronavirus/">per one million population</a>. - April 10, 2020</li>
</ul>

<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;format=interactive"></iframe>

<p><a name="cfr bias"></a></p>
<ul>
  <li>How does selection bias affect CFR?
    <blockquote>
      <p>[In Italy], a change in strategy on Feb 25 limited testing to patients who had severe signs and symptoms also resulted in a 19% positive rate (21,157 of 109,170 tested as of Mar 14) and an apparent increase in the death rate—from 3.1% on Feb 24 to 7.2% on Mar 17—patients with milder illness were no longer tested.  In the UK, only patients deemed ill enough to require at least one night in hospital met the criteria for a Covid-19 test.</p>

      <p>CFR rates are subject to selection bias as more severe cases are tested, generally those in the hospital settings or those with more severe symptoms. The number of currently infected asymptomatics is uncertain: estimates put it at least a half are asymptomatic; the proportion not coming forward for testing is also highly doubtful (i.e. you are symptomatic, but you do not present for testing). Therefore we can assume the IFR is significantly lower than the CFR.</p>
    </blockquote>
  </li>
  <li>When is CFR accurate?
    <blockquote>
      <p>Iceland’s higher rates of testing, the smaller population, and their ability to ascertain all those with Sars-CoV-2  means they can obtain. an accurate estimate of the CFR and the infection fatality rate (IFR) during the pandemic (most countries will only be able to do this after the pandemic). Current data from Iceland suggests their IFR is somewhere between 0.01% and 0.19%.</p>
    </blockquote>
  </li>
</ul>

<p>The bottom line is, CFR is probably <strong>inflated</strong> in many countries and IFR is <strong>much lower</strong> than CFR.</p>

<h2 id="resources">Resources</h2>
<h4 id="websites">Websites</h4>
<ol>
  <li><strong>Bloomberg</strong>: <a href="https://www.bloomberg.com/graphics/2020-coronavirus-cases-world-map/?srnd=premium-asia">Mapping the Coronavirus Outbreak Across the World</a></li>
  <li><strong>Johns Hopkins University</strong>: <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">Coronavirus COVID-19 Global Cases</a></li>
  <li><strong>Global MediXchange</strong>: <a href="https://www.alibabacloud.com/universal-service/pdf_reader?spm=a3c0i.14138300.8102420620.dreadnow.646d647fDWbsii&amp;cdnorigin=video-intl&amp;pdf=Read%20Online-Handbook%20of%20COVID-19%20Prevention%20and%20Treatment.pdf">Handbook of COVID-19 Prevention and Treatment</a></li>
</ol>

<h4 id="articles">Articles</h4>
<ol>
  <li><a href="https://www.nytimes.com/2020/03/19/us/politics/trump-coronavirus-outbreak.html">Before Virus Outbreak, a Cascade of Warnings Went Unheeded</a>, March 19, 2020</li>
  <li><a href="https://www.citylab.com/life/2020/03/coronavirus-cases-france-train-hospital-tgv-covid-19-patient/608833/">To Fight a Fast-Moving Pandemic, Get a Faster Hospital</a>, March 26, 2020</li>
  <li><a href="https://www.businessinsider.sg/coronavirus-spain-says-rapid-tests-sent-from-china-missing-cases-2020-3?_ga=2.212074516.1285585527.1585620210-963085568.1583747541&amp;r=US&amp;IR=T">Spain, Europe’s worst-hit country after Italy, says coronavirus tests it bought from China are failing to detect positive cases</a>, March 26, 2020</li>
  <li><a href="https://time.com/5812555/germany-coronavirus-deaths/">Why Is Germany’s Coronavirus Death Rate So Low?</a>, March 30, 2020</li>
  <li><a href="https://www.nytimes.com/interactive/2020/04/14/science/coronavirus-transmission-cough-6-feet-ar-ul.html">This 3-D Simulation Shows Why Social Distancing Is So Important</a>, April 14, 2020</li>
</ol>

<h4 id="data-sources">Data sources</h4>
<ul>
  <li>Japan: <a href="https://www3.nhk.or.jp/news/special/coronavirus/#infection-status">NHK</a></li>
  <li>Singapore: <a href="https://www.moh.gov.sg/covid-19">Ministry of Health</a></li>
  <li>Other countries: JHU <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">Coronavirus COVID-19 Global Cases</a></li>
  <li>US testing numbers: <a href="https://covidtracking.com/">The COIVD Tracking Project</a></li>
  <li>Search interests: <a href="https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=%2Fm%2F0b23px,%2Fm%2F01kr41,%2Fm%2F0cycc,%2Fm%2F01b_06">Google Trends</a></li>
</ul>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes">
  <ol>
    <li id="fn:percentage">
      <p>The percentage only indicates a relative change. The actual number of new cases reported in each country may be very different, as it depends on the absolute number of cumulative cases in that country. <a href="#fnref:percentage" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:diamond_princess">
      <p>Russell, Timothy W., et al. “<a href="https://www.medrxiv.org/content/10.1101/2020.03.05.20031773v2">Estimating the infection and case fatality ratio for COVID-19 using age-adjusted data from the outbreak on the Diamond Princess cruise ship.</a>” medRxiv (2020). <a href="#fnref:diamond_princess" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-statistical_learning_map.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools. 
<img src="/assets/ST-road-map/mindmap.png" alt="Mindmap" /></p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools. 
<img src="/assets/ST-road-map/mindmap.png" alt="Mindmap" /></p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-line-outage-detection-via-sequential-hypothesis-testing.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-line-outage-detection-based-on-particle-filtering.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-introduction-to-particle-filtering.html">
        Particle filtering: vallina and advanced
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

    <!-- <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

<p>Other methods - Kalman-based fiter?</p>
<ul>
  <li>limitations</li>
</ul>

<p>The method - Bayesian estimation through Monte Carlo method</p>
<ul>
  <li>Bayesian estimation</li>
  <li>Monte Carlo method</li>
</ul>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2019/11/12/how-to-read.html">
        读书笔记：读书体验是什么
      </a>
    </h2>

    <span class="post-date">12 Nov 2019</span>

    <p>我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。</p>

    <!-- <p>我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。</p>

<p style="display: block; margin-left: auto; margin-right: auto; width: 50%;"><img src="/assets/2019-11-12/book_cover.jpg" alt="cover" /></p>
<ul>
  <li>书名：<a href="https://book.douban.com/subject/26789567/">如何有效阅读一本书</a></li>
  <li>原名：読書は1冊のノートにまとめなさい</li>
  <li>作者：<a href="http://okuno0904.com/about/index.html">奥野宣之</a>（おくの・のぶゆき）</li>
  <li>购入日期：2019.11.11（打折！）</li>
</ul>

<h3 id="读书体验">读书体验</h3>
<p>这本149页的小书，我觉得可以看做是“如何读书”的一本参考书；里面介绍的观点和方法，不同级别的读者都可以在适当的时候读一读，并学到些东西。这本小书旨在回答这样一个问题：如何才能不忘记读过的书中的内容，并使之融入身心，真正使书籍影响自己？</p>

<p>而他的回答是，我们应该重新思考读书这件事儿。读书不应始于翻开书籍，也不终于最后一页。每一次读书，我们应该去创造属于自己的“读书体验”。要如何理解这个“体验”呢？我们可以想想生活中的其他体验，尤其是使用体验。我不经常买东西，但如果要买什么，会尽量去想清楚为什么要买，会提前去了解这个东西的背景，功能，评价，使用过程中会不断更新最初的设想，并持续影响我下一次购物的判断。这也就是一次有意识有目的能影响未来的购物体验。也就是说，作者推崇的是一种有目的能重温、甚至历久弥新的读书体验。实际上，作者也认为，这样的读书体验比书本身重要多了。</p>

<p>作者总结了下面五个具体可行的步骤来创造所谓的读书体验：</p>
<ol>
  <li><strong>选书</strong>：收集随想，建立目的</li>
  <li><strong>购书</strong>：冷静评估，书籍确认</li>
  <li><strong>读书</strong>：适当标记，提炼重点</li>
  <li><strong>记录</strong>：原文摘抄，原创思考</li>
  <li><strong>活用</strong>：重读笔记，思想输出</li>
</ol>

<p>而在作者的心中，创造这个读书体验必不可少的伙伴是一个普通的笔记本。因为它在上面五个步骤中所发挥的重要作用，这笔记本应时常伴我们左右，并且，如果直译本书的原书名，你会发现，它其实意思是“请用一个笔记本整理你的读书”。</p>

<h3 id="用购书清单指名购书">用购书清单指名购书</h3>
<p>创造读书体验的第一步，是给自己开出一个购书清单。开清单不是为了逛书店的时候容易找（也有这好处），更重要的是，让我们能清楚认识到读每一本书的目的是什么。作者是这么说的：</p>
<blockquote>
  <p>那么，为什么要把列清单的过程也作为读书方法的一部分来说明呢？理由之一，就是要培养带着目的去读书的目的意识。</p>
</blockquote>

<p>下面是作者推荐的选书购书的具体操作步骤：</p>

<blockquote>
  <p><strong>好奇心激发</strong> → <strong>随想笔记</strong> → <strong>购书清单</strong> → <strong>购书</strong></p>
</blockquote>

<p>第一步是将激发好奇心的源头记进笔记本里，可以叫做随想笔记。这源头的可能性就很多了：</p>
<ul>
  <li>报刊上读到有意思的书评</li>
  <li>听到感兴趣的政治时事评论</li>
  <li>来自朋友的书籍推荐</li>
  <li>等等</li>
</ul>

<p>只要是激起了我们好奇心的东西，都应该记进随想笔记里去。之后便能根据随想笔记，建立起读书的目的，并去寻找相关书籍。在经过冷静的评估之后，将想要购买的书籍列进清单。这流程的好处是，我们相对能够准确地选出自己真正想读并且明白为什么想读的书，这样买回来感兴趣、读下去的几率都比较高（这个很重要😂）。并且，在读书过程中，可以带着最初被激发的好奇心去读，读完之后也能回顾一开始好奇心被激发的契机，因为这些都被一一记录在笔记本里。</p>

<h3 id="用笔记把读过的书变为精神财富">用笔记把读过的书变为精神财富</h3>
<p>这里讲的包含了步骤三和四：读书，记录。</p>

<p>读书的时候，我们遵循这样一个流程：通读 → 片段重读 → 标记。意思就是，通读之后，去重读你感到有共鸣、疑惑、感兴趣等等的片段，有必要就用统一符号标记下来，比如：</p>
<ul>
  <li>下划直线 ＿＿：客观重要</li>
  <li>下划波浪线 ˷˷˷˷˷˷：我觉得重要，非常重要</li>
  <li>圆圈 ◯：关键词、专业名称</li>
</ul>

<p>这样读过一个章节、一本书，就能进入到下一个步骤：记录。这个时候就可以一一回顾上一步所标记出来的部分，参照下面的格式，在笔记本上写下读过这本书后的读书笔记：</p>
<ul>
  <li>⚬ …接原文摘抄、要点概括</li>
  <li>⭑ …接评论、感想</li>
  <li>重复上面</li>
</ul>

<p>具体来说，我们摘抄的时候，可以摘抄些什么呢？</p>
<ol>
  <li>能让我主观产生共鸣的</li>
  <li>不是读后觉得”理应如此“，而是“这么一说确实如此”的</li>
  <li>能颠覆我已有的想法、动摇我认识的</li>
  <li>等等</li>
</ol>

<p>摘抄或要点概括很大程度上是原作者的思想，而在评论里，我们应该尽量去挖掘些原创的思考。这会是个耗时间费精力的过程，不过只有试过的人才能知道到底值不值得。另外值得一提的是，作者还提倡将跟这本书有关的时间、空间印记也一起贴进笔记本里，比如说新书买来时的书腰、看那本书时所坐火车的票根等等。以后的某个时间，再次读起笔记本的这一页，看那本书时所经过的风景闻过的花香也会跃然纸上吧。</p>

<p>当然了，上面讲的记录的形式都是作者的推荐，我们大可不必居于某种形式，只需按照自己舒服的方式坚持记下属于自己的读书笔记就好了：</p>
<blockquote>
  <p>说句老生常谈的话，只有把读书笔记控制在自己能力允许的范围内，才能长久地坚持下去。所以，要选择对自己来说比较方便的笔记方法。</p>
</blockquote>

<p>我想这道理几乎适用于所有需要长久坚持的事，探究一门深奥的学问、学习一门外语、亦或是健身减肥等等。我发现人往往能轻松看到漫长过程之后的一种状态，这或许是我们这样高等生物的特殊能力；但人又往往忍不住会对期待的状态过于着急。这的确与我们身处的社会环境有所关系，但在我们的基因当中是否也有着这种既有远见又企求触手可得的回报的种子呢？</p>

<p>我在读这本书的时候所做的读书笔记就没有按照作者推荐的格式，而是采用了自己习惯的类似于PPT设计的风格：
<img src="/assets/2019-11-12/how_to_read.jpg" alt="how_to_read" /></p>

<p>不难看出，笔记里的结构几乎原封不动地变成了我这篇文章的结构，再加上在书里相关标记写下的评论，这篇文章的主要内容在我做完读书笔记的同时也就完成了。而这也是作者推崇的，以自己的读书笔记为基础，进一步写出原创文章，做属于自己的思想的输出。</p>

<h3 id="通过重读笔记提高自我">通过重读笔记提高自我</h3>
<p>读书体验的最后一步，也是我从没做过的一步：重读笔记。就像有人会偶尔重读日记一样，时常重读读书笔记能让自己读过的书好像一直存在自己生活中，不断酝酿，不断跟自己的经历、知识发生新的碰撞：</p>
<blockquote>
  <p>如果把一本书比作一个“场所”，那么读书笔记就是在这个场所拍摄的照片。在不同时间去同一个场所拍照，拍出来的照片都会有所不同，而过一段时间再去看这些照片，对那个场所的印象也会发生变化。</p>
</blockquote>

<p>关于如何重读，作者推荐：</p>
<ul>
  <li>简单回顾：读笔记</li>
  <li>细致回顾：读笔记 + 原书标记</li>
  <li>经典重温：读笔记 + 原书</li>
</ul>

<p>这么看，我确实一个回顾都没做过😂。一开始读这本书是因为买了新的笔记本，我老是买新笔记本，想着要写点什么文字，最后都沦为平时工作用的草稿纸（也是很重要啦）。看了这本书的评论觉得可能会帮我结束这个循环，目前看来很有希望。读之前，如何写读书笔记是我最感兴趣的部分，不过读完发现，以随想笔记到笔记回顾的一整个读书体验来理解读书这事儿，才是奥野宣之这本书给我最大的收获吧。</p>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2019/11/11/reading-academic-deep-work.html">
        深度工作：一个研究者（我自己）应该做的事
      </a>
    </h2>

    <span class="post-date">11 Nov 2019</span>

    <p>什么是深度工作？为什么即时讯息不会提高而是降低我的工作效率？为什么忙绿工作了十个小时却并没完成什么重要的事情？为什么我要选择鼓励深度工作的公司？对于这些问题，Newport在他的深度工作里做出了回答。我想利用这篇文章来总结一下此书，读后的感想以及能为我所用的知识。后半段将结合马华灵先生建议的学术江湖的七种武器，提炼出对于我这般刚上路的研究者，应该花足够精力和时间去做的事儿，换言之：一个研究者应该做的深度工作。</p>

    <!-- <p>什么是深度工作？为什么即时讯息不会提高而是降低我的工作效率？为什么忙绿工作了十个小时却并没完成什么重要的事情？为什么我要选择鼓励深度工作的公司？对于这些问题，Newport在他的深度工作里做出了回答。我想利用这篇文章来总结一下此书，读后的感想以及能为我所用的知识。后半段将结合马华灵先生建议的学术江湖的七种武器，提炼出对于我这般刚上路的研究者，应该花足够精力和时间去做的事儿，换言之：一个研究者应该做的深度工作。</p>

<h2 id="深度工作">深度工作</h2>
<p>Cal Newport的《深度工作》<a href="https://www.calnewport.com/books/deep-work/">(Deep Work)</a></p>

<h4 id="ritualize-the-deep-work">Ritualize the deep work</h4>

<ol>
  <li>Where you’ll work and for how long?
    <ul>
      <li>制定好深度工作的地点与时间，尽最大可能保持这个地点和时间。如果中间有会议或者seminar之类的，提前调整好时间，算作是浅型工作，总的来说要保持弹性。
        <ul>
          <li>Place of work: Home office; Duration: 9am - 5pm:
            <ul>
              <li>Plan my work: 830am - 9am</li>
              <li>Deep work: 9am - 12am</li>
              <li>Lunch: 12am - 1pm</li>
              <li>Shallow work: 1pm - 130pm</li>
              <li>Nap: 0130pm - 2pm</li>
              <li>Deep work: 2pm - 530pm</li>
              <li>Shallow work: 530pm - 6pm</li>
            </ul>
          </li>
          <li>Optional
            <ul>
              <li>Dinner: 6pm - 7pm</li>
              <li>Japanese/Blog: 830pm - 930pm</li>
              <li>Reading: 930pm - 1030pm</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How you’ll work once you start to work?
    <ul>
      <li>制定好深度工作时的规矩，必须遵循这个规矩。这个规矩应该可以尽可能实际，也能保证深度工作的效果。</li>
      <li>深度工作时：
        <ul>
          <li>坚决不使用手机；</li>
          <li>电脑上不查email，不浏览跟工作无关的网页，不看跟工作无关的文件。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How you’ll support your work?
    <ul>
      <li>尽可能从环境、吃喝等等方面帮助完成上面的ritual。</li>
      <li>准备：
        <ul>
          <li>早晨深度工作开始之前喝一杯咖啡，并将水壶接满水；</li>
          <li>9点开启潮汐专注模式，并将手机放进抽屉；</li>
          <li>下午2点开始潮汐专注模式，并将手机放进抽屉。</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h4 id="embrace-boredom">Embrace boredom</h4>

<ul>
  <li>This is to prevent our brains from being addicted to distractions. Resist the temptation to use handphone or surf the internet at the slightest hint of boredom.</li>
  <li>Memory training helps one’s ability to concentrate.</li>
</ul>

<h4 id="quit-social-media">Quit social media</h4>
<ul>
  <li>Put conscience effort in scheduling off-work hour activities. Our brain relaxes when we swtich focus, not by induldging in semi-conscious web surfing.
    <ul>
      <li>晚上的时候不固定具体时间和时长，但是尽量做到三件事：
        <ul>
          <li>背N1单词</li>
          <li>读正在读的书</li>
          <li>11点睡床上</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="学术武器">学术武器</h2>
<p><a href="https://www.douban.com/note/671893735/">马华灵：我的思想历程（附：学术江湖的七种武器）</a></p>

<p>马华灵根据自己的经历总结出了做学术需要修炼的七大“武器”，这七个方向包括了对学术人的追求的拷问，也有对做学术的具体操作的指导，下面列出他的七大“武器”以及我的一些总结和思考。</p>

<ul>
  <li>学术源泉
    <ul>
      <li>我为什么做学术？</li>
      <li>什么是学术？</li>
      <li>学术的发展历史是怎样的？</li>
    </ul>
  </li>
  <li>学术问题
    <ul>
      <li>我的学术问题是什么？</li>
    </ul>
  </li>
  <li>学术概念
    <ul>
      <li>我对我使用的概念真的了解了吗？</li>
    </ul>
  </li>
  <li>学术阅读
    <ul>
      <li>梳理逻辑框架</li>
      <li>把握文章论述重点</li>
    </ul>
  </li>
  <li>学术笔记
    <ul>
      <li>记录并明白核心框架</li>
      <li>建立一个有层次的文章思路地图（i.e. Sections, subsections, sub-subsections）</li>
      <li>注明文章的出版信息和页码</li>
      <li>尝试对文章的每一步进行质问</li>
      <li>总结文章的优点和缺点，思考优点如何借鉴，缺点如何避免与解决</li>
    </ul>
  </li>
  <li>学术批评
    <ul>
      <li>建设性批评</li>
      <li>目的是能完善文章</li>
    </ul>
  </li>
  <li>学术写作
    <ul>
      <li>逻辑清晰</li>
      <li>观点明确</li>
      <li>化整为零，然后化零为整（i.e. paragraph to subsection, subsection to section, section to paper）</li>
    </ul>
  </li>
</ul>
 -->
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

    </div>

  </body>
</html>
