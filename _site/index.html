<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Xiaozhou's Notes &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">




  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <!-- <a href="/"> -->
          Xiaozhou's Notes
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">Publication</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/yangxiaozhou/yangxiaozhou.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:xiaozhou.yang@u.nus.edu" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/yangxiaozhou" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/YangXiaozhou" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/a315345751" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h2 class="post-title">
      <a href="/journal/2020/06/06/journal-2020-05.html">
        2020年5月：浅尝Snack Writing
      </a>
    </h2>

    <span class="post-date">06 Jun 2020</span>

    <p>五月因为疫情的原因继续在家里工作。这段时间工作重心主要在写东西上面，不太需要跑大量的数据、也不需要太多跟同事老板的交流，虽然我蛮想念办公室的espresso咖啡和时不时不同领域的人过来做的seminar的，这两件事儿家里没法复制。</p>

    <!-- <p>五月因为疫情的原因继续在家里工作。这段时间工作重心主要在写东西上面，不太需要跑大量的数据、也不需要太多跟同事老板的交流，虽然我蛮想念办公室的espresso咖啡和时不时不同领域的人过来做的seminar的，这两件事儿家里没法复制。</p>

<p>提交了上一篇文章的审稿意见回复，两个审稿人提了不少问题但都是建设性或者澄清类的，我写的回复初稿被老板修改不少（First version is always subpar.)。我发现我的回复往往更婉转、迂回，用词也习惯性地用我熟悉的这个工作的语言；而对比老板修改过的版本，表达的点是一样的，但这个点在他的回复里就很明确、更assertive，同时也把那些更技术性的词语换成了更易理解的。这么一想，每次做报告的问答环节我也是这样。最后提交上去的回复，给我的感觉是：同意的点会明确说、不清楚的点会用浅显的语言解释、误解了的点会用不卑不亢的姿态澄清。最后根据审稿人意见做过修改的文章，确实是要比之前的版本更完整、可能造成误解的地方更少了。</p>

<p>下半月把之前好不容易搞懂的方法用$\LaTeX$一一打出来写进了自己的笔记里，然后又基于这个笔记写了第二个工作的文章的初稿（主要是方法部分），包括电力系统的动态模型、用particle filter做状态估计以及如何用EM做参数估计。方法里面自己花时间最久的应该是最后那个参数估计的方法，上一篇月志也提到过，在笔记里的篇幅也是最长的，因为各种步骤。不过，因为不是这个工作的重点，最终写到文章里就只有几段话。这道理，博一的时候<a href="https://www.eng.nus.edu.sg/isem/staff/ye-zhisheng/">叶老师</a>课上就已经跟我们分享过了，写文章的时候要</p>
<blockquote>
  <p>Write around your contributions, not about what you have learnt.</p>
</blockquote>

<p>这一次写，尝试了一个新的写作模式：每天早上起床后写一个小时任何我现在在写的、在学的东西。缘由是我回去再读了一下上个日志说到的那本书，里面有专门写到研究生应该如何写作。这里的“如何写作”不是指的写作风格、语言组织之类，而是指什么时候写，写些什么。书中主要提到</p>
<ol>
  <li>Don’t write when ready, write to be ready.</li>
  <li>Don’t be binge writing, be snack writing.</li>
  <li>Schedule regular sessions for writing.</li>
  <li>Writing means writing, not editing.</li>
</ol>

<p>确实是这样。我经常觉得得等我所有方法都完全搞懂了，每一个细节都走通了，才能动手开始写；而且因为经常是在短时间内写大量的内容，写完总会感觉很累；有的时候也会发现花了时间却只写了几句话，原来是时间都用去做编辑$\LaTeX$符号、插入文献这种事了。</p>

<p>所以我给自己定了个新的写作模式：早上起床后专注写一小时的内容，内容是接着昨天写的，并且只有写作；白天的时候再来慢慢修改字符，添加文献和检查错别字之类的编辑工作。这样的模式写了大半月，发现写作进度确实很快（相对以前），一点一点把之前大多只是停留在草稿本和代码上的工作都系统地写进了笔记，然后又根据笔记写了文章的初稿，在写的过程中也把很多细节的东西捋清了不少。不过，我觉得用这样的写作模式有一个要求，那就是得提前打好草稿：具体内容、提纲、每一部分的核心内容等自己得知道，不然的话，早上一小时很难写出东西，即使写出来了，大部分内容我后来都想改掉重写。</p>

<p>为什么要把学过的东西像书一样写进笔记里呢？因为前面写到的写作模式和需要写文章的原因，开始寻思着能不能把花时间搞懂的东西以系统的、我自己能明白的方式写进笔记？当然可以，但问题是为什么要这么做？毕竟学术文章所需的内容和行文风格跟笔记还是很不一样的，这就意味着更多的写作工作量，$\LaTeX$写起来也没那么轻松。其实这事儿前两年也想过，主要还是想建立一个属于自己的知识体系并能不断修改和扩充它吧。</p>

<p>前段时间因为写“<a href="https://yangxiaozhou.github.io/data/2020/05/17/francis-galton.html">Francis Galton: 维多利亚时代的博学家与他观察到的奇妙世界</a>”，在豆瓣搜索关于Galton的文章，读到了于淼的博客，起初惊叹于他的笔触间渗透着生物、数学、统计和编程等方面多年积累的知识以及对于科研和这个世界的一些独到的见解。了解之后才发现这位来自中科院的博士在环境科学方面做研究，16年从中科院毕业的他算是超哥（我的co-author）小三届的师弟。从时间线上看得到，读博以来这么多年，一直有在博客上写讨论各种问题的博文，大多是基于观察、学习与研究的议题。同时这位哥们热衷于上网课读教材，更爱做笔记整理知识，目前他的博客上有这么多年积攒下来的知识笔记的<a href="http://yufree.github.io/notes/index.html">汇编</a>，最后我发现，他还是统计之都的编辑部主编。厉害！</p>

<p>大概就是想尝试做这个事儿吧，一个一个对学过的东西做系统性的整理，梳理框架然后写成相对完整的笔记，对框架里的每个部分逐渐形成自己的理解；在之后的扩充过程中既能找到新知识在框架中的位置，也能不断更新自己的框架和理解。</p>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2020/06/01/reading-guns-germs-steels.html">
        枪炮、病菌与钢铁
      </a>
    </h2>

    <span class="post-date">01 Jun 2020</span>

    <p>[History: Science or art?]</p>

    <!-- <p>[History: Science or art?]</p>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/05/17/francis-galton.html">
        Francis Galton: 维多利亚时代的博学家与他观察到的奇妙世界
      </a>
    </h2>

    <span class="post-date">17 May 2020</span>

    <p>周末读Aeon的一篇文章：<a href="https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;utm_medium=email&amp;utm_term=0_411a82e59d-f7c118f081-69607277">Algorithms associating appearance and criminality have a dark past</a>，讲现在有研究人员用机器学习算法通过人脸来判断某人犯罪的几率。文中讲到这种从人外表提取预见性特征的尝试，在犯罪学历史上并不新奇，19世纪的意大利犯罪学家Cesare Lombroso认为罪犯的脸部有独特的样貌：突出的前额、鹰型鼻梁；而18世纪的Francis Galton则尝试回答一个更广泛的问题：人的外表跟他或她的健康状况、犯罪倾向、智力等等有关系吗？或者说，人的基因是否决定了健康、行为、智力和竞争力？</p>

    <!-- <p>周末读Aeon的一篇文章：<a href="https://aeon.co/ideas/algorithms-associating-appearance-and-criminality-have-a-dark-past?utm_source=Aeon+Newsletter&amp;utm_campaign=f7c118f081-EMAIL_CAMPAIGN_2020_05_11_01_52&amp;utm_medium=email&amp;utm_term=0_411a82e59d-f7c118f081-69607277">Algorithms associating appearance and criminality have a dark past</a>，讲现在有研究人员用机器学习算法通过人脸来判断某人犯罪的几率。文中讲到这种从人外表提取预见性特征的尝试，在犯罪学历史上并不新奇，19世纪的意大利犯罪学家Cesare Lombroso认为罪犯的脸部有独特的样貌：突出的前额、鹰型鼻梁；而18世纪的Francis Galton则尝试回答一个更广泛的问题：人的外表跟他或她的健康状况、犯罪倾向、智力等等有关系吗？或者说，人的基因是否决定了健康、行为、智力和竞争力？</p>

<h3 id="francis-galton是谁">Francis Galton是谁？</h3>
<p>这名字看起来有点眼熟，我隐约记得在老板的一门Forecasting统计课上听到过。仔细一想，对，在线性回归的部分，老板上课专门介绍了他。Sir Francis Galton，姓Galton，名Francis，但当提到他时，出于礼仪，你得加个Sir，因为他在1909年被英国女王授予了骑士爵位。为什么在讲线性回归的时候要介绍他呢？因为他作为第一个人，观察并记录了这样一种现象<sup id="fnref:Galton_heights"><a href="#fn:Galton_heights" class="footnote">1</a></sup>：平均身高很高的父母，往往会有身高更接近普通的孩子；而平均身高偏低的父母的孩子，成年后通常有着更接近普通人的身高。
下图是<a href="https://www.ams.org/journals/bull/2013-50-01/S0273-0979-2012-01374-5/S0273-0979-2012-01374-5.pdf">Bradley Efron</a>根据Galton当时收集到的父母和孩子的身高数据重新制的图，完美地展现了我们现在所知道的Bivariate normal distribution。
<img src="/assets/francis-galton/regression_to_mean.png" alt="regression_to_mean" /></p>

<p>他把这种现象称为<a href="https://www.jstor.org/stable/2841583">regression towards mediocrity</a>，现在通常叫做regression toward the mean，中文貌似叫“向均数回归”。同样的现象，我们在生活中很多地方都能观察到：因为运气而押中题目的学生考出了高分，下一次考试的成绩却没那么突出；连续投中三个三分球的朋友，下个球往往“容易”失手；我上周做<a href="https://yangxiaozhou.github.io/learning/2019/01/01/recipe.html#%E6%B2%B9%E6%B3%BC%E7%8C%AA%E6%89%8B">油泼猪手</a>时各种调料拿捏得很好，味道超棒，这周再做一次，大概率味道会比较普通🤷‍♂️。</p>

<p>符合这原则的现象，他们有一个共通点：他们的结果往往完全或部分由随机因素决定，而随机因素的影响往往符合以0为中心的正态分布（时好时坏）。比如说，三分球进或不进，有投手能力的影响但也有运气的成分；我做的某道菜的味道，取决于下厨能力，但我的专心程度、手抖程度以及心情等几乎随机的因素也会有所影响。也就是说，假设某一天我超级走运，做出了迄今为止最好吃的一道菜，这种事件发生的概率是很小的（得到正态分布上的极大值或极小值的概率）。下一次做，大概率我会正常发挥，菜的味道也没上次好（取到了正态分布上0周围的某个值）。</p>

<p>想象这样一种情况：朋友在我搬新家的时候来家里吃饭，刚好碰到我前面说的超常发挥，都说做的猪手好吃！过了几个月，家里聚会，应朋友强烈要求，再次做出一盘猪手，不过这次是正常发挥。朋友吃后回忆起之前，评论到：“水平下降了呀！” 我冤不冤？ 这样的冤枉我们生活中还真不少，以至于它有个专门的称呼：Regression fallacy，中文叫“回归谬误”。Daniel Kahneman讲过亲身经历的这样<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3292229/">一个例子</a>：他有一次给飞行员学校做培训，提到了表扬能使学员变得更优秀。下面的一个教官不同意了，说他每次一夸完降落做得简直完美的学员，下一次一定做得没那么好，而刚被他骂过的学员，马上就能看到提升。听了教官的抗议，Kahneman当下有了一个eureka moment，他说道：</p>
<blockquote>
  <p>because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.</p>
</blockquote>

<p>回归谬误可以用数学证明，假设两个变量以bivariate normal distribution分布，只要他们的correlation小于1，就会有回归谬误出现，对证明感兴趣的朋友可以看<a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">维基</a>。</p>

<h3 id="年度公牛体重竞猜">年度公牛体重竞猜</h3>
<p>让我们回到观察小天才Galton。1907年三月的自然杂志上刊登了他一篇篇幅只有一页的<a href="https://www.nature.com/articles/075450a0">来信</a>，名为：Vox Populi，直译为“民众的声音”，现在指大多数人的意见。住在英国Plymouth的他，注意到了家附近的镇子上每年都有举办这样一种家禽体重竞猜活动：主办方拉一头牛出来，参与竞猜的本地农夫、屠夫等感兴趣且有经验者对牛进行评估，并将他认为这头牛被宰杀洗净之后的体重提交上去。本着对大众智慧的科学研究态度，他通过某种方式获得了一次竞猜比赛中的数据：牛的真实体重以及787个竞猜者的估计。</p>

<p style="display: block; margin-left: auto; margin-right: auto; width: 100%;"><img src="/assets/francis-galton/stock_show.jpg" alt="stock_show" /></p>

<p>他把提交的所有竞猜体重从小到大排列开，发现中位数（一半的数比它低，一半比它高，”median”一词就是他给取的）是1207磅，而那头牛的真实净体重是1198磅，也就是说，民众的判断在这里跟真实值只差了0.8%！<sup id="fnref:correction"><a href="#fn:correction" class="footnote">2</a></sup>
在那个线性回归还不是所有数据分析课程的第一节课，数据科学也还不是一种职业的时候，Galton从787个竞猜体重中通过简单的手算看到了以平均值或中位数对真实值进行估计的准确性。现在我们知道了，sample mean is an unbiased estimator of the true population mean。</p>

<p>Galton的观察没有停在估计的准确性上，他还想知道，每个人估计的误差有多大。他随即把所有估计与中位数的偏差画了出来，他发现，每一个有经验的“肉眼测体重者”所做的估计，从低估的到高估的，一系列的偏差与正态分布极为相似。也就是说，如果把真实净体重看做是这个采样分布的mean，那任意一个参赛者（有经验的）来估计，他的估计值将是以真实净体重为中心的正态分布而分布着的（绕口！）。Let’s try again. The estimate by any pair of trained eyes is distributed normally around the true dressed weight of the ox. 这里我们得提一个无数现代科学依赖的理论：Central limit theorem (CLT)。对，就是那个可以解释为什么正态分布在现实生活中如此普遍的理论。因为CLT，我们现在确切地知道，当样本量足够大时，样本平均值呈以真实值为中心的正态分布。所以，从年度公牛体重竞猜的真实数据上，他，Sir Francis Galton，看到了central limit theorem。</p>

<p>附上他原稿里的跟理论正态分布做对比的图，横轴是百分位，纵轴是偏差：
<img src="/assets/francis-galton/francis-galton-the-wisdom-of-crowds.jpg" alt="francis-galton-the-wisdom-of-crowds" /></p>

<h3 id="galton-board与柏青哥">Galton Board与柏青哥</h3>
<p>Galton对于这种没有征兆但又近乎定律般呈正态分布的偏差非常着迷，他把偏差呈现出来的图称为:The Curve of Frenquency，也就是我们现在熟知的样本偏差的正态分布图。为了展现这种偏差的正态分布（即，CLT)以及前面提到的回归谬误，Galton设计了一个令人拍案叫绝的装置：Galton Board，现在也叫bean machine，如下图：</p>

<p style="display: block; margin-left: auto; margin-right: auto; width: 75%;"><img src="/assets/francis-galton/GaltonBoard.png" alt="GaltonBoard" /></p>

<p>像不像我们小时候在街巷的小卖部里玩过的弹珠机？没错，他们的背后是同样的原理。实际上，风靡全日本的柏青哥也是用的这样的设计原理：弹珠从顶部落下，经过跟若干层的撞针的撞击，最终掉进最下面从左到右N个桶当中的某个桶里。因为我们并不知道弹珠在跟每一根撞针撞击之后是走左还是走右，所以某个弹珠的最终位置并不能提前知道（i.e. 随机事件，随机漫步，随机过程）。</p>

<p>但！虽然单独一个弹珠的去向无法提前获知，但我们却有办法知道某个弹珠落入某个区间的概率。粗略来说，弹珠到达某一个桶的路线数量除以所有它可能走的路线，就是它进入某个桶的概率。比如，一颗弹珠想要到达最左边的区间，它只有一条路可以走：从第一层开始一直往左弹。算出其他区间的路线数和概率可以有很多方法，比如枚举（费劲）或用斐波那契数列（你也很能观察！），也可以根据Binomial distribution的probability mass function (pmf)得到（$n$是撞针的层数，$k$是桶的编号，$p$是弹珠撞击后弹左的概率）：</p>

<script type="math/tex; mode=display">\operatorname{Pr}(X=k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k}(1-p)^{n-k}</script>

<p>for $k = 0, \dots, n$.</p>

<p>读到这里，了解CLT的朋友或许已经明白为什么这个Galton board可以展示呈正态分布的偏差了。CLT的一个特殊应用是证明当试验的次数($n$)足够大的时候，binomial distribution的pmf会跟正态分布十分相似。换句话说，当我们的Galton board足够大，同时扔下的弹珠足够多的时候，我们应该就能看到经典的正态分布Bell curve！Genius!</p>

<center><iframe width="560" height="315" src="https://www.youtube.com/embed/jiWt77xme64" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>

<hr />
<p>为什么我们说这个弹珠机也能展示之前提到的回归谬误呢？首先，让我们把刚才那几百个弹珠落下来之后呈现出来的分布记在脑海中。让我再次使用做菜的例子，假设落到最右端的弹珠代表着我做出了迄今为止最好吃的一道菜，因为不寻常地走运（弹珠掉入最右边的几率非常小）。现在，我把这颗弹珠拿出来，让它从顶部再一次下落（再做一次同样的菜），你觉得大概率会是掉在哪片地方？有多大概率再次到达最右端（做出同样高水平的菜）？</p>

<p>呵，Life!</p>

<p>对于众多弹珠看似随机、无法预测地落下，最后被某种魔力聚拢，一个挨着一个，逐渐呈现出美丽的正态分布的现象，Galton自己是这样描述的：</p>
<blockquote>
  <p>Order in Apparent Chaos: I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the Law of Frequency of Error. The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.</p>
</blockquote>

<h3 id="结语">结语</h3>
<p>Francis Galton作为英国维多利亚时期的一位博学家，经历实在是太过丰富。自幼出生在富足精英的家庭，他是达尔文的表弟，年轻时继承了父亲的大笔遗产之后去非洲大陆探险，回国之后写成的游记成了畅销书。用他敏锐的观察力和好奇心，Galton研究了很多问题，有些没啥实际影响（最佳切蛋糕法、最佳沏茶法），有些却改变了众多领域接下来一百多年的发展。他做了早期的回归分析、提出了correlation的概念、将统计应用到遗传学、心理学，数理统计最重要的学者之一Karl Pearson是他的学生。同时，他为了得到数据，发明了问卷调查；研究天气，发明了第一张天气地图、开启了对气候的科学研究；提出了一种有效识别指纹的方法，对当时的法医学做了推动。哦，对了，正如我们开头所说，他也提出了一种根据不同人脸图像提取“平均特征”的方法。</p>

<p>Galton所观察到的世界，让他有了很多疑问，他尝试用各种方法去丈量这个世界，并从看似混沌无序的现象中找到秩序和规律。我惊叹于Galton的观察力、跟随自己好奇心不断的探索与尝试以及对自己专业不设限的态度。文艺复兴人的精神劲儿可见一斑。好了，不多说了，我要去入手一个Galton board了。</p>

<p>最后附上一个把Galton board解释得比我清楚得多、诙谐又幽默的哥们的<a href="https://www.youtube.com/embed/UCmPmkHqHXk">视频</a>。</p>

<h3 id="注释">注释</h3>

<div class="footnotes">
  <ol>
    <li id="fn:Galton_heights">
      <p>这里放上Galton自己制作的父母孩子身高回归图：<img src="/assets/francis-galton/Galton's_correlation_diagram_1875.jpg" alt="Galton_heights" /> <a href="#fnref:Galton_heights" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:correction">
      <p>后来的研究修正了Galton原稿的数据错误，当时那头牛的真实净体重应该是1197，而中位数估计应该是1208。在原稿中，Galton用中位数进行了真实值估计。不过，当时787个估计的平均数是1197。也就是说，平均数其实以零误差的表现估计到了真实值！ <a href="#fnref:correction" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/journal/2020/04/30/journal-2020-04.html">
        2020年4月：更难的方法与有套路的交流
      </a>
    </h2>

    <span class="post-date">30 Apr 2020</span>

    <p>这个月收到了稿子的消息，在尝试拼上最后一块问题的“拼图”，以及思考如何坐上司机的位置。</p>

    <!-- <p>这个月收到了稿子的消息，在尝试拼上最后一块问题的“拼图”，以及思考如何坐上司机的位置。</p>

<p>一转眼又两个星期没跟老板（博士导师）讨论研究上的问题了。</p>

<p>上周聊过关于回复审稿人意见的事，第一次投稿和收到修改意见，多少有点忐忑。学术文章发表的过程大致是投稿、编辑抄送给匿名审稿人、审稿人回复（推荐与否加上具体意见）、编辑回复（接受、修改、拒绝）。如果回复是修改，那可能会重复几轮这个步骤。可能是因为疫情的关系，去年十一月就投出去的文章，历时五个月，上上周才收到回复，并且只有两个审稿人。我打心里感谢这两位审稿人在这特殊时期还审我的稿，并且提的大多是寻求进一步解释或者论述性质的意见，而不是“质疑”型的问题。</p>

<p>每周周二，是我们往常组会和见面讨论的日子。自从疫情在新加坡扩散以来，学校先是规定会议要测体温登记时间等等，不久就要求全员在家工作了。全校人员撤离校园的同时，所有跟新冠病毒相关的研究活动得到特殊允许，可以继续开展。我们这一类不用做实验有电脑就能工作的研究，在家工作没太大影响；Science、土木一类需要学校资源做实验的朋友们就没那么幸运了，不知道他们现在在家里如何继续科研，刚好集中精力整理数据写写文章？</p>

<p>不知道是否是出于习惯，周二一上班就把最近困扰我的问题好好组织打稿一番发给了老板。这问题算是我目前工作的最后一块“拼图”吧。粗略来说，我模型里有个未知的参数要估计，但问题是线下（offline)还是线上(online)估计。线下估计需要一定数量的训练数据，也很难保证普遍性，万一有个没见过的情况，效果可能就不好；线上估计的适应性就强很多，也不那么依赖训练样本质量。但是，线上估计的方法我前段时间看了一下文献，没怎么看懂。当时急着想要看看整体方法的表现，姑且用线下学习估计了，初步结果看上去也不错。然而，这参数普遍性的问题在我尝试做大量仿真检验的时候不出意外地出现了！虽然心里知道这问题得解决，但真处理起这最后一块“拼图”时，我还是犹豫的。</p>

<p>问题发给了老板，很快回复了我：可以线上估计，你看看xxx（几个思想的关键词，方便我去查阅）。得到回复的时候没有失望，反而是安心了一些，跟自己说，踏踏实实做吧。</p>

<h3 id="一个学期只联系一次研一都是放养的吗">一个学期只联系一次，研一都是放养的吗？</h3>

<p>前两天偶然看到豆瓣有人问了如上的问题，他/她看上去很困惑，说希望能跟导师有更多的交流。蛮多人回复说这事儿得看老师。确实，每个老板指导风格不同会有不少区别，不过这只是公式的一半吧，自己作为另一半能做的事儿也很多。</p>

<p>我的老板也是中间偏“放养”型的，比较少会主动过问我进展如何，时不时会发点文章给我看，不过我想讨论的时候，也会认真指导。记得博一刚开始那会儿跟老板聊过关于学生跟博导的关系，他觉得读博做研究这事儿归根结底是我们自己的，学生应该把自己放到驱动者的角色；而导师的角色应该是一个advisor/mentor，会负责给评价、提建议、提供我需要的帮助，但并不负责帮我做事儿、监督我做事儿。对这，我是完全同意的，并且庆幸老板在一开始就跟我们说清楚、让我们有了合理的期待(manage expectations)。</p>

<p>我后来意识到，既然要坐到司机的位置上，那就意味着得学习怎么开车。不是说的学习如何做数学推导，而是学习如何做研究生，其中很重要的一项就是如何跟老板保持沟通（尤其是“放养”型）。有些学校会给新入学的研究生提供类似培训的课程，但不是所有学校都这样做。在摸索的过程中，同研究中心的德国博后同事推荐了本小册子给我：“<a href="/assets/month-journal/The 7 Secrets.pdf" target="_blank">The Seven Secrets of Highly Successful Research Students</a>”。我们研究中心有点特殊，中心很多博士都是在瑞士招了然后来新加坡读的，没办法用到瑞士那边大学里的众多资源，他们的博导们也大多base在瑞士。这也是大家比较关心“如何跟老板保持沟通”，“如何有效管理自己的研究进度”等问题的原因吧。小册子里面给出了蛮多诚恳的建议。其中关于如何跟老板保持沟通，交流工作的部分让我也受益匪浅。小册子推荐跟老板交流工作时用这样一个模板：</p>

<ol>
  <li>自从上次会议我做了：1..2..3..</li>
  <li>新遇到的问题：1..2..3..</li>
  <li>（老板的）评价与建议：1..2..3..</li>
  <li>接下来要做：1..2..3..</li>
</ol>

<p>这模板用起来特别容易上手。我是按这样的顺序使用的：根据上次会议老板提出的评价与建议去解决我新遇到的问题，然后去做“接下来要做”的事儿。在这过程中，开启新的一页，并记录新的第一和第二点，然后重复。用这模板跟老板做每周的讨论快一年了，跟以前开会前一天匆忙准备讨论的点相比，能明显感觉到现在自己在讨论时思路清晰很多，比较少因为老板的发散性提问而忘记原本想要讨论的点。另外一个改变是，当我能参照自己的记录，比较准确地讲出这问题的来龙去脉时，往往能得到比较清晰的建议与评价。这样记录下来的笔记，日后往回查看也方便很多，尤其是想要查找老板曾经给出的“自相矛盾”的建议的时候😃。</p>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/03/24/COVID-19.html">
        Tracking the COVID-19 outbreak and signals of containment
      </a>
    </h2>

    <span class="post-date">24 Mar 2020</span>

    <p><strong>Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.</strong> 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are <strong>interactive</strong> and best viewed on a desktop browser.</p>

    <!-- <p><strong>Any conclusion drawn from the data should be viewed with caution due to the dynamic nature of a pandemic and the adundant sources of bias associated with reporting.</strong> 
I periodically update here the COVID-19 situation in the US, Europe, and Asia, tracking both the outbreak and signals of containment. The intent of this blog is not to feed daily news, but to present perspectives worth considering when reading the news. The graphs in this blog are <strong>interactive</strong> and best viewed on a desktop browser.</p>

<h2 id="signals-of-containment">Signals of Containment</h2>

<p><a name="Confirmed and Death Cases"></a></p>
<h3 id="the-interplay-of-confirmed-and-death-cases">The Interplay of Confirmed and Death Cases</h3>
<p>When should the economy reopen? To try to answer this question, we could look at the interplay of new confirmed cases and death cases.</p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1739652220&amp;format=interactive"></iframe>
<p>More cases means more healthcare resource demand, and doctors and nurses have to make tough decisions. Unfortunately, more patients who need intensive care might not get it, leading to higher fatalities. We are probably going to see a peak in daily cases, and after some time, a peak in daily fatalities. This phenomenon is visible in the graph below. Passing the first peak means measures are taking effect; passing the second means our healthcare system is now able to cope. So, where do countries stand as of now?</p>

<p>Of course, the decision has to also depend on other factors such as the ability of testing and tracking down close contacts of those infected.</p>

<p>There are actually many questions that we could ask from this graph. For example:</p>
<ol>
  <li>Why does Germany has much higher daily confirms than Switzerland, and yet manages a much flatter death curve?</li>
  <li>Why do the two peaks for the UK seem to occur at the same time while that’s not the case for the rest?</li>
</ol>

<ul>
  <li>Countries with hopes of relaxing some of the lockdown measures: Germany and Switzerland. Both of them have passed the peaks, have low daily cases (&lt;20), and relatively flat and low death case curve (&lt;5).</li>
  <li>Countries that probably need more time: They are at the edge of passing the first peak and record about 80 daily cases. What’s more worrying, though, is the evident pressure on the healthcare system. UK sees a drop in daily death cases, but that number is still high at 11; the US’s death case curve seems not at its peak yet. They probably need more time. - April 23, 2020</li>
</ul>

<p><a name="Percentage Change"></a></p>
<h3 id="daily-case-percentage-change">Daily Case Percentage Change</h3>
<p>Look out for the 7-day moving average of the day-on-day percentage change in confirmed cases. It is important to see both the current percentage change and its trend. To easily classify the situation, we can use the following scale<sup id="fnref:percentage"><a href="#fn:percentage" class="footnote">1</a></sup>:</p>
<ul>
  <li><script type="math/tex">r > 10\%</script>: <strong>Rapidly increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 10\% %]]></script>: <strong>Increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 5\% %]]></script>: <strong>Slowly increasing</strong>.</li>
  <li><script type="math/tex">% <![CDATA[
r < 1\% %]]></script>: <strong>Under control</strong>.</li>
</ul>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=565833280&amp;format=interactive"></iframe>
<ul>
  <li>Japan had a turning point on 23rd March where the increase of cases started accelerating. Coincidently (or maybe not), Japan and I.O.C. officially anounced the <a href="https://www.nytimes.com/2020/03/24/sports/olympics/coronavirus-summer-olympics-postponed.html">postponement of Tokyo 2020</a> on the next day.</li>
  <li>The cases in Japan have been rising at an increasing rate, now at a 10% <a href="#Percentage Change">day-on-day growth rate</a>. Considering the exponential growth of infections, Abe, Japanese prime minister, is declaring emergency state for seven prefectures. - April 7, 2020</li>
  <li>Japan sees a slowdown of daily new cases. It’s been two weeks since the first declaration of “Emergency Situation” by the prime minister. On average, a 50% reduction in the number of people going out in monitored areas <a href="https://www3.nhk.or.jp/news/special/coronavirus/#infection-status">are observed</a>. Meanwhile, mask sales have skyrocketed in Japan. - April 22, 2020</li>
</ul>

<p><a name="Google Search Interest"></a></p>
<h3 id="google-search-interest">Google Search Interest</h3>
<p>This figure tells us how many people in the US are searching for keywords such as “hand sanitizer” or “symptom”. I suspect that as the community spread of the virus is being contained, we can expect to see a drop in searches for words like “symptom” and “influenza”, similar to the trends shown in Singapore.</p>

<p>There are drastic differences in terms of the US and Singapore google search interests during this pandemic. When signs of community infection emerged in early March, people in the US were searching for “symptom” at a record-high frequency, similarly for “influenza” and “hand sanitizer”. Searches for “mask”, however, were not so heightened. The picture in Singapore looks very different. When more infections emerged inside the border in late January and early February, the search for “mask” shoot up rapidly, and masks went out of stock everywhere in Singapore. There are probably two main reasons for this:</p>
<ol>
  <li>A high percentage of Chinese living in Singapore;</li>
  <li>As a nation that went through SARS, it feels natural for most people to wear masks when a contagion is spreading in the community.</li>
</ol>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=783455223&amp;format=interactive"></iframe>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=196247116&amp;format=interactive"></iframe>

<p><a name="US Testing Numbers"></a></p>
<h3 id="us-testing-numbers">US Testing Numbers</h3>
<p>As the containment takes effect, we expect to see the number of positive and negative tests stabilize, and the number of tests pending result drops. As you can see, we are not there yet.</p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=481777218&amp;format=interactive"></iframe>

<h2 id="cumulative-case-progression">Cumulative Case Progression</h2>
<hr />
<p><a name="Case progression"></a></p>
<iframe width="696.0000000000001" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=967719983&amp;format=interactive"></iframe>
<ul>
  <li>Japan has a relatively flat curve. However, there are legitimate concerns that Japan has been under-testing its population to know what is really going on. Assuming the true CFR is 1.2%<sup id="fnref:diamond_princess"><a href="#fn:diamond_princess" class="footnote">2</a></sup>, Japan’s current fatality number, 77, indicates that at least 6,417 people have been infected. However, only 3,139 cases are officially confirmed as of now. Also, Japan has conducted 486 tests <a href="#https://www.worldometers.info/coronavirus/">per one million population</a>. In Singapore, that number is 11,110. - April 5, 2020.</li>
  <li>For the first time, Singapore is going into a national “Shelter in Place” mode. The timeing is not surprising as some degree of wide-spread community infection is going on. The number of unlinked cases, those yet to find the source of infection, spiked over the last few days; Singapore also recorded 12 new clusters of infection just over the past five days (One of them is right across the river from my house). - April 5, 2020.</li>
  <li>Singapore sees a steady increase in confirmed cases, mainly in foreign worker dormitory clusters. However, if we look at the <a href="#Case progression">progression of confirmed cases</a> in Singapore, it’s an almost perfect example of what “flatten the curve” looks like. For the most part, the cases double every ten days, whereas cases in some of the worst-hit countries double every one to three days.  - April 8, 2020</li>
</ul>

<h2 id="death-cases">Death Cases</h2>
<hr />
<p><a name="case fatality rate"></a></p>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;format=interactive"></iframe>
<ul>
  <li>Germany and Switzerland fare well in this regard and manage to record comparatively low CFRs. Austria, too, has managed one of the lowerest CFRs among European nations. Austria, Germany, and a large part of Switzerland are German-speaking.🤔</li>
  <li>While the CFRs in Switzerland and Germany have been comparatively low, they are steadily climbing. Switzerland is probably the first country in Europe to flatten the curve, which conducted one of the highest number of tests <a href="#https://www.worldometers.info/coronavirus/">per one million population</a>. - April 10, 2020</li>
</ul>

<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;format=interactive"></iframe>

<p><a name="cfr bias"></a></p>
<ul>
  <li>How does selection bias affect CFR?
    <blockquote>
      <p>[In Italy], a change in strategy on Feb 25 limited testing to patients who had severe signs and symptoms also resulted in a 19% positive rate (21,157 of 109,170 tested as of Mar 14) and an apparent increase in the death rate—from 3.1% on Feb 24 to 7.2% on Mar 17—patients with milder illness were no longer tested.  In the UK, only patients deemed ill enough to require at least one night in hospital met the criteria for a Covid-19 test.</p>

      <p>CFR rates are subject to selection bias as more severe cases are tested, generally those in the hospital settings or those with more severe symptoms. The number of currently infected asymptomatics is uncertain: estimates put it at least a half are asymptomatic; the proportion not coming forward for testing is also highly doubtful (i.e. you are symptomatic, but you do not present for testing). Therefore we can assume the IFR is significantly lower than the CFR.</p>
    </blockquote>
  </li>
  <li>When is CFR accurate?
    <blockquote>
      <p>Iceland’s higher rates of testing, the smaller population, and their ability to ascertain all those with Sars-CoV-2  means they can obtain. an accurate estimate of the CFR and the infection fatality rate (IFR) during the pandemic (most countries will only be able to do this after the pandemic). Current data from Iceland suggests their IFR is somewhere between 0.01% and 0.19%.</p>
    </blockquote>
  </li>
</ul>

<p>The bottom line is, CFR is probably <strong>inflated</strong> in many countries and IFR is <strong>much lower</strong> than CFR.</p>

<h2 id="resources">Resources</h2>
<h4 id="websites">Websites</h4>
<ol>
  <li><strong>Bloomberg</strong>: <a href="https://www.bloomberg.com/graphics/2020-coronavirus-cases-world-map/?srnd=premium-asia">Mapping the Coronavirus Outbreak Across the World</a></li>
  <li><strong>Johns Hopkins University</strong>: <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">Coronavirus COVID-19 Global Cases</a></li>
  <li><strong>Global MediXchange</strong>: <a href="https://www.alibabacloud.com/universal-service/pdf_reader?spm=a3c0i.14138300.8102420620.dreadnow.646d647fDWbsii&amp;cdnorigin=video-intl&amp;pdf=Read%20Online-Handbook%20of%20COVID-19%20Prevention%20and%20Treatment.pdf">Handbook of COVID-19 Prevention and Treatment</a></li>
</ol>

<h4 id="articles">Articles</h4>
<ol>
  <li><a href="https://www.nytimes.com/2020/03/19/us/politics/trump-coronavirus-outbreak.html">Before Virus Outbreak, a Cascade of Warnings Went Unheeded</a>, March 19, 2020</li>
  <li><a href="https://www.citylab.com/life/2020/03/coronavirus-cases-france-train-hospital-tgv-covid-19-patient/608833/">To Fight a Fast-Moving Pandemic, Get a Faster Hospital</a>, March 26, 2020</li>
  <li><a href="https://www.businessinsider.sg/coronavirus-spain-says-rapid-tests-sent-from-china-missing-cases-2020-3?_ga=2.212074516.1285585527.1585620210-963085568.1583747541&amp;r=US&amp;IR=T">Spain, Europe’s worst-hit country after Italy, says coronavirus tests it bought from China are failing to detect positive cases</a>, March 26, 2020</li>
  <li><a href="https://time.com/5812555/germany-coronavirus-deaths/">Why Is Germany’s Coronavirus Death Rate So Low?</a>, March 30, 2020</li>
  <li><a href="https://www.nytimes.com/interactive/2020/04/14/science/coronavirus-transmission-cough-6-feet-ar-ul.html">This 3-D Simulation Shows Why Social Distancing Is So Important</a>, April 14, 2020</li>
</ol>

<h4 id="data-sources">Data sources</h4>
<ul>
  <li>Japan: <a href="https://www3.nhk.or.jp/news/special/coronavirus/#infection-status">NHK</a></li>
  <li>Singapore: <a href="https://www.moh.gov.sg/covid-19">Ministry of Health</a></li>
  <li>Other countries: JHU <a href="https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6">Coronavirus COVID-19 Global Cases</a></li>
  <li>US testing numbers: <a href="https://covidtracking.com/">The COIVD Tracking Project</a></li>
  <li>Search interests: <a href="https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=%2Fm%2F0b23px,%2Fm%2F01kr41,%2Fm%2F0cycc,%2Fm%2F01b_06">Google Trends</a></li>
</ul>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes">
  <ol>
    <li id="fn:percentage">
      <p>The percentage only indicates a relative change. The actual number of new cases reported in each country may be very different, as it depends on the absolute number of cumulative cases in that country. <a href="#fnref:percentage" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:diamond_princess">
      <p>Russell, Timothy W., et al. “<a href="https://www.medrxiv.org/content/10.1101/2020.03.05.20031773v2">Estimating the infection and case fatality ratio for COVID-19 using age-adjusted data from the outbreak on the Diamond Princess cruise ship.</a>” medRxiv (2020). <a href="#fnref:diamond_princess" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-statistical_learning_map.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools. 
<img src="/assets/ST-road-map/mindmap.png" alt="Mindmap" /></p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools. 
<img src="/assets/ST-road-map/mindmap.png" alt="Mindmap" /></p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-line-outage-detection-via-sequential-hypothesis-testing.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-line-outage-detection-based-on-particle-filtering.html">
        Roadmap of statistical learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is “local”? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute “average”? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisher’s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/data/2020/01/10/data-introduction-to-particle-filtering.html">
        Particle filtering: vallina and advanced
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

    <!-- <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

<p>Other methods - Kalman-based fiter?</p>
<ul>
  <li>limitations</li>
</ul>

<p>The method - Bayesian estimation through Monte Carlo method</p>
<ul>
  <li>Bayesian estimation</li>
  <li>Monte Carlo method</li>
</ul>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2019/11/12/how-to-read.html">
        读书笔记：读书体验是什么
      </a>
    </h2>

    <span class="post-date">12 Nov 2019</span>

    <p>我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。</p>

    <!-- <p>我有选书购书的习惯，也有逛书店花五分钟决定带回哪本书的时候；我时不时读书，但往往读完之后忘掉书里有什么精彩内容；我有在留白评论的习惯，也有不再翻看当时写下的评论、观点的习惯。家里未拆封的书越来越多，断断续续在读的书具体也说不出来哪里吸引我，读过的书好像被快速消费过一样没再露脸。坦白讲，我并不了解应该如何读书吧。幸运的是，奥野宣之在他的《如何有效阅读一本书》里提供了清晰的方法。</p>

<p style="display: block; margin-left: auto; margin-right: auto; width: 50%;"><img src="/assets/2019-11-12/book_cover.jpg" alt="cover" /></p>
<ul>
  <li>书名：<a href="https://book.douban.com/subject/26789567/">如何有效阅读一本书</a></li>
  <li>原名：読書は1冊のノートにまとめなさい</li>
  <li>作者：<a href="http://okuno0904.com/about/index.html">奥野宣之</a>（おくの・のぶゆき）</li>
  <li>购入日期：2019.11.11（打折！）</li>
</ul>

<h3 id="读书体验">读书体验</h3>
<p>这本149页的小书，我觉得可以看做是“如何读书”的一本参考书；里面介绍的观点和方法，不同级别的读者都可以在适当的时候读一读，并学到些东西。这本小书旨在回答这样一个问题：如何才能不忘记读过的书中的内容，并使之融入身心，真正使书籍影响自己？</p>

<p>而他的回答是，我们应该重新思考读书这件事儿。读书不应始于翻开书籍，也不终于最后一页。每一次读书，我们应该去创造属于自己的“读书体验”。要如何理解这个“体验”呢？我们可以想想生活中的其他体验，尤其是使用体验。我不经常买东西，但如果要买什么，会尽量去想清楚为什么要买，会提前去了解这个东西的背景，功能，评价，使用过程中会不断更新最初的设想，并持续影响我下一次购物的判断。这也就是一次有意识有目的能影响未来的购物体验。也就是说，作者推崇的是一种有目的能重温、甚至历久弥新的读书体验。实际上，作者也认为，这样的读书体验比书本身重要多了。</p>

<p>作者总结了下面五个具体可行的步骤来创造所谓的读书体验：</p>
<ol>
  <li><strong>选书</strong>：收集随想，建立目的</li>
  <li><strong>购书</strong>：冷静评估，书籍确认</li>
  <li><strong>读书</strong>：适当标记，提炼重点</li>
  <li><strong>记录</strong>：原文摘抄，原创思考</li>
  <li><strong>活用</strong>：重读笔记，思想输出</li>
</ol>

<p>而在作者的心中，创造这个读书体验必不可少的伙伴是一个普通的笔记本。因为它在上面五个步骤中所发挥的重要作用，这笔记本应时常伴我们左右，并且，如果直译本书的原书名，你会发现，它其实意思是“请用一个笔记本整理你的读书”。</p>

<h3 id="用购书清单指名购书">用购书清单指名购书</h3>
<p>创造读书体验的第一步，是给自己开出一个购书清单。开清单不是为了逛书店的时候容易找（也有这好处），更重要的是，让我们能清楚认识到读每一本书的目的是什么。作者是这么说的：</p>
<blockquote>
  <p>那么，为什么要把列清单的过程也作为读书方法的一部分来说明呢？理由之一，就是要培养带着目的去读书的目的意识。</p>
</blockquote>

<p>下面是作者推荐的选书购书的具体操作步骤：</p>

<blockquote>
  <p><strong>好奇心激发</strong> → <strong>随想笔记</strong> → <strong>购书清单</strong> → <strong>购书</strong></p>
</blockquote>

<p>第一步是将激发好奇心的源头记进笔记本里，可以叫做随想笔记。这源头的可能性就很多了：</p>
<ul>
  <li>报刊上读到有意思的书评</li>
  <li>听到感兴趣的政治时事评论</li>
  <li>来自朋友的书籍推荐</li>
  <li>等等</li>
</ul>

<p>只要是激起了我们好奇心的东西，都应该记进随想笔记里去。之后便能根据随想笔记，建立起读书的目的，并去寻找相关书籍。在经过冷静的评估之后，将想要购买的书籍列进清单。这流程的好处是，我们相对能够准确地选出自己真正想读并且明白为什么想读的书，这样买回来感兴趣、读下去的几率都比较高（这个很重要😂）。并且，在读书过程中，可以带着最初被激发的好奇心去读，读完之后也能回顾一开始好奇心被激发的契机，因为这些都被一一记录在笔记本里。</p>

<h3 id="用笔记把读过的书变为精神财富">用笔记把读过的书变为精神财富</h3>
<p>这里讲的包含了步骤三和四：读书，记录。</p>

<p>读书的时候，我们遵循这样一个流程：通读 → 片段重读 → 标记。意思就是，通读之后，去重读你感到有共鸣、疑惑、感兴趣等等的片段，有必要就用统一符号标记下来，比如：</p>
<ul>
  <li>下划直线 ＿＿：客观重要</li>
  <li>下划波浪线 ˷˷˷˷˷˷：我觉得重要，非常重要</li>
  <li>圆圈 ◯：关键词、专业名称</li>
</ul>

<p>这样读过一个章节、一本书，就能进入到下一个步骤：记录。这个时候就可以一一回顾上一步所标记出来的部分，参照下面的格式，在笔记本上写下读过这本书后的读书笔记：</p>
<ul>
  <li>⚬ …接原文摘抄、要点概括</li>
  <li>⭑ …接评论、感想</li>
  <li>重复上面</li>
</ul>

<p>具体来说，我们摘抄的时候，可以摘抄些什么呢？</p>
<ol>
  <li>能让我主观产生共鸣的</li>
  <li>不是读后觉得”理应如此“，而是“这么一说确实如此”的</li>
  <li>能颠覆我已有的想法、动摇我认识的</li>
  <li>等等</li>
</ol>

<p>摘抄或要点概括很大程度上是原作者的思想，而在评论里，我们应该尽量去挖掘些原创的思考。这会是个耗时间费精力的过程，不过只有试过的人才能知道到底值不值得。另外值得一提的是，作者还提倡将跟这本书有关的时间、空间印记也一起贴进笔记本里，比如说新书买来时的书腰、看那本书时所坐火车的票根等等。以后的某个时间，再次读起笔记本的这一页，看那本书时所经过的风景闻过的花香也会跃然纸上吧。</p>

<p>当然了，上面讲的记录的形式都是作者的推荐，我们大可不必居于某种形式，只需按照自己舒服的方式坚持记下属于自己的读书笔记就好了：</p>
<blockquote>
  <p>说句老生常谈的话，只有把读书笔记控制在自己能力允许的范围内，才能长久地坚持下去。所以，要选择对自己来说比较方便的笔记方法。</p>
</blockquote>

<p>我想这道理几乎适用于所有需要长久坚持的事，探究一门深奥的学问、学习一门外语、亦或是健身减肥等等。我发现人往往能轻松看到漫长过程之后的一种状态，这或许是我们这样高等生物的特殊能力；但人又往往忍不住会对期待的状态过于着急。这的确与我们身处的社会环境有所关系，但在我们的基因当中是否也有着这种既有远见又企求触手可得的回报的种子呢？</p>

<p>我在读这本书的时候所做的读书笔记就没有按照作者推荐的格式，而是采用了自己习惯的类似于PPT设计的风格：
<img src="/assets/2019-11-12/how_to_read.jpg" alt="how_to_read" /></p>

<p>不难看出，笔记里的结构几乎原封不动地变成了我这篇文章的结构，再加上在书里相关标记写下的评论，这篇文章的主要内容在我做完读书笔记的同时也就完成了。而这也是作者推崇的，以自己的读书笔记为基础，进一步写出原创文章，做属于自己的思想的输出。</p>

<h3 id="通过重读笔记提高自我">通过重读笔记提高自我</h3>
<p>读书体验的最后一步，也是我从没做过的一步：重读笔记。就像有人会偶尔重读日记一样，时常重读读书笔记能让自己读过的书好像一直存在自己生活中，不断酝酿，不断跟自己的经历、知识发生新的碰撞：</p>
<blockquote>
  <p>如果把一本书比作一个“场所”，那么读书笔记就是在这个场所拍摄的照片。在不同时间去同一个场所拍照，拍出来的照片都会有所不同，而过一段时间再去看这些照片，对那个场所的印象也会发生变化。</p>
</blockquote>

<p>关于如何重读，作者推荐：</p>
<ul>
  <li>简单回顾：读笔记</li>
  <li>细致回顾：读笔记 + 原书标记</li>
  <li>经典重温：读笔记 + 原书</li>
</ul>

<p>这么看，我确实一个回顾都没做过😂。一开始读这本书是因为买了新的笔记本，我老是买新笔记本，想着要写点什么文字，最后都沦为平时工作用的草稿纸（也是很重要啦）。看了这本书的评论觉得可能会帮我结束这个循环，目前看来很有希望。读之前，如何写读书笔记是我最感兴趣的部分，不过读完发现，以随想笔记到笔记回顾的一整个读书体验来理解读书这事儿，才是奥野宣之这本书给我最大的收获吧。</p>
 -->
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

    </div>

  </body>
</html>
