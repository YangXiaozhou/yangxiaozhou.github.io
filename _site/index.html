<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  
  <!-- include collecttags -->
  
  





  

  <title>
    
      Xiaozhou's Notes &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link href="https://fonts.googleapis.com/css?family=East+Sea+Dokdo&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.0/css/all.min.css" rel="stylesheet">




  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- MathJax -->
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  // Autonumbering by mathjax
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-89141653-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-89141653-4');
</script>



  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <!-- <a href="/"> -->
          Xiaozhou's Notes
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <!-- Manual set order -->
      <a class="sidebar-nav-item" href="/categories">Categories</a>
      <a class="sidebar-nav-item" href="/publication">Publication</a>
      <a class="sidebar-nav-item" href="/about">About</a>

      <!-- Uncomment for auto order -->
      <!-- 

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/categories/">Categories</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/publication/">Publication</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
       -->

      
      <!-- <a class="sidebar-nav-item" href="https://github.com/yangxiaozhou/yangxiaozhou.github.io">GitHub project</a> -->
      <!-- <span class="sidebar-nav-item">Currently v</span> -->
      
<div id="social-media">
    
    
        
        
            <a href="mailto:xiaozhou.yang@u.nus.edu" title="Email"><i class="fa fa-envelope"></i></a>
        
    
        
        
            <a href="https://www.linkedin.com/in/yangxiaozhou" title="Linkedin"><i class="fab fa-linkedin"></i></a>
        
    
        
        
            <a href="https://github.com/YangXiaozhou" title="GitHub"><i class="fab fa-github"></i></a>
        
    
        
        
            <a href="https://www.youtube.com/user/a315345751" title="YouTube"><i class="fab fa-youtube"></i></a>
        
    
</div>


    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h2 class="post-title">
      <a href="/2020/03/24/COVID-19.html">
        Dashboard: Tracking the Covid-19 Disease
      </a>
    </h2>

    <span class="post-date">24 Mar 2020</span>

    <p>Latest update time: 2020.03.24</p>

    <!-- <p>Latest update time: 2020.03.24</p>

<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=15610113&amp;format=interactive"></iframe>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=1124767386&amp;format=interactive"></iframe>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=709712852&amp;format=interactive"></iframe>
<iframe width="696" height="432" seamless="" frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQnqDux0h60qbKyhlYff1YMpjwhPZA694IOW4ixe0XNFi-JUHjXxJ69AFIGXajrBURwUKW2FnELgE1C/pubchart?oid=366153234&amp;format=interactive"></iframe>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/statistics/2020/01/10/statistical_learning_map.html">
        Roadmap of Statistical Learning
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

    <!-- <p>A (work-in-progress) roadmap for statistical learning concepts and tools.</p>

<p><strong>Regression</strong></p>
<ol>
  <li>Regularised Linear Regression
    <ul>
      <li>Ridge regression</li>
      <li>Lasso regression</li>
      <li>Logistic (ridge/lasso) regression</li>
    </ul>
  </li>
</ol>

<p>Next: however, linear relationship might be too restrictive in many cases, we wish to allow nonlinear relationship between response and predictors.</p>

<ol>
  <li>Basis Expansion Method
    <ul>
      <li>Approximate mean function in a piecewise way
        <ul>
          <li>kth order spline for kth order piece, i.e. cubic spline means each segment is approximated by a cubic function formed by basis functions</li>
          <li>Positive part remain operator (x - r)+ to ensure continuity at knots</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Additive Model (semi-parametric method, building on top of basis expansion method)
    <ul>
      <li>More flexible than linear but retains the interpretability
        <ul>
          <li>Partially linear additive model: summation of linear variables and functions of variable
            <ul>
              <li>Nonlinear variables can be assumed to be represented by spline basis</li>
              <li>What if a variable influences the relationship between Y and X?
                <ul>
                  <li>Varying coefficient regression model: variable coefficients are functions of a variable(Z)</li>
                  <li>Such function can be assumed to have good estimation by spline method</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Generalised: extend the method beyond linear link function, e.g. logistic</li>
      <li>If we approximate each additive variable by piecewise linear model: multivariate adaptive regression spline (MARS)</li>
    </ul>
  </li>
  <li>Local Averaging Method
    <ul>
      <li>To estimate the regression surface, we can use local averaging method by defining two things
        <ol>
          <li>What is â€œlocalâ€? i.e. how do we partition the space/find out the regression surface?</li>
          <li>How to compute â€œaverageâ€? i.e. simple average, weighted average?</li>
        </ol>
      </li>
      <li>Point 1 above
        <ul>
          <li>Binary recursive method: Regression and classification tree (CART)</li>
          <li>Neighbourhood method: identifies a neighbour through some metric (kNN)</li>
        </ul>
      </li>
      <li>Point 2 above
        <ul>
          <li>Simple average: CART, kNN</li>
          <li>Weighted average: weighted kNN, see point 7 below</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>But this kind of method produces non-smooth m(x) estimation since the weight used is indicator function.</p>

<ol>
  <li>Kernel Smoothing
    <ul>
      <li>Replace the indicator function by a kernel function (symmetric pdf)</li>
      <li>Nadaraya-Watson (NW) estimator: least square estimator weighted by kernel function</li>
      <li>The value of h defines the size of the neighbourhood</li>
      <li>Hence h determines the trade-off between model complexity and stability</li>
      <li>But y need not be locally constant (regression tree, kNN, KS), we can assume y is locally linear or polynomial
        <ul>
          <li>LLKS</li>
          <li>LPKS</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Next: But the MSE of KS (nonparametric) methods increases with p (CoD). We can do dimension reduction, besides assuming a structure for m(x).</li>
  <li>Dimension-reduction based method
    <ol>
      <li>Ridge function = 1: Single-index model: one projection direction, in terms of unknown link function
        <ul>
          <li>More flexible than linear regression, but also more interpretable than PPR</li>
        </ul>
      </li>
      <li>Ridge function &gt; 1: Projection pursuit regression: one projection direction for each ridge function
        <ul>
          <li>Approximate target by non-linear function of the linear combination of input</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Machine Learning
    <ul>
      <li>Without assuming the model of the data, learning a function that maps features (X) to predictions (Y), assume a space of the function (linear space, non-linear space), assume a convex loss/risk function (square error function).</li>
      <li>Representer theorem provides the theoretical foundation for functions from RKHS to approximate the relationship by assuming a certain kernel, which is defined by the kind of kernel.</li>
      <li>Support Vector Machine: learning the location of the support vectors and the value of alpha for the kernel of support vectors, i.e. linear kernel, gaussian kernel, rbf kernel.</li>
      <li>How is it different (performance, computation and etc.) for low and high dimensional case when we use non-linear kernel SVM?</li>
      <li>Classification
        <ul>
          <li>Fisherâ€™s linear discriminant: linear combination of dimensions of X -&gt; find a linear hyperplane that maximally separates the linear combination and minimises the variance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Classification</strong></p>
<ol>
  <li>Linear methods
    <ul>
      <li>Linear regression</li>
      <li>Linear discriminant analysis</li>
    </ul>
  </li>
  <li>Non-linear methods
    <ul>
      <li>SVM</li>
      <li>Discriminant analysis
        <ol>
          <li>QDA ( assume unequal variance)</li>
          <li>Flexible discriminant analysis</li>
          <li>Mixture discriminant analysis</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Bayesian Inference</strong></p>
<ol>
  <li>Inference for dynamic systems/state-space models(SSMs)
    <ul>
      <li>Finite SSM: Baum-Petrie filter is the optimal filter with O(K^2) complexity.</li>
      <li>Linear-Gaussian SSM: Kalman filter is the optimal filter, propagate mean and variance for inference.</li>
      <li>Non-linear dynamics and/or non-Gaussian noise:
        <ol>
          <li>Extended Kalman filter</li>
          <li>Unscented Kalman filter</li>
          <li>Sequential Monte Carlo (Particle filters) methods can be used since the distribution information is preserved beyond mean and covariance:
            <ol>
              <li>Importance sampling to tackle difficult-to-sample posterior distribution problem</li>
              <li>Recursive formulation to tackle online inference complexity problem</li>
              <li>Resampling to ensure long-term stability of the particle method (mitigates sample impoverishment)</li>
            </ol>
          </li>
          <li>Particle filtering
            <ol>
              <li></li>
            </ol>
          </li>
          <li>Particle smoothing</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/statistics/2020/01/10/particle-filter.html">
        Particle filters: vallina and advanced
      </a>
    </h2>

    <span class="post-date">10 Jan 2020</span>

    <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

    <!-- <p>The problem - Non-linear dynamic state estimation</p>
<ul>
  <li>state-space model for dynamic systems</li>
</ul>

<p>Other methods - Kalman-based fiter?</p>
<ul>
  <li>limitations</li>
</ul>

<p>The method - Bayesian estimation through Monte Carlo method</p>
<ul>
  <li>Bayesian estimation</li>
  <li>Monte Carlo method</li>
</ul>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/learning/2020/01/06/Japanese-n1-words.html">
        N1-ã‚­ã‚¯ã‚¿ãƒ³æ—¥æœ¬èª
      </a>
    </h2>

    <span class="post-date">06 Jan 2020</span>

    <h3 id="day-1">Day 1</h3>
<ol>
  <li>ã‚¢ãƒ«ãƒã‚¤ãƒˆã§å­¦æ¥­ï¼ˆãŒããã‚‡ã†ï¼‰ãŒãŠã‚ãã‹ã«ã«ãªã‚‹å¤§å­¦ç”Ÿã‚‚ã„ã‚‹ã€‚</li>
  <li>æ¥é€±ã®é‡‘æ›œæ—¥ã¾ã§ã«ã€ã“ã®èª²é¡Œï¼ˆã‹ã ã„ï¼‰ã‚’æå‡ºã—ã¦ãã ã•ã„ã€‚</li>
  <li>ç”°ä¸­ã•ã‚“ã®å­¦ä½ã¯ä¿®å£«ï¼ˆã—ã‚…ã†ã—ï¼‰ã§ã™ã€‚</li>
  <li>ä¸­å­¦å—é¨“ã®ãŸã‚ã€å­ã©ã‚‚ã‚’å¡¾ï¼ˆã˜ã‚…ãï¼‰ã«é€šã‚ã›ã‚‹ã“ã¨ã«ã—ãŸã€‚</li>
</ol>

    <!-- <h3 id="day-1">Day 1</h3>
<ol>
  <li>ã‚¢ãƒ«ãƒã‚¤ãƒˆã§å­¦æ¥­ï¼ˆãŒããã‚‡ã†ï¼‰ãŒãŠã‚ãã‹ã«ã«ãªã‚‹å¤§å­¦ç”Ÿã‚‚ã„ã‚‹ã€‚</li>
  <li>æ¥é€±ã®é‡‘æ›œæ—¥ã¾ã§ã«ã€ã“ã®èª²é¡Œï¼ˆã‹ã ã„ï¼‰ã‚’æå‡ºã—ã¦ãã ã•ã„ã€‚</li>
  <li>ç”°ä¸­ã•ã‚“ã®å­¦ä½ã¯ä¿®å£«ï¼ˆã—ã‚…ã†ã—ï¼‰ã§ã™ã€‚</li>
  <li>ä¸­å­¦å—é¨“ã®ãŸã‚ã€å­ã©ã‚‚ã‚’å¡¾ï¼ˆã˜ã‚…ãï¼‰ã«é€šã‚ã›ã‚‹ã“ã¨ã«ã—ãŸã€‚</li>
</ol>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2019/11/12/how-to-read.html">
        å¦‚ä½•è¯»ä¹¦ï¼šè¯»ä¹¦ä½“éªŒæ¯”ä¹¦æœ¬èº«é‡è¦å¤šäº†
      </a>
    </h2>

    <span class="post-date">12 Nov 2019</span>

    <p>æˆ‘æœ‰é€‰ä¹¦è´­ä¹¦çš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰é€›ä¹¦åº—èŠ±äº”åˆ†é’Ÿå†³å®šå¸¦å›å“ªæœ¬ä¹¦çš„ä¹ æƒ¯ï¼›æˆ‘æœ‰è¯»ä¹¦çš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰è¯»å®Œä¹‹åå¿˜æ‰ä¹¦é‡Œæœ‰ä»€ä¹ˆç²¾å½©å†…å®¹çš„ä¹ æƒ¯ï¼›æˆ‘æœ‰è¾¹è¯»è¾¹åœ¨ç•™ç™½è¯„è®ºçš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰ä¸å†ç¿»çœ‹è¯„è®ºã€å¯¹å½“æ—¶çš„è§‚ç‚¹ååˆåŠ å·¥çš„ä¹ æƒ¯ã€‚æ‰€ä»¥å®¶é‡Œæœªæ‹†å°çš„ä¹¦è¶Šæ¥è¶Šå¤šï¼Œæ–­æ–­ç»­ç»­åœ¨è¯»çš„ä¹¦å…·ä½“è¯´ä¸å‡ºæ¥å“ªé‡Œå¸å¼•æˆ‘ï¼Œè¯»è¿‡çš„ä¹¦ä¹Ÿå¥½åƒè¢«å¿«é€Ÿæ¶ˆè´¹è¿‡æ²¡èƒ½åœ¨æˆ‘çš„ç”Ÿæ´»ä¸­å†æ¬¡éœ²è„¸ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘å¹¶ä¸äº†è§£åº”è¯¥å¦‚ä½•è¯»ä¹¦ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¥¥é‡å®£ä¹‹åœ¨ä»–çš„ã€Šå¦‚ä½•æœ‰æ•ˆé˜…è¯»ä¸€æœ¬ä¹¦ã€‹é‡Œå‘Šè¯‰äº†æˆ‘ã€‚</p>

    <!-- <p>æˆ‘æœ‰é€‰ä¹¦è´­ä¹¦çš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰é€›ä¹¦åº—èŠ±äº”åˆ†é’Ÿå†³å®šå¸¦å›å“ªæœ¬ä¹¦çš„ä¹ æƒ¯ï¼›æˆ‘æœ‰è¯»ä¹¦çš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰è¯»å®Œä¹‹åå¿˜æ‰ä¹¦é‡Œæœ‰ä»€ä¹ˆç²¾å½©å†…å®¹çš„ä¹ æƒ¯ï¼›æˆ‘æœ‰è¾¹è¯»è¾¹åœ¨ç•™ç™½è¯„è®ºçš„ä¹ æƒ¯ï¼Œä¹Ÿæœ‰ä¸å†ç¿»çœ‹è¯„è®ºã€å¯¹å½“æ—¶çš„è§‚ç‚¹ååˆåŠ å·¥çš„ä¹ æƒ¯ã€‚æ‰€ä»¥å®¶é‡Œæœªæ‹†å°çš„ä¹¦è¶Šæ¥è¶Šå¤šï¼Œæ–­æ–­ç»­ç»­åœ¨è¯»çš„ä¹¦å…·ä½“è¯´ä¸å‡ºæ¥å“ªé‡Œå¸å¼•æˆ‘ï¼Œè¯»è¿‡çš„ä¹¦ä¹Ÿå¥½åƒè¢«å¿«é€Ÿæ¶ˆè´¹è¿‡æ²¡èƒ½åœ¨æˆ‘çš„ç”Ÿæ´»ä¸­å†æ¬¡éœ²è„¸ã€‚æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘å¹¶ä¸äº†è§£åº”è¯¥å¦‚ä½•è¯»ä¹¦ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¥¥é‡å®£ä¹‹åœ¨ä»–çš„ã€Šå¦‚ä½•æœ‰æ•ˆé˜…è¯»ä¸€æœ¬ä¹¦ã€‹é‡Œå‘Šè¯‰äº†æˆ‘ã€‚</p>

<ul>
  <li>ä¹¦åï¼š<a href="https://book.douban.com/subject/26789567/">å¦‚ä½•æœ‰æ•ˆé˜…è¯»ä¸€æœ¬ä¹¦</a></li>
  <li>åŸåï¼šèª­æ›¸ã¯1å†Šã®ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ãªã•ã„</li>
  <li>ä½œè€…ï¼š<a href="http://okuno0904.com/about/index.html">å¥¥é‡å®£ä¹‹</a>ï¼ˆãŠãã®ãƒ»ã®ã¶ã‚†ãï¼‰</li>
  <li>è´­å…¥æ—¥æœŸï¼š2019.11.11ï¼ˆæ‰“æŠ˜ï¼ï¼‰</li>
</ul>

<h3 id="è¯»ä¹¦ä½“éªŒ">è¯»ä¹¦ä½“éªŒ</h3>
<p>è¿™æœ¬149é¡µçš„å°ä¹¦ï¼Œæˆ‘è§‰å¾—å¯ä»¥çœ‹åšæ˜¯â€œå¦‚ä½•è¯»ä¹¦â€çš„ä¸€æœ¬å‚è€ƒä¹¦ï¼›é‡Œé¢ä»‹ç»çš„è§‚ç‚¹å’Œæ–¹æ³•ï¼Œä¸åŒçº§åˆ«çš„è¯»è€…éƒ½å¯ä»¥åœ¨é€‚å½“çš„æ—¶å€™è¯»ä¸€è¯»ï¼Œå¹¶å­¦åˆ°äº›ä¸œè¥¿ã€‚è¿™æœ¬å°ä¹¦æ—¨åœ¨å›ç­”è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•æ‰èƒ½ä¸å¿˜è®°è¯»è¿‡çš„ä¹¦ä¸­çš„å†…å®¹ï¼Œå¹¶ä½¿ä¹‹èå…¥èº«å¿ƒï¼ŒçœŸæ­£ä½¿ä¹¦ç±å½±å“è‡ªå·±ï¼Ÿ</p>

<p>è€Œä»–çš„å›ç­”æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥é‡æ–°æ€è€ƒè¯»ä¹¦è¿™ä»¶äº‹å„¿ã€‚è¯»ä¹¦ä¸åº”å§‹äºç¿»å¼€ä¹¦ç±ï¼Œä¹Ÿä¸ç»ˆäºæœ€åä¸€é¡µã€‚æ¯ä¸€æ¬¡è¯»ä¹¦ï¼Œæˆ‘ä»¬åº”è¯¥å»åˆ›é€ å±äºè‡ªå·±çš„â€œè¯»ä¹¦ä½“éªŒâ€ã€‚è¦å¦‚ä½•ç†è§£è¿™ä¸ªâ€œä½“éªŒâ€å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥æƒ³æƒ³ç”Ÿæ´»ä¸­çš„å…¶ä»–ä½“éªŒï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ä½“éªŒã€‚æˆ‘ä¸ç»å¸¸ä¹°ä¸œè¥¿ï¼Œä½†å¦‚æœè¦ä¹°ä»€ä¹ˆï¼Œä¼šå°½é‡å»æƒ³æ¸…æ¥šä¸ºä»€ä¹ˆè¦ä¹°ï¼Œä¼šæå‰å»äº†è§£è¿™ä¸ªä¸œè¥¿çš„èƒŒæ™¯ï¼ŒåŠŸèƒ½ï¼Œè¯„ä»·ï¼Œä½¿ç”¨è¿‡ç¨‹ä¸­ä¼šä¸æ–­æ›´æ–°æœ€åˆçš„è®¾æƒ³ï¼Œå¹¶æŒç»­å½±å“æˆ‘ä¸‹ä¸€æ¬¡è´­ç‰©çš„åˆ¤æ–­ã€‚è¿™ä¹Ÿå°±æ˜¯ä¸€æ¬¡æœ‰æ„è¯†æœ‰ç›®çš„èƒ½å½±å“æœªæ¥çš„è´­ç‰©ä½“éªŒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä½œè€…æ¨å´‡çš„æ˜¯ä¸€ç§æœ‰ç›®çš„èƒ½é‡æ¸©ã€ç”šè‡³å†ä¹…å¼¥æ–°çš„è¯»ä¹¦ä½“éªŒã€‚å®é™…ä¸Šï¼Œä½œè€…ä¹Ÿè®¤ä¸ºï¼Œè¿™æ ·çš„è¯»ä¹¦ä½“éªŒæ¯”ä¹¦æœ¬èº«é‡è¦å¤šäº†ã€‚</p>

<p>ä½œè€…æ€»ç»“äº†ä¸‹é¢äº”ä¸ªå…·ä½“å¯è¡Œçš„æ­¥éª¤æ¥åˆ›é€ æ‰€è°“çš„è¯»ä¹¦ä½“éªŒï¼š</p>
<ol>
  <li><strong>é€‰ä¹¦</strong>ï¼šæ”¶é›†éšæƒ³ï¼Œå»ºç«‹ç›®çš„</li>
  <li><strong>è´­ä¹¦</strong>ï¼šå†·é™è¯„ä¼°ï¼Œä¹¦ç±ç¡®è®¤</li>
  <li><strong>è¯»ä¹¦</strong>ï¼šé€‚å½“æ ‡è®°ï¼Œæç‚¼é‡ç‚¹</li>
  <li><strong>è®°å½•</strong>ï¼šåŸæ–‡æ‘˜æŠ„ï¼ŒåŸåˆ›æ€è€ƒ</li>
  <li><strong>æ´»ç”¨</strong>ï¼šé‡è¯»ç¬”è®°ï¼Œæ€æƒ³è¾“å‡º</li>
</ol>

<p>è€Œåœ¨ä½œè€…çš„å¿ƒä¸­ï¼Œåˆ›é€ è¿™ä¸ªè¯»ä¹¦ä½“éªŒå¿…ä¸å¯å°‘çš„ä¼™ä¼´æ˜¯ä¸€ä¸ªæ™®é€šçš„ç¬”è®°æœ¬ã€‚å› ä¸ºå®ƒåœ¨ä¸Šé¢äº”ä¸ªæ­¥éª¤ä¸­æ‰€å‘æŒ¥çš„é‡è¦ä½œç”¨ï¼Œè¿™ç¬”è®°æœ¬åº”æ—¶å¸¸ä¼´æˆ‘ä»¬å·¦å³ï¼Œå¹¶ä¸”ï¼Œå¦‚æœç›´è¯‘æœ¬ä¹¦çš„åŸä¹¦åï¼Œä½ ä¼šå‘ç°ï¼Œå®ƒå…¶å®æ„æ€æ˜¯â€œè¯·ç”¨ä¸€ä¸ªç¬”è®°æœ¬æ•´ç†ä½ çš„è¯»ä¹¦â€ã€‚</p>

<h3 id="ç”¨è´­ä¹¦æ¸…å•æŒ‡åè´­ä¹¦">ç”¨è´­ä¹¦æ¸…å•æŒ‡åè´­ä¹¦</h3>
<p>åˆ›é€ è¯»ä¹¦ä½“éªŒçš„ç¬¬ä¸€æ­¥ï¼Œæ˜¯ç»™è‡ªå·±å¼€å‡ºä¸€ä¸ªè´­ä¹¦æ¸…å•ã€‚å¼€æ¸…å•ä¸æ˜¯ä¸ºäº†é€›ä¹¦åº—çš„æ—¶å€™å®¹æ˜“æ‰¾ï¼ˆä¹Ÿæœ‰è¿™å¥½å¤„ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œè®©æˆ‘ä»¬èƒ½æ¸…æ¥šè®¤è¯†åˆ°è¯»æ¯ä¸€æœ¬ä¹¦çš„ç›®çš„æ˜¯ä»€ä¹ˆã€‚ä½œè€…æ˜¯è¿™ä¹ˆè¯´çš„ï¼š</p>
<blockquote>
  <p>é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¦æŠŠåˆ—æ¸…å•çš„è¿‡ç¨‹ä¹Ÿä½œä¸ºè¯»ä¹¦æ–¹æ³•çš„ä¸€éƒ¨åˆ†æ¥è¯´æ˜å‘¢ï¼Ÿç†ç”±ä¹‹ä¸€ï¼Œå°±æ˜¯è¦åŸ¹å…»å¸¦ç€ç›®çš„å»è¯»ä¹¦çš„ç›®çš„æ„è¯†ã€‚</p>
</blockquote>

<p>ä¸‹é¢æ˜¯ä½œè€…æ¨èçš„é€‰ä¹¦è´­ä¹¦çš„å…·ä½“æ“ä½œæ­¥éª¤ï¼š</p>

<blockquote>
  <p><strong>å¥½å¥‡å¿ƒæ¿€å‘</strong> â†’ <strong>éšæƒ³ç¬”è®°</strong> â†’ <strong>è´­ä¹¦æ¸…å•</strong> â†’ <strong>è´­ä¹¦</strong></p>
</blockquote>

<p>ç¬¬ä¸€æ­¥æ˜¯å°†æ¿€å‘å¥½å¥‡å¿ƒçš„æºå¤´è®°è¿›ç¬”è®°æœ¬é‡Œï¼Œå¯ä»¥å«åšéšæƒ³ç¬”è®°ã€‚è¿™æºå¤´çš„å¯èƒ½æ€§å°±å¾ˆå¤šäº†ï¼š</p>
<ul>
  <li>æŠ¥åˆŠä¸Šè¯»åˆ°æœ‰æ„æ€çš„ä¹¦è¯„</li>
  <li>å¬åˆ°æ„Ÿå…´è¶£çš„æ”¿æ²»æ—¶äº‹è¯„è®º</li>
  <li>æ¥è‡ªæœ‹å‹çš„ä¹¦ç±æ¨è</li>
  <li>ç­‰ç­‰</li>
</ul>

<p>åªè¦æ˜¯æ¿€èµ·äº†æˆ‘ä»¬å¥½å¥‡å¿ƒçš„ä¸œè¥¿ï¼Œéƒ½åº”è¯¥è®°è¿›éšæƒ³ç¬”è®°é‡Œå»ã€‚ä¹‹åä¾¿èƒ½æ ¹æ®éšæƒ³ç¬”è®°ï¼Œå»ºç«‹èµ·è¯»ä¹¦çš„ç›®çš„ï¼Œå¹¶å»å¯»æ‰¾ç›¸å…³ä¹¦ç±ã€‚åœ¨ç»è¿‡å†·é™çš„è¯„ä¼°ä¹‹åï¼Œå°†æƒ³è¦è´­ä¹°çš„ä¹¦ç±åˆ—è¿›æ¸…å•ã€‚è¿™æµç¨‹çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ç›¸å¯¹èƒ½å¤Ÿå‡†ç¡®åœ°é€‰å‡ºè‡ªå·±çœŸæ­£æƒ³è¯»å¹¶ä¸”æ˜ç™½ä¸ºä»€ä¹ˆæƒ³è¯»çš„ä¹¦ï¼Œè¿™æ ·ä¹°å›æ¥æ„Ÿå…´è¶£ã€è¯»ä¸‹å»çš„å‡ ç‡éƒ½æ¯”è¾ƒé«˜ï¼ˆè¿™ä¸ªå¾ˆé‡è¦ğŸ˜‚ï¼‰ã€‚å¹¶ä¸”ï¼Œåœ¨è¯»ä¹¦è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å¸¦ç€æœ€åˆè¢«æ¿€å‘çš„å¥½å¥‡å¿ƒå»è¯»ï¼Œè¯»å®Œä¹‹åä¹Ÿèƒ½å›é¡¾ä¸€å¼€å§‹å¥½å¥‡å¿ƒè¢«æ¿€å‘çš„å¥‘æœºï¼Œå› ä¸ºè¿™äº›éƒ½è¢«ä¸€ä¸€è®°å½•åœ¨ç¬”è®°æœ¬é‡Œã€‚</p>

<h3 id="ç”¨ç¬”è®°æŠŠè¯»è¿‡çš„ä¹¦å˜ä¸ºç²¾ç¥è´¢å¯Œ">ç”¨ç¬”è®°æŠŠè¯»è¿‡çš„ä¹¦å˜ä¸ºç²¾ç¥è´¢å¯Œ</h3>
<p>è¿™é‡Œè®²çš„åŒ…å«äº†æ­¥éª¤ä¸‰å’Œå››ï¼šè¯»ä¹¦ï¼Œè®°å½•ã€‚</p>

<p>è¯»ä¹¦çš„æ—¶å€™ï¼Œæˆ‘ä»¬éµå¾ªè¿™æ ·ä¸€ä¸ªæµç¨‹ï¼šé€šè¯» â†’ ç‰‡æ®µé‡è¯» â†’ æ ‡è®°ã€‚æ„æ€å°±æ˜¯ï¼Œé€šè¯»ä¹‹åï¼Œå»é‡è¯»ä½ æ„Ÿåˆ°æœ‰å…±é¸£ã€ç–‘æƒ‘ã€æ„Ÿå…´è¶£ç­‰ç­‰çš„ç‰‡æ®µï¼Œæœ‰å¿…è¦å°±ç”¨ç»Ÿä¸€ç¬¦å·æ ‡è®°ä¸‹æ¥ï¼Œæ¯”å¦‚ï¼š</p>
<ul>
  <li>ä¸‹åˆ’ç›´çº¿ ï¼¿ï¼¿ï¼šå®¢è§‚é‡è¦</li>
  <li>ä¸‹åˆ’æ³¢æµªçº¿ Ë·Ë·Ë·Ë·Ë·Ë·ï¼šæˆ‘è§‰å¾—é‡è¦ï¼Œéå¸¸é‡è¦</li>
  <li>åœ†åœˆ â—¯ï¼šå…³é”®è¯ã€ä¸“ä¸šåç§°</li>
</ul>

<p>è¿™æ ·è¯»è¿‡ä¸€ä¸ªç« èŠ‚ã€ä¸€æœ¬ä¹¦ï¼Œå°±èƒ½è¿›å…¥åˆ°ä¸‹ä¸€ä¸ªæ­¥éª¤ï¼šè®°å½•ã€‚è¿™ä¸ªæ—¶å€™å°±å¯ä»¥ä¸€ä¸€å›é¡¾ä¸Šä¸€æ­¥æ‰€æ ‡è®°å‡ºæ¥çš„éƒ¨åˆ†ï¼Œå‚ç…§ä¸‹é¢çš„æ ¼å¼ï¼Œåœ¨ç¬”è®°æœ¬ä¸Šå†™ä¸‹è¯»è¿‡è¿™æœ¬ä¹¦åçš„è¯»ä¹¦ç¬”è®°ï¼š</p>
<ul>
  <li>âš¬ â€¦æ¥åŸæ–‡æ‘˜æŠ„ã€è¦ç‚¹æ¦‚æ‹¬</li>
  <li>â­‘ â€¦æ¥è¯„è®ºã€æ„Ÿæƒ³</li>
  <li>é‡å¤ä¸Šé¢</li>
</ul>

<p>å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ‘˜æŠ„çš„æ—¶å€™ï¼Œå¯ä»¥æ‘˜æŠ„äº›ä»€ä¹ˆå‘¢ï¼Ÿ</p>
<ol>
  <li>èƒ½è®©æˆ‘ä¸»è§‚äº§ç”Ÿå…±é¸£çš„</li>
  <li>ä¸æ˜¯è¯»åè§‰å¾—â€ç†åº”å¦‚æ­¤â€œï¼Œè€Œæ˜¯â€œè¿™ä¹ˆä¸€è¯´ç¡®å®å¦‚æ­¤â€çš„</li>
  <li>èƒ½é¢ è¦†æˆ‘å·²æœ‰çš„æƒ³æ³•ã€åŠ¨æ‘‡æˆ‘è®¤è¯†çš„</li>
  <li>ç­‰ç­‰</li>
</ol>

<p>æ‘˜æŠ„æˆ–è¦ç‚¹æ¦‚æ‹¬å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯åŸä½œè€…çš„æ€æƒ³ï¼Œè€Œåœ¨è¯„è®ºé‡Œï¼Œæˆ‘ä»¬åº”è¯¥å°½é‡å»æŒ–æ˜äº›åŸåˆ›çš„æ€è€ƒã€‚è¿™ä¼šæ˜¯ä¸ªè€—æ—¶é—´è´¹ç²¾åŠ›çš„è¿‡ç¨‹ï¼Œä¸è¿‡åªæœ‰è¯•è¿‡çš„äººæ‰èƒ½çŸ¥é“åˆ°åº•å€¼ä¸å€¼å¾—ã€‚å¦å¤–å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä½œè€…è¿˜æå€¡å°†è·Ÿè¿™æœ¬ä¹¦æœ‰å…³çš„æ—¶é—´ã€ç©ºé—´å°è®°ä¹Ÿä¸€èµ·è´´è¿›ç¬”è®°æœ¬é‡Œï¼Œæ¯”å¦‚è¯´æ–°ä¹¦ä¹°æ¥æ—¶çš„ä¹¦è…°ã€çœ‹é‚£æœ¬ä¹¦æ—¶æ‰€åç«è½¦çš„ç¥¨æ ¹ç­‰ç­‰ã€‚ä»¥åçš„æŸä¸ªæ—¶é—´ï¼Œå†æ¬¡è¯»èµ·ç¬”è®°æœ¬çš„è¿™ä¸€é¡µï¼Œçœ‹é‚£æœ¬ä¹¦æ—¶æ‰€ç»è¿‡çš„é£æ™¯é—»è¿‡çš„èŠ±é¦™ä¹Ÿä¼šè·ƒç„¶çº¸ä¸Šå§ã€‚</p>

<p>å½“ç„¶äº†ï¼Œä¸Šé¢è®²çš„è®°å½•çš„å½¢å¼éƒ½æ˜¯ä½œè€…çš„æ¨èï¼Œæˆ‘ä»¬å¤§å¯ä¸å¿…å±…äºæŸç§å½¢å¼ï¼Œåªéœ€æŒ‰ç…§è‡ªå·±èˆ’æœçš„æ–¹å¼åšæŒè®°ä¸‹å±äºè‡ªå·±çš„è¯»ä¹¦ç¬”è®°å°±å¥½äº†ï¼š</p>
<blockquote>
  <p>è¯´å¥è€ç”Ÿå¸¸è°ˆçš„è¯ï¼Œåªæœ‰æŠŠè¯»ä¹¦ç¬”è®°æ§åˆ¶åœ¨è‡ªå·±èƒ½åŠ›å…è®¸çš„èŒƒå›´å†…ï¼Œæ‰èƒ½é•¿ä¹…åœ°åšæŒä¸‹å»ã€‚æ‰€ä»¥ï¼Œè¦é€‰æ‹©å¯¹è‡ªå·±æ¥è¯´æ¯”è¾ƒæ–¹ä¾¿çš„ç¬”è®°æ–¹æ³•ã€‚</p>
</blockquote>

<p>æˆ‘æƒ³è¿™é“ç†å‡ ä¹é€‚ç”¨äºæ‰€æœ‰éœ€è¦é•¿ä¹…åšæŒçš„äº‹ï¼Œæ¢ç©¶ä¸€é—¨æ·±å¥¥çš„å­¦é—®ã€å­¦ä¹ ä¸€é—¨å¤–è¯­ã€äº¦æˆ–æ˜¯å¥èº«å‡è‚¥ç­‰ç­‰ã€‚æˆ‘å‘ç°äººå¾€å¾€èƒ½è½»æ¾çœ‹åˆ°æ¼«é•¿è¿‡ç¨‹ä¹‹åçš„ä¸€ç§çŠ¶æ€ï¼Œè¿™æˆ–è®¸æ˜¯æˆ‘ä»¬è¿™æ ·é«˜ç­‰ç”Ÿç‰©çš„ç‰¹æ®Šèƒ½åŠ›ï¼›ä½†äººåˆå¾€å¾€å¿ä¸ä½ä¼šå¯¹æœŸå¾…çš„çŠ¶æ€è¿‡äºç€æ€¥ã€‚è¿™çš„ç¡®ä¸æˆ‘ä»¬èº«å¤„çš„ç¤¾ä¼šç¯å¢ƒæœ‰æ‰€å…³ç³»ï¼Œä½†åœ¨æˆ‘ä»¬çš„åŸºå› å½“ä¸­æ˜¯å¦ä¹Ÿæœ‰ç€è¿™ç§æ—¢æœ‰è¿œè§åˆä¼æ±‚è§¦æ‰‹å¯å¾—çš„å›æŠ¥çš„ç§å­å‘¢ï¼Ÿ</p>

<p>æˆ‘åœ¨è¯»è¿™æœ¬ä¹¦çš„æ—¶å€™æ‰€åšçš„è¯»ä¹¦ç¬”è®°å°±æ²¡æœ‰æŒ‰ç…§ä½œè€…æ¨èçš„æ ¼å¼ï¼Œè€Œæ˜¯é‡‡ç”¨äº†è‡ªå·±ä¹ æƒ¯çš„ç±»ä¼¼äºPPTè®¾è®¡çš„é£æ ¼ï¼š
<img src="/assets/2019-11-12/how_to_read.jpg" alt="how_to_read" />
ä¸éš¾çœ‹å‡ºï¼Œç¬”è®°é‡Œçš„ç»“æ„å‡ ä¹åŸå°ä¸åŠ¨åœ°å˜æˆäº†æˆ‘è¿™ç¯‡æ–‡ç« çš„ç»“æ„ï¼Œå†åŠ ä¸Šåœ¨ä¹¦é‡Œç›¸å…³æ ‡è®°å†™ä¸‹çš„è¯„è®ºï¼Œè¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹åœ¨æˆ‘åšå®Œè¯»ä¹¦ç¬”è®°çš„åŒæ—¶ä¹Ÿå°±å®Œæˆäº†ã€‚è€Œè¿™ä¹Ÿæ˜¯ä½œè€…æ¨å´‡çš„ï¼Œä»¥è‡ªå·±çš„è¯»ä¹¦ç¬”è®°ä¸ºåŸºç¡€ï¼Œè¿›ä¸€æ­¥å†™å‡ºåŸåˆ›æ–‡ç« ï¼Œåšå±äºè‡ªå·±çš„æ€æƒ³çš„è¾“å‡ºã€‚</p>

<h3 id="é€šè¿‡é‡è¯»ç¬”è®°æé«˜è‡ªæˆ‘">é€šè¿‡é‡è¯»ç¬”è®°æé«˜è‡ªæˆ‘</h3>
<p>è¯»ä¹¦ä½“éªŒçš„æœ€åä¸€æ­¥ï¼Œä¹Ÿæ˜¯æˆ‘ä»æ²¡åšè¿‡çš„ä¸€æ­¥ï¼šé‡è¯»ç¬”è®°ã€‚å°±åƒæœ‰äººä¼šå¶å°”é‡è¯»æ—¥è®°ä¸€æ ·ï¼Œæ—¶å¸¸é‡è¯»è¯»ä¹¦ç¬”è®°èƒ½è®©è‡ªå·±è¯»è¿‡çš„ä¹¦å¥½åƒä¸€ç›´å­˜åœ¨è‡ªå·±ç”Ÿæ´»ä¸­ï¼Œä¸æ–­é…é…¿ï¼Œä¸æ–­è·Ÿè‡ªå·±çš„ç»å†ã€çŸ¥è¯†å‘ç”Ÿæ–°çš„ç¢°æ’ï¼š</p>
<blockquote>
  <p>å¦‚æœæŠŠä¸€æœ¬ä¹¦æ¯”ä½œä¸€ä¸ªâ€œåœºæ‰€â€ï¼Œé‚£ä¹ˆè¯»ä¹¦ç¬”è®°å°±æ˜¯åœ¨è¿™ä¸ªåœºæ‰€æ‹æ‘„çš„ç…§ç‰‡ã€‚åœ¨ä¸åŒæ—¶é—´å»åŒä¸€ä¸ªåœºæ‰€æ‹ç…§ï¼Œæ‹å‡ºæ¥çš„ç…§ç‰‡éƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œè€Œè¿‡ä¸€æ®µæ—¶é—´å†å»çœ‹è¿™äº›ç…§ç‰‡ï¼Œå¯¹é‚£ä¸ªåœºæ‰€çš„å°è±¡ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ã€‚</p>
</blockquote>

<p>å…³äºå¦‚ä½•é‡è¯»ï¼Œä½œè€…æ¨èï¼š</p>
<ul>
  <li>ç®€å•å›é¡¾ï¼šè¯»ç¬”è®°</li>
  <li>ç»†è‡´å›é¡¾ï¼šè¯»ç¬”è®° + åŸä¹¦æ ‡è®°</li>
  <li>ç»å…¸é‡æ¸©ï¼šè¯»ç¬”è®° + åŸä¹¦</li>
</ul>

<p>è¿™ä¹ˆçœ‹ï¼Œæˆ‘ç¡®å®ä¸€ä¸ªå›é¡¾éƒ½æ²¡åšè¿‡ğŸ˜‚ã€‚ä¸€å¼€å§‹è¯»è¿™æœ¬ä¹¦æ˜¯å› ä¸ºä¹°äº†æ–°çš„ç¬”è®°æœ¬ï¼Œæˆ‘è€æ˜¯ä¹°æ–°ç¬”è®°æœ¬ï¼Œæƒ³ç€è¦å†™ç‚¹ä»€ä¹ˆæ–‡å­—ï¼Œæœ€åéƒ½æ²¦ä¸ºå¹³æ—¶å·¥ä½œç”¨çš„è‰ç¨¿çº¸ï¼ˆä¹Ÿæ˜¯å¾ˆé‡è¦å•¦ï¼‰ã€‚çœ‹äº†è¿™æœ¬ä¹¦çš„è¯„è®ºè§‰å¾—å¯èƒ½ä¼šå¸®æˆ‘ç»“æŸè¿™ä¸ªå¾ªç¯ï¼Œç›®å‰çœ‹æ¥å¾ˆæœ‰å¸Œæœ›ã€‚è¯»ä¹‹å‰ï¼Œå¦‚ä½•å†™è¯»ä¹¦ç¬”è®°æ˜¯æˆ‘æœ€æ„Ÿå…´è¶£çš„éƒ¨åˆ†ï¼Œä¸è¿‡è¯»å®Œå‘ç°ï¼Œä»¥éšæƒ³ç¬”è®°åˆ°ç¬”è®°å›é¡¾çš„ä¸€æ•´ä¸ªè¯»ä¹¦ä½“éªŒæ¥ç†è§£è¯»ä¹¦è¿™äº‹å„¿ï¼Œæ‰æ˜¯å¥¥é‡å®£ä¹‹è¿™æœ¬ä¹¦ç»™æˆ‘æœ€å¤§çš„æ”¶è·å§ã€‚</p>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/reading/2019/11/11/academic-deep-work.html">
        æ·±åº¦å·¥ä½œï¼šä¸€ä¸ªç ”ç©¶è€…ï¼ˆæˆ‘è‡ªå·±ï¼‰åº”è¯¥åšçš„äº‹
      </a>
    </h2>

    <span class="post-date">11 Nov 2019</span>

    <p>ä»€ä¹ˆæ˜¯æ·±åº¦å·¥ä½œï¼Ÿä¸ºä»€ä¹ˆå³æ—¶è®¯æ¯ä¸ä¼šæé«˜è€Œæ˜¯é™ä½æˆ‘çš„å·¥ä½œæ•ˆç‡ï¼Ÿä¸ºä»€ä¹ˆå¿™ç»¿å·¥ä½œäº†åä¸ªå°æ—¶å´å¹¶æ²¡å®Œæˆä»€ä¹ˆé‡è¦çš„äº‹æƒ…ï¼Ÿä¸ºä»€ä¹ˆæˆ‘è¦é€‰æ‹©é¼“åŠ±æ·±åº¦å·¥ä½œçš„å…¬å¸ï¼Ÿå¯¹äºè¿™äº›é—®é¢˜ï¼ŒNewportåœ¨ä»–çš„æ·±åº¦å·¥ä½œé‡Œåšå‡ºäº†å›ç­”ã€‚æˆ‘æƒ³åˆ©ç”¨è¿™ç¯‡æ–‡ç« æ¥æ€»ç»“ä¸€ä¸‹æ­¤ä¹¦ï¼Œè¯»åçš„æ„Ÿæƒ³ä»¥åŠèƒ½ä¸ºæˆ‘æ‰€ç”¨çš„çŸ¥è¯†ã€‚ååŠæ®µå°†ç»“åˆé©¬åçµå…ˆç”Ÿå»ºè®®çš„å­¦æœ¯æ±Ÿæ¹–çš„ä¸ƒç§æ­¦å™¨ï¼Œæç‚¼å‡ºå¯¹äºæˆ‘è¿™èˆ¬åˆšä¸Šè·¯çš„ç ”ç©¶è€…ï¼Œåº”è¯¥èŠ±è¶³å¤Ÿç²¾åŠ›å’Œæ—¶é—´å»åšçš„äº‹å„¿ï¼Œæ¢è¨€ä¹‹ï¼šä¸€ä¸ªç ”ç©¶è€…åº”è¯¥åšçš„æ·±åº¦å·¥ä½œã€‚</p>

    <!-- <p>ä»€ä¹ˆæ˜¯æ·±åº¦å·¥ä½œï¼Ÿä¸ºä»€ä¹ˆå³æ—¶è®¯æ¯ä¸ä¼šæé«˜è€Œæ˜¯é™ä½æˆ‘çš„å·¥ä½œæ•ˆç‡ï¼Ÿä¸ºä»€ä¹ˆå¿™ç»¿å·¥ä½œäº†åä¸ªå°æ—¶å´å¹¶æ²¡å®Œæˆä»€ä¹ˆé‡è¦çš„äº‹æƒ…ï¼Ÿä¸ºä»€ä¹ˆæˆ‘è¦é€‰æ‹©é¼“åŠ±æ·±åº¦å·¥ä½œçš„å…¬å¸ï¼Ÿå¯¹äºè¿™äº›é—®é¢˜ï¼ŒNewportåœ¨ä»–çš„æ·±åº¦å·¥ä½œé‡Œåšå‡ºäº†å›ç­”ã€‚æˆ‘æƒ³åˆ©ç”¨è¿™ç¯‡æ–‡ç« æ¥æ€»ç»“ä¸€ä¸‹æ­¤ä¹¦ï¼Œè¯»åçš„æ„Ÿæƒ³ä»¥åŠèƒ½ä¸ºæˆ‘æ‰€ç”¨çš„çŸ¥è¯†ã€‚ååŠæ®µå°†ç»“åˆé©¬åçµå…ˆç”Ÿå»ºè®®çš„å­¦æœ¯æ±Ÿæ¹–çš„ä¸ƒç§æ­¦å™¨ï¼Œæç‚¼å‡ºå¯¹äºæˆ‘è¿™èˆ¬åˆšä¸Šè·¯çš„ç ”ç©¶è€…ï¼Œåº”è¯¥èŠ±è¶³å¤Ÿç²¾åŠ›å’Œæ—¶é—´å»åšçš„äº‹å„¿ï¼Œæ¢è¨€ä¹‹ï¼šä¸€ä¸ªç ”ç©¶è€…åº”è¯¥åšçš„æ·±åº¦å·¥ä½œã€‚</p>

<h2 id="æ·±åº¦å·¥ä½œ">æ·±åº¦å·¥ä½œ</h2>
<p>Cal Newportçš„ã€Šæ·±åº¦å·¥ä½œã€‹<a href="https://www.calnewport.com/books/deep-work/">(Deep Work)</a></p>

<h4 id="ritualize-the-deep-work">Ritualize the deep work</h4>

<ol>
  <li>Where youâ€™ll work and for how long?
    <ul>
      <li>åˆ¶å®šå¥½æ·±åº¦å·¥ä½œçš„åœ°ç‚¹ä¸æ—¶é—´ï¼Œå°½æœ€å¤§å¯èƒ½ä¿æŒè¿™ä¸ªåœ°ç‚¹å’Œæ—¶é—´ã€‚å¦‚æœä¸­é—´æœ‰ä¼šè®®æˆ–è€…seminarä¹‹ç±»çš„ï¼Œæå‰è°ƒæ•´å¥½æ—¶é—´ï¼Œç®—ä½œæ˜¯æµ…å‹å·¥ä½œï¼Œæ€»çš„æ¥è¯´è¦ä¿æŒå¼¹æ€§ã€‚
        <ul>
          <li>Place of work: CREATE office; Duration: 9am - 5pm:
            <ul>
              <li>Morning deep work: 9am - 11am</li>
              <li>Lunch break: 11am - 1230pm</li>
              <li>Afternoon shallow work: 1230pm - 0130pm</li>
              <li>Nap: 0130pm - 2pm</li>
              <li>Afternoon deep work: 2pm - 5pm</li>
            </ul>
          </li>
          <li>Optional
            <ul>
              <li>Dinner break: 5pm - 6pm</li>
              <li>Night deep work: 6pm - 7pm</li>
              <li>Night shallow work/shutdown: 7pm - 8pm</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How youâ€™ll work once you start to work?
    <ul>
      <li>åˆ¶å®šå¥½æ·±åº¦å·¥ä½œæ—¶çš„è§„çŸ©ï¼Œå¿…é¡»éµå¾ªè¿™ä¸ªè§„çŸ©ã€‚è¿™ä¸ªè§„çŸ©åº”è¯¥å¯ä»¥å°½å¯èƒ½å®é™…ï¼Œä¹Ÿèƒ½ä¿è¯æ·±åº¦å·¥ä½œçš„æ•ˆæœã€‚</li>
      <li>æ·±åº¦å·¥ä½œæ—¶ï¼š
        <ul>
          <li>åšå†³ä¸ä½¿ç”¨æ‰‹æœºï¼›</li>
          <li>ç”µè„‘ä¸Šä¸æŸ¥emailï¼Œä¸æµè§ˆè·Ÿå·¥ä½œæ— å…³çš„ç½‘é¡µï¼Œä¸çœ‹è·Ÿå·¥ä½œæ— å…³çš„æ–‡ä»¶ã€‚</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How youâ€™ll support your work?
    <ul>
      <li>å°½å¯èƒ½ä»ç¯å¢ƒã€åƒå–ç­‰ç­‰æ–¹é¢å¸®åŠ©å®Œæˆä¸Šé¢çš„ritualã€‚</li>
      <li>å‡†å¤‡ï¼š
        <ul>
          <li>æ—©æ™¨æ·±åº¦å·¥ä½œå¼€å§‹ä¹‹å‰å–ä¸€æ¯å’–å•¡ï¼Œå¹¶å°†æ°´å£¶æ¥æ»¡æ°´ï¼›</li>
          <li>9ç‚¹å¼€å¯æ½®æ±ä¸“æ³¨æ¨¡å¼ï¼Œå¹¶å°†æ‰‹æœºæ”¾è¿›æŠ½å±‰ï¼›</li>
          <li>ä¸‹åˆ2ç‚¹å¼€å§‹æ½®æ±ä¸“æ³¨æ¨¡å¼ï¼Œå¹¶å°†æ‰‹æœºæ”¾è¿›æŠ½å±‰ã€‚</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h4 id="embrace-boredom">Embrace boredom</h4>

<ul>
  <li>This is to prevent our brains from being addicted to distractions. Resist the temptation to use handphone or surf the internet at the slightest hint of boredom.</li>
  <li>Memory training helps oneâ€™s ability to concentrate.</li>
</ul>

<h4 id="quit-social-media">Quit social media</h4>
<ul>
  <li>Put conscience effort in scheduling off-work hour activities. Our brain relaxes when we swtich focus, not by induldging in semi-conscious web surfing.
    <ul>
      <li>æ™šä¸Šçš„æ—¶å€™ä¸å›ºå®šå…·ä½“æ—¶é—´å’Œæ—¶é•¿ï¼Œä½†æ˜¯å°½é‡åšåˆ°ä¸‰ä»¶äº‹ï¼š
        <ul>
          <li>èƒŒN1å•è¯</li>
          <li>è¯»æ­£åœ¨è¯»çš„ä¹¦</li>
          <li>11ç‚¹ç¡åºŠä¸Š</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="å­¦æœ¯æ­¦å™¨">å­¦æœ¯æ­¦å™¨</h2>
<p><a href="https://www.douban.com/note/671893735/">é©¬åçµï¼šæˆ‘çš„æ€æƒ³å†ç¨‹ï¼ˆé™„ï¼šå­¦æœ¯æ±Ÿæ¹–çš„ä¸ƒç§æ­¦å™¨ï¼‰</a></p>

<p>é©¬åçµæ ¹æ®è‡ªå·±çš„ç»å†æ€»ç»“å‡ºäº†åšå­¦æœ¯éœ€è¦ä¿®ç‚¼çš„ä¸ƒå¤§â€œæ­¦å™¨â€ï¼Œè¿™ä¸ƒä¸ªæ–¹å‘åŒ…æ‹¬äº†å¯¹å­¦æœ¯äººçš„è¿½æ±‚çš„æ‹·é—®ï¼Œä¹Ÿæœ‰å¯¹åšå­¦æœ¯çš„å…·ä½“æ“ä½œçš„æŒ‡å¯¼ï¼Œä¸‹é¢åˆ—å‡ºä»–çš„ä¸ƒå¤§â€œæ­¦å™¨â€ä»¥åŠæˆ‘çš„ä¸€äº›æ€»ç»“å’Œæ€è€ƒã€‚</p>

<ul>
  <li>å­¦æœ¯æºæ³‰
    <ul>
      <li>æˆ‘ä¸ºä»€ä¹ˆåšå­¦æœ¯ï¼Ÿ</li>
      <li>ä»€ä¹ˆæ˜¯å­¦æœ¯ï¼Ÿ</li>
      <li>å­¦æœ¯çš„å‘å±•å†å²æ˜¯æ€æ ·çš„ï¼Ÿ</li>
    </ul>
  </li>
  <li>å­¦æœ¯é—®é¢˜
    <ul>
      <li>æˆ‘çš„å­¦æœ¯é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ</li>
    </ul>
  </li>
  <li>å­¦æœ¯æ¦‚å¿µ
    <ul>
      <li>æˆ‘å¯¹æˆ‘ä½¿ç”¨çš„æ¦‚å¿µçœŸçš„äº†è§£äº†å—ï¼Ÿ</li>
    </ul>
  </li>
  <li>å­¦æœ¯é˜…è¯»
    <ul>
      <li>æ¢³ç†é€»è¾‘æ¡†æ¶</li>
      <li>æŠŠæ¡æ–‡ç« è®ºè¿°é‡ç‚¹</li>
    </ul>
  </li>
  <li>å­¦æœ¯ç¬”è®°
    <ul>
      <li>è®°å½•å¹¶æ˜ç™½æ ¸å¿ƒæ¡†æ¶</li>
      <li>å»ºç«‹ä¸€ä¸ªæœ‰å±‚æ¬¡çš„æ–‡ç« æ€è·¯åœ°å›¾ï¼ˆi.e. Sections, subsections, sub-subsectionsï¼‰</li>
      <li>æ³¨æ˜æ–‡ç« çš„å‡ºç‰ˆä¿¡æ¯å’Œé¡µç </li>
      <li>å°è¯•å¯¹æ–‡ç« çš„æ¯ä¸€æ­¥è¿›è¡Œè´¨é—®</li>
      <li>æ€»ç»“æ–‡ç« çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œæ€è€ƒä¼˜ç‚¹å¦‚ä½•å€Ÿé‰´ï¼Œç¼ºç‚¹å¦‚ä½•é¿å…ä¸è§£å†³</li>
    </ul>
  </li>
  <li>å­¦æœ¯æ‰¹è¯„
    <ul>
      <li>å»ºè®¾æ€§æ‰¹è¯„</li>
      <li>ç›®çš„æ˜¯èƒ½å®Œå–„æ–‡ç« </li>
    </ul>
  </li>
  <li>å­¦æœ¯å†™ä½œ
    <ul>
      <li>é€»è¾‘æ¸…æ™°</li>
      <li>è§‚ç‚¹æ˜ç¡®</li>
      <li>åŒ–æ•´ä¸ºé›¶ï¼Œç„¶ååŒ–é›¶ä¸ºæ•´ï¼ˆi.e. paragraph to subsection, subsection to section, section to paperï¼‰</li>
    </ul>
  </li>
</ul>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/statistics/2019/10/03/beyond-lda.html">
        Beyond LDA: Flexible, penalized, and mixture discriminant analysis
      </a>
    </h2>

    <span class="post-date">03 Oct 2019</span>

    <p>This is the second article that talks about the classification and dimension reduction tool LDA.</p>

    <!-- <p>This is the second article that talks about the classification and dimension reduction tool LDA.</p>

<hr />
<h3 id="flexible-discriminant-analysis-fda">Flexible discriminant analysis (FDA)</h3>
<h3 id="penalized-discriminant-analysis-pda">Penalized discriminant analysis (PDA)</h3>
<h3 id="mixture-discriminant-analysis-mda">Mixture discriminant analysis (MDA)</h3>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/statistics/2019/10/02/linear-discriminant-analysis.html">
        LDA: Linear discriminant analysis in vanilla form
      </a>
    </h2>

    <span class="post-date">02 Oct 2019</span>

    <p>Linear discriminant analysis (LDA) is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces stable, effective, and interpretable classification results. Therefore, when tackling a classification problem, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed.</p>

    <!-- <p>Linear discriminant analysis (LDA) is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces stable, effective, and interpretable classification results. Therefore, when tackling a classification problem, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed.</p>

<p>Two prominent examples of using LDA (and itâ€™s variants) include:</p>
<ul>
  <li><em>Bankruptcy prediction</em>: Edward Altmanâ€™s <a href="https://en.wikipedia.org/wiki/Altman_Z-score">1968 model</a> predicts the probability of company bankruptcy using trained LDA coefficients. The accuracy is said to be between 80% and 90%, evaluated over 31 years of data.</li>
  <li><em>Facial recognition</em>: While features learned from Principal Component Analysis (PCA) are called Eigenfaces, those learned from LDA are called <a href="http://www.scholarpedia.org/article/Fisherfaces">Fisherfaces</a>, named after the statistician, Sir Ronald Fisher. We explain this connection later.</li>
</ul>

<p>This article starts by introducing the classic LDA and its reduced-rank version. Then we summarize the merits and disadvantages of LDA. The second article following this generalizes LDA to handle more complex problems. By the way, you can find a set of <a href="/assets/2019-10-02/Discriminant_Analysis.pdf" target="_blank">corresponding slides</a> where I present roughly the same materials written in this article.</p>

<h3 id="classification-by-discriminant-analysis">Classification by discriminant analysis</h3>
<p>Consider a generic classification problem: A random variable $X$ comes from one of $K$ classes, $G = 1, \dots, K$, with density $f_k(\mathbf{x})$ on $\mathbb{R}^p$. A discriminant rule divides the space into $K$ disjoint regions $\mathbb{R}_1, \dots, \mathbb{R}_K$. Classification by discriminant analysis simply means that we allocate $\mathbf{x }$ to $\Pi_{j}$ if $\mathbf{x} \in \mathbb{R}_j$. We can follow two allocation rules:</p>

<ul>
  <li><em>Maximum likelihood rule</em>: If we assume that each class could occur with equal probability, then allocate $\mathbf{x }$ to $\Pi_{j}$ if $j = \arg\max_i f_i(\mathbf{x})$ .</li>
  <li><em>Bayesian rule</em>: If we know the class prior probabilities, $\pi_1, \dots, \pi_K$, then allocate $\mathbf{x }$ to $\Pi_{j}$ if $j = \arg\max_i \pi_i f_i(\mathbf{x}) $.</li>
</ul>

<h4 id="linear-and-quadratic-discriminant-analysis">Linear and quadratic discriminant analysis</h4>
<p>If we assume data comes from multivariate Gaussian distribution, i.e. $X \sim N(\mathbf{\mu}, \mathbf{\Sigma})$, explicit forms of the above allocation rules can be obtained. Following the Bayesian rule, we classify $\mathbf{x}$ to $\Pi_{j}$ if $j = \arg\max_i \delta_i(\mathbf{x})$ where</p>

<script type="math/tex; mode=display">\begin{align}
    \delta_i(\mathbf{x}) = \log f_i(\mathbf{x}) + \log \pi_i
\end{align}</script>

<p>is called the discriminant function. Note the use of log-likelihood here.  The decision boundary separating any two classes, $k$ and $\ell$, is the set of $\mathbf{x}$ where two discriminant functions have the same value, i.e. <script type="math/tex">\{\mathbf{x}: \delta_k(\mathbf{x}) = \delta_{\ell}(\mathbf{x})\}</script>.</p>

<p>LDA arises in the case where we assume equal covariance among $K$ classes, i.e. $\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \dots = \mathbf{\Sigma}_K$. Then we can obtain the following discriminant function:</p>

<script type="math/tex; mode=display">\begin{align}
    \delta_{k}(\mathbf{x}) = \mathbf{x}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}-\frac{1}{2} \mathbf{\mu}_{k}^{T} \mathbf{\Sigma}^{-1} \mathbf{\mu}_{k}+\log \pi_{k} \,.
    \label{eqn_lda}
\end{align}</script>

<p>This is a linear function in $\mathbf{x}$. Thus, the decision boundary between any pair of classes is also a linear function in $\mathbf{x}$, the reason for its name: linear discriminant analysis. Without the equal covariance assumption, the quadratic term in the likelihood does not cancel out, hence the resulting discriminant function is a quadratic function in $\mathbf{x}$:
<script type="math/tex">\begin{align}
    \delta_{k}(\mathbf{x}) = 
    - \frac{1}{2} \log|\mathbf{\Sigma}_k| 
    - \frac{1}{2} (\mathbf{x} - \mathbf{\mu}_{k})^{T} \mathbf{\Sigma}_k^{-1} (\mathbf{x} - \mathbf{\mu}_{k}) + \log \pi_{k} \,.
    \label{eqn_qda}
\end{align}</script></p>

<p>Similarly, the decision boundary is quadratic in $\mathbf{x}$. This is known as quadratic discriminant analysis (QDA).</p>

<h4 id="number-of-parameters">Number of parameters</h4>
<p>In real problems, population parameters are usually unknown and estimated from training data as $\hat{\pi}_k, \hat{\mathbf{\mu}}_k, \hat{\mathbf{\Sigma}}_k$. While QDA accommodates more flexible decision boundaries compared to LDA, the number of parameters needed to be estimated also increases faster than that of LDA. From (\ref{eqn_lda}), $p+1$ parameters (nonlinear transformation of the original distribution parameters) are needed to construct the discriminant function. For a problem with $K$ classes, we would only need $K-1$ such discriminant functions by arbitrarily choosing one class to be the base class, i.e.</p>

<script type="math/tex; mode=display">\delta_{k}'(\mathbf{x}) = \delta_{k}(\mathbf{x}) - \delta_{K}(\mathbf{x})\,,</script>

<p>$k = 1, \dots, K-1$. Hence, the total number of estimated parameters for LDA is <script type="math/tex">(K-1)(p+1)</script>. On the other hand, for each QDA discriminant function (\ref{eqn_qda}), mean vector, covariance matrix, and class prior need to be estimated:</p>
<ul>
  <li>Mean: $p$</li>
  <li>Covariance: $p(p+1)/2$</li>
  <li>Class prior: 1</li>
</ul>

<p>The total number of estimated parameters for QDA is <script type="math/tex">(K-1)\{p(p+3)/2+1\}</script>. <em>Therefore, the number of parameters estimated in LDA increases linearly with $p$ while that of QDA increases quadratically with $p$.</em> We would expect QDA to have worse performance than LDA when the dimension $p$ is large.</p>

<h4 id="compromise-between-lda--qda">Compromise between LDA &amp; QDA</h4>
<p>We can find a compromise between LDA and QDA by regularizing the individual class covariance matrices. That is, individual covariance matrix shrinks toward a common pooled covariance matrix through a penalty parameter $\alpha$:</p>

<script type="math/tex; mode=display">\hat{\mathbf{\Sigma}}_k (\alpha) = \alpha \hat{\mathbf{\Sigma}}_k + (1-\alpha) \hat{\mathbf{\Sigma}} \,.</script>

<p>The pooled covariance matrix can also be regularized toward an identity matrix through a penalty parameter $\beta$:</p>

<script type="math/tex; mode=display">\hat{\mathbf{\Sigma}} (\beta) = \beta \hat{\mathbf{\Sigma}} + (1-\beta) \mathbf{I} \,.</script>

<p>In situations where the number of input variables greatly exceed the number of samples, covariance matrix can be poorly estimated. Shrinkage can hopefully improve the estimation and classification accuracy.<br />
<img src="/assets/2019-10-02/lda_shrinkage.png" alt="lda_shrinkage" /></p>
<details>
<summary>Click here for the script to generate the above plot, credit to <a href="https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html">scikit-learn</a>.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">n_train</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># samples for training
</span><span class="n">n_test</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># samples for testing
</span><span class="n">n_averages</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># how often to repeat classification
</span><span class="n">n_features_max</span> <span class="o">=</span> <span class="mi">75</span>  <span class="c1"># maximum number of features
</span><span class="n">step</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># step size for the calculation
</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
    <span class="s">"""Generate random blob-ish data with noisy features.

    This returns an array of input data with shape `(n_samples, n_features)`
    and an array of `n_samples` target labels.

    Only one feature contains discriminative information, the other features
    contain only noise.
    """</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>

    <span class="c1"># add non-discriminative features
</span>    <span class="k">if</span> <span class="n">n_features</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">acc_clf1</span><span class="p">,</span> <span class="n">acc_clf2</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">n_features_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n_features</span> <span class="ow">in</span> <span class="n">n_features_range</span><span class="p">:</span>
    <span class="n">score_clf1</span><span class="p">,</span> <span class="n">score_clf2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_averages</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>

        <span class="n">clf1</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s">'lsqr'</span><span class="p">,</span> <span class="n">shrinkage</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">clf2</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s">'lsqr'</span><span class="p">,</span> <span class="n">shrinkage</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
        <span class="n">score_clf1</span> <span class="o">+=</span> <span class="n">clf1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">score_clf2</span> <span class="o">+=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">acc_clf1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_clf1</span> <span class="o">/</span> <span class="n">n_averages</span><span class="p">)</span>
    <span class="n">acc_clf2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_clf2</span> <span class="o">/</span> <span class="n">n_averages</span><span class="p">)</span>

<span class="n">features_samples_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_features_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_train</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s">'seaborn-talk'</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_samples_ratio</span><span class="p">,</span> <span class="n">acc_clf1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s">"LDA with shrinkage"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'navy'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">features_samples_ratio</span><span class="p">,</span> <span class="n">acc_clf2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s">"LDA"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gold'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'n_features / n_samples'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Classification accuracy'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">18</span><span class="p">})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>
</details>

<h4 id="computation-for-lda">Computation for LDA</h4>
<p>We can see from (\ref{eqn_lda}) and (\ref{eqn_qda}) that computations of discriminant functions can be simplified if we diagonalize the covariance matrices first. That is, data are transformed to have an identity covariance matrices. In the case of LDA, hereâ€™s how we proceed with the computation:</p>

<ol>
  <li>Perform eigen-decompostion on the pooled covariance matrix: 
<script type="math/tex">\hat{\mathbf{\Sigma}} = \mathbf{U}\mathbf{D}\mathbf{U}^{T} \,.</script></li>
  <li>Sphere the data:
<script type="math/tex">\mathbf{X}^{*} \leftarrow \mathbf{D}^{-\frac{1}{2}} \mathbf{U}^{T} \mathbf{X} \,.</script></li>
  <li>Obtain class centroids in the transformed space: <script type="math/tex">\hat{\mu}_1, \dots, \hat{\mu}_{K}</script>.</li>
  <li>Classify $\mathbf{x}$ according to $\delta_{k}(\mathbf{x}^{*})$:</li>
</ol>

<script type="math/tex; mode=display">\begin{align}
\delta_{k}(\mathbf{x}^{*})=\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k} \,.
\label{eqn_lda_sphered}
\end{align}</script>

<p>Step 2 spheres the data to produce an identity covariance matrix in the transformed space. Step 4 is obtained by following (\ref{eqn_lda}). Letâ€™s take a two-class example to see what LDA is doing. Suppose there are two classes, $k$ and $\ell$. We classify $\mathbf{x}$ to class $k$ if <script type="math/tex">\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) > 0</script>. Following the four steps outlined above, we write</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\delta_{k}(\mathbf{x}^{*}) - \delta_{\ell}(\mathbf{x}^{*}) &= 
\mathbf{x^{*}}^{T} \hat{\mu}_{k}-\frac{1}{2} \hat{\mu}_{k}^{T} \hat{\mu}_{k}+\log \hat{\pi}_{k}
- \mathbf{x^{*}}^{T} \hat{\mu}_{\ell} + \frac{1}{2} \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell} - \log \hat{\pi}_{k} \\
&= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k}^{T}\hat{\mu}_{k} - \hat{\mu}_{\ell}^{T} \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&= \mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) + \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \\
&> 0 \,.
\end{align*} %]]></script>

<p>That is, we classify $\mathbf{x}$ to class $k$ if</p>

<script type="math/tex; mode=display">\mathbf{x^{*}}^{T} (\hat{\mu}_{k} - \hat{\mu}_{\ell}) > \frac{1}{2} (\hat{\mu}_{k} + \hat{\mu}_{\ell})^{T}(\hat{\mu}_{k} - \hat{\mu}_{\ell}) - \log \hat{\pi}_{k}/\hat{\pi}_{\ell} \,.</script>

<p>The derived allocation rule reveals the working of LDA. The left-hand side of the equation is the length of the orthogonal projection of <script type="math/tex">\mathbf{x^{*}}</script> onto the line segment joining the two class centroids. The right-hand side is the location of the center of the segment corrected by class prior probabilities. <em>Essentially, LDA classifies the data to the closest class centroid.</em> We make two observations here.</p>
<ol>
  <li>The decision point deviates from the middle point when the class prior probabilities are not the same, i.e., the boundary is pushed toward the class with a smaller prior probability.</li>
  <li>Data are projected onto the space spanned by class centroids, e.g. <script type="math/tex">\hat{\mu}_{k} - \hat{\mu}_{\ell}</script>. Distance comparisons are then done in that space.</li>
</ol>

<h3 id="reduced-rank-lda">Reduced-rank LDA</h3>
<p>What Iâ€™ve just described is the classification by LDA. LDA is also famous for its ability to find a small number of meaningful dimensions, allowing us to visualize high-dimensional problems. What do we mean by meaningful, and how does LDA find these dimensions? We will answer these questions shortly. First, take a look at the below plot. For a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine">wine classification</a> problem with three different types of wines and 13 input variables, the plot visualizes the data in two discriminant coordinates found by LDA. In this two-dimensional space, the classes can be well-separated. In comparison, the classes are not as clearly separated using the first two principal components found by PCA.</p>

<p><img src="/assets/2019-10-02/lda_vs_pca.png" alt="lda_vs_pca" /></p>
<details>
<summary>Click here for the script to generate the above plot.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">wine</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_wine</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">target</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">target_names</span>

<span class="n">X_r_lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_r_pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s">'seaborn-talk'</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'navy'</span><span class="p">,</span> <span class="s">'turquoise'</span><span class="p">,</span> <span class="s">'darkorange'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">color</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">colors</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_names</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_r_pca</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_r_pca</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s">'LDA for Wine dataset'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s">'PCA for Wine dataset'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 1'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 2'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'PC 1'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'PC 2'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>
</details>

<h4 id="inherent-dimension-reduction">Inherent dimension reduction</h4>
<p>In the above wine example, a 13-dimensional problem is visualized in a 2d space. Why is this possible? This is possible because thereâ€™s an inherent dimension reduction in LDA. We have observed from the previous section that LDA makes distance comparison in the space spanned by different class centroids. Two distinct points lie on a 1d line; three distinct points lie on a 2d plane. Similarly, $K$ class centroids lie on a hyperplane with dimension at most $(K-1)$. In particular, the subspace spanned by the centroids is</p>

<script type="math/tex; mode=display">H_{K-1}=\mu_{1} \oplus \operatorname{span}\left\{\mu_{i}-\mu_{1}, 2 \leq i \leq K\right\} \,.</script>

<p>When making distance comparisons, distances orthogonal to this subspace would add no information since they contribute equally for each class. Hence, by restricting distance comparisons to this subspace only would not lose any information useful for LDA classification. That means, we can safely transform our task from a $p$-dimensional problem to a $(K-1)$-dimensional problem by an orthogonal projection of the data onto this subspace. When $p \gg K$, this is a considerable drop in the number of dimensions. What if we want to reduce the dimension further from $p$ to $L$ where $K \gg L$? We can construct an $L$-dimensional subspace, $H_L$, from $H_{K-1}$, and this subspace is optimal, in some sense, to LDA classification.</p>

<h4 id="optimal-subspace-and-computation">Optimal subspace and computation</h4>
<p>Fisher proposes that the subspace $H_L$ is optimal when the class centroids of sphered data have maximum separation in this subspace in terms of variance. Following this definition, optimal subspace coordinates are simply found by doing PCA on sphered class centroids. The computation steps are summarized below:</p>
<ol>
  <li>
    <p>Find class centroid matrix, $\mathbf{M}_{(K\times p)}$, and pooled var-cov, <script type="math/tex">\mathbf{W}_{(p\times p)}</script>, where</p>

    <script type="math/tex; mode=display">\begin{align}
 \mathbf{W} = \sum_{k=1}^{K} \sum_{g_i = k} (\mathbf{x}_i - \hat{\mu}_k)(\mathbf{x}_i - \hat{\mu}_k)^T \,.
 \label{within_w}
 \end{align}</script>
  </li>
  <li>Sphere the centroids: $\mathbf{M}^* = \mathbf{M} \mathbf{W}^{-\frac{1}{2}}$, using eigen-decomposition of $\mathbf{W}$.</li>
  <li>
    <p>Compute <script type="math/tex">\mathbf{B}^* = \operatorname{cov}(\mathbf{M}^*)</script>, the between-class covariance of sphered class centroids by</p>

    <script type="math/tex; mode=display">\mathbf{B}^* = \sum_{k=1}^{K} (\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)(\hat{\mathbf{\mu}}^*_k - \hat{\mathbf{\mu}}^*)^T \,.</script>
  </li>
  <li>Obtain $L$ eigenvectors <script type="math/tex">(\mathbf{v}^*_\ell)</script> in <script type="math/tex">\mathbf{V}^*</script> of 
<script type="math/tex">\mathbf{B}^* = \mathbf{V}^* \mathbf{D_B} \mathbf{V^*}^T</script> cooresponding to the $L$ largest eigenvalues. These define the coordinates of the optimal subspace.</li>
  <li>Obtain $L$ new (discriminant) variables $Z_\ell = (\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell)^T X$, for $\ell = 1, \dots, L$.</li>
</ol>

<p>Through this procedure, we reduce our data dimension from <script type="math/tex">\mathbf{X}_{(N \times p)}</script> to <script type="math/tex">\mathbf{Z}_{(N \times L)}</script>. Discriminant coordinate 1 and 2 in the previous wine plot are found by setting $L = 2$. Repeating LDA procedures for classification using the new data, $\mathbf{Z}$, is called the reduced-rank LDA.</p>

<h3 id="fishers-lda">Fisherâ€™s LDA</h3>
<p>Fisher derived the computation steps according to his optimality definition in a different way<sup id="fnref:Fisher"><a href="#fn:Fisher" class="footnote">1</a></sup>. His steps of performing the reduced-rank LDA would later be known as the Fisherâ€™s LDA. Fisher does not make any assumption about the distribution of the populations, $\Pi_1, \dots, \Pi_K$. Instead, he tries to find a â€œsensibleâ€ rule so that the classification task becomes easier. In particular, Fisher finds a linear combination <script type="math/tex">Z = \mathbf{a}^T X</script> where the between-class variance, $\mathbf{B} = \operatorname{cov}(\mathbf{M})$, is maximized relative to the within-class variance, $\mathbf{W}$, as defined in (\ref{within_w}).</p>

<p>The below plot, taken from ESL<sup id="fnref:ESL"><a href="#fn:ESL" class="footnote">2</a></sup>, shows why this rule makes intuitive sense. The rule sets out to find a direction, $\mathbf{a}$, where, after projecting the data onto that direction, class centroids have maximum separation between them, and each class has minimum variance within them. The projection direction found under this rule, shown in the plot on the right, is much better.
<img src="/assets/2019-10-02/sensible_rule.png" alt="sensible_rule" /></p>

<h4 id="generalized-eigenvalue-problem">Generalized eigenvalue problem</h4>
<p>Finding the optimal direction(s) above amounts to solving an optimization problem:
<script type="math/tex">\max_{\mathbf{a}} (\mathbf{a}^{T} \mathbf{B} \mathbf{a})/(\mathbf{a}^{T} \mathbf{W} \mathbf{a}) \,,</script>
which is equivalent to</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\label{eqn_g_eigen}
\max_{\mathbf{a}} {}&{} \mathbf{a}^{T} \mathbf{B} \mathbf{a} \,,\\ 
\text{s.t. } &{} \mathbf{a}^{T} \mathbf{W} \mathbf{a} = 1 \,, \nonumber
\end{align} %]]></script>

<p>since the scaling of $\mathbf{a}$ does not affect the solution. Let $\mathbf{W}^{\frac12}$ be the symmetric square root of $\mathbf{W}$, and $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$. We can rewrite the problem (\ref{eqn_g_eigen}) as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\label{eqn_g_eigen_1}
\max_{\mathbf{y}} {}&{} \mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} \,,\\ 
\text{s.t } &{} \mathbf{y}^{T} \mathbf{y} = 1 \,. \nonumber
\end{align} %]]></script>

<p>Since $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$ is symmetric, we can find the spectral decomposition of it as</p>

<script type="math/tex; mode=display">\begin{align}
\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} = \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \,.
\label{eqn_fisher_eigen}
\end{align}</script>

<p>Let $\mathbf{z} = \mathbf{\Gamma}^T \mathbf{y}$. So $\mathbf{z}^T \mathbf{z} = \mathbf{y}^T \mathbf{\Gamma} \mathbf{\Gamma}^T \mathbf{y} = \mathbf{y}^T \mathbf{y}$, and</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbf{y}^{T} \mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12} \mathbf{y} &= \mathbf{y}^{T} \mathbf{\Gamma} \mathbf{\Lambda} \mathbf{\Gamma}^T \mathbf{y} \\
&= \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} \,.
\end{align*} %]]></script>

<p>Problem (\ref{eqn_g_eigen_1}) can then be written as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\label{eqn_g_eigen_2}
\max_{\mathbf{z}} {}&{} \mathbf{z}^T \mathbf{\Lambda} \mathbf{z} = \sum_i \lambda_i z_i^2 \,,\\ 
\text{s.t } &{} \mathbf{z}^{T} \mathbf{z} = 1 \,. \nonumber
\end{align} %]]></script>

<p>If the eigenvalues are written in descending order, then</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\max_{\mathbf{z}} \sum_i \lambda_i z_i^2 &\le \lambda_1 \sum_i z_i^2 \,,\\
&= \lambda_1 \,,
\end{align*} %]]></script>

<p>and the upper bound is attained at $\mathbf{z} = (1,0,0,\dots,0)^T$. Since $\mathbf{y} = \mathbf{\Gamma} \mathbf{z}$, the solution is <script type="math/tex">\mathbf{y} = \pmb \gamma_{(1)}</script>, the eigenvector corresponding to the largest eigenvalue in (\ref{eqn_fisher_eigen}). Since $\mathbf{y} = \mathbf{W}^{\frac12} \mathbf{a}$, the optimal projection direction is <script type="math/tex">\mathbf{a} = \mathbf{W}^{-\frac12} \pmb \gamma_{(1)}</script>.</p>

<p><strong>Theorem A.6.2</strong> from MA<sup id="fnref:MA"><a href="#fn:MA" class="footnote">3</a></sup>: For <script type="math/tex">\mathbf{A}_(n \times p)</script> and $\mathbf{B}_(p \times n)$, the non-zero eigenvalues of
$\mathbf{AB}$ and $\mathbf{BA}$ are the same and have the same multiplicity. If $\mathbf{x}$ is a non-trivial eigenvector of $\mathbf{AB}$ for an eigenvalue $\lambda \neq 0$, then $\mathbf{y}=\mathbf{Bx}$ is a non-trivial eigenvector of $\mathbf{BA}$.</p>

<p>Since <script type="math/tex">\pmb \gamma_{(1)}</script> is an eigenvector of $\mathbf{W}^{\frac12} \mathbf{B} \mathbf{W}^{\frac12}$, then, $\mathbf{W}^{-\frac12} \pmb \gamma_{(1)}$ is also the eigenvector of $\mathbf{W}^{-\frac12} \mathbf{W}^{-\frac12} \mathbf{B} = \mathbf{W}^{-1} \mathbf{B}$, using <strong>Theorem A.6.2</strong>.</p>

<p><em>In summary, optimal subspace coordinates, also known as discriminant coordinates, are obtained from eigenvectors <script type="math/tex">\mathbf{a}_\ell</script> of <script type="math/tex">\mathbf{W}^{-1}\mathbf{B}</script>, for <script type="math/tex">\ell = 1, ... , \min\{p,K-1\}</script>.</em> It can be shown that the <script type="math/tex">\mathbf{a}_\ell</script>s obtained are the same as <script type="math/tex">\mathbf{W}^{-\frac{1}{2}} \mathbf{v}^*_\ell</script>s obtained in the reduced-rank LDA formulation. Surprisingly, Fisher arrives at this formulation without any Gaussian assumption on the population, unlike the reduced-rank LDA formulation. The hope is that, with this sensible rule, LDA would perform well even when the data do not follow exactly the Gaussian distribution.</p>

<h2 id="handwritten-digits-problem">Handwritten digits problem</h2>
<p>Hereâ€™s an example to show the visualization and classification ability of Fisherâ€™s LDA, or simply LDA. We need to recognize ten different digits, i.e., 0 to 9, using 64 variables (pixel values from images). The dataset is taken from <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits">here</a>.</p>

<p>First, we can visualze the training images and they look like these: 
<img src="/assets/2019-10-02/digits.png" alt="digits" /></p>
<details>
<summary>Click here for the script.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span><span class="p">,</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="n">images_and_labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">))</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images_and_labels</span><span class="p">[:</span><span class="mi">4</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training: </span><span class="si">%</span><span class="s">i'</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>
</details>

<p>Next, we train an LDA classifier on the first half of the data. Solving the generalized eigenvalue problem mentioned previously gives us a list of optimal projection directions. In this problem, we keep the top 4 coordinates, and the transformed data are shown below. 
<img src="/assets/2019-10-02/reduced_lda_digits.png" alt="lda_vs_pca" /></p>
<details>
<summary>Click here for the script.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target_names</span>

<span class="c1"># Create a classifier: a Fisher's LDA classifier
</span><span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'eigen'</span><span class="p">,</span> <span class="n">shrinkage</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Train lda on the first half of the digits
</span><span class="n">lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">X_r_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Visualize transformed data on learnt discriminant coordinates
</span><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s">'seaborn-talk'</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="n">target_names</span><span class="p">):</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span>
                        <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'$</span><span class="si">%</span><span class="s">.f$'</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">X_r_lda</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span>
                        <span class="n">label</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'$</span><span class="si">%</span><span class="s">.f$'</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 1'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 2'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 3'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Discriminant Coordinate 4'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>
</details>

<p>The above plot allows us to interpret the trained LDA classifier. For example, coordinate 1 helps to contrast 4â€™s and 2/3â€™s while coordinate 2 contrasts 0â€™s and 1â€™s. Subsequently, coordinate 3 and 4 help to discriminate digits not well-separated in coordinate 1 and 2. We test the trained classifier using the other half of the dataset. The report below summarizes the result.</p>

<table>
  <thead>
    <tr>
      <th>Â </th>
      <th style="text-align: right">precision</th>
      <th style="text-align: right">recall</th>
      <th style="text-align: right">f1-score</th>
      <th style="text-align: right">support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td style="text-align: right">0.96</td>
      <td style="text-align: right">0.99</td>
      <td style="text-align: right">0.97</td>
      <td style="text-align: right">88</td>
    </tr>
    <tr>
      <td>1</td>
      <td style="text-align: right">0.94</td>
      <td style="text-align: right">0.85</td>
      <td style="text-align: right">0.89</td>
      <td style="text-align: right">91</td>
    </tr>
    <tr>
      <td>2</td>
      <td style="text-align: right">0.99</td>
      <td style="text-align: right">0.90</td>
      <td style="text-align: right">0.94</td>
      <td style="text-align: right">86</td>
    </tr>
    <tr>
      <td>3</td>
      <td style="text-align: right">0.91</td>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">0.93</td>
      <td style="text-align: right">91</td>
    </tr>
    <tr>
      <td>4</td>
      <td style="text-align: right">0.99</td>
      <td style="text-align: right">0.91</td>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">92</td>
    </tr>
    <tr>
      <td>5</td>
      <td style="text-align: right">0.92</td>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">0.93</td>
      <td style="text-align: right">91</td>
    </tr>
    <tr>
      <td>6</td>
      <td style="text-align: right">0.97</td>
      <td style="text-align: right">0.99</td>
      <td style="text-align: right">0.98</td>
      <td style="text-align: right">91</td>
    </tr>
    <tr>
      <td>7</td>
      <td style="text-align: right">0.98</td>
      <td style="text-align: right">0.96</td>
      <td style="text-align: right">0.97</td>
      <td style="text-align: right">89</td>
    </tr>
    <tr>
      <td>8</td>
      <td style="text-align: right">0.92</td>
      <td style="text-align: right">0.86</td>
      <td style="text-align: right">0.89</td>
      <td style="text-align: right">88</td>
    </tr>
    <tr>
      <td>9</td>
      <td style="text-align: right">0.77</td>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">0.85</td>
      <td style="text-align: right">92</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>avg / total</td>
      <td style="text-align: right">0.93</td>
      <td style="text-align: right">0.93</td>
      <td style="text-align: right">0.93</td>
      <td style="text-align: right">899</td>
    </tr>
  </tbody>
</table>

<details>
<summary>Click here for the script.</summary>
<div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Predict the value of the digit on the second half:
</span><span class="n">expected</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:])</span>

<span class="n">report</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Classification report:</span><span class="se">\n</span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="p">(</span><span class="n">report</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>
</details>

<p>The highest precision is 99%, and the lowest is 77%, a decent result knowing that the method was proposed some 70 years ago. Besides, we have not done anything to make the procedure better for this specific problem. For example, there is collinearity in the input variables, and the shrinkage parameter might not be optimal.</p>

<h2 id="summary-of-lda">Summary of LDA</h2>
<p>Here I summarize the virtues and shortcomings of LDA.</p>

<p>Virtues of LDA:</p>

<ol>
  <li>Simple prototype classifier: simple to interpret.</li>
  <li>Decision boundary is linear: simple to implement and robust.</li>
  <li>Dimension reduction: provides informative low-dimensional view on
data.</li>
</ol>

<p>Shortcomings of LDA:</p>

<ol>
  <li>Linear decision boundaries may not adequately separate the classes. Support for more general boundaries is desired.</li>
  <li>In a high-dimensional setting, LDA uses too many parameters. A regularized version of LDA is desired.</li>
  <li>Support for more complex prototype classification is desired.</li>
</ol>

<p>In the next article, flexible, penalized, and mixture discriminant analysis will be introduced to address each of the three shortcomings of LDA. With these generalizations, LDA can take on much more difficult and complex problems.</p>

<hr />
<h2 id="references">References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:Fisher">
      <p>Fisher, R. A. (1936). <em>The use of multiple measurements in taxonomic problems. Annals of eugenics</em>, 7(2), 179-188.Â <a href="#fnref:Fisher" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:ESL">
      <p>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2001). <em>The elements of statistical learning</em> (Vol. 1, No. 10). New York: Springer series in statistics.Â <a href="#fnref:ESL" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:MA">
      <p>Mardia, K. V., Kent, J. T., &amp; Bibby, J. M. <em>Multivariate analysis</em>. 1979. Probability and mathematical statistics. Academic Press Inc.Â <a href="#fnref:MA" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/code/2019/09/25/set-up-blog.html">
        How did I set up my blog using Jekyll, Hyde and GitHub
      </a>
    </h2>

    <span class="post-date">25 Sep 2019</span>

    <p>What would be a better way to start this blog than writing a post about how I set it up? Because after three days of sifting through all the documents, blogs, StackOverflow answers and GitHub issues, I finally realized that the process is not as straightforward as I thought it would be. Anyway, I got it to work (for now).</p>

    <!-- <p>What would be a better way to start this blog than writing a post about how I set it up? Because after three days of sifting through all the documents, blogs, StackOverflow answers and GitHub issues, I finally realized that the process is not as straightforward as I thought it would be. Anyway, I got it to work (for now).</p>

<p>My aim is simple, to set up a blog for myself where I can post stuff about my life. The blog needs to be free, elegant, intuitive and support math. My current set up, Jekyll + Hyde + Github + MathJax, matches with that. Since there are many resources online about setting up a blog using Jekyll and serve it with GitHub, I am going to skip all the standard procedures by referring to the official documents. Instead, this post specifically documents:</p>
<ul>
  <li>the sequence of setting up different parts,</li>
  <li>adding support for Tags, Categories and their corresponding pages,</li>
  <li>adding MathJax to support $\LaTeX$-like math.</li>
</ul>

<p>I am using a macbook, so the steps will be described assuming the system is macOS. When in doubt, just google the relevant steps for other OSs.</p>

<h2 id="1-set-up-jekyll">1. Set up Jekyll</h2>
<p>Jekyll is the package that is generating all your website pages. First thing you want to do is to make sure that <a href="https://jekyllrb.com">Jekyll</a> is installed and ready to run.</p>

<p>Follow the official <a href="https://jekyllrb.com/docs/">instructions</a>. If you successfully made a new site, good!</p>

<ol>
  <li>But if you ran into a <strong>failed to build native extension error</strong>, install macOS SDK headers with the following line.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg
</code></pre></div>    </div>
  </li>
  <li>If you ran into a <strong>file permission error</strong>, run the following lines to set GEM_HOME to your user directory.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'# Install Ruby Gems to ~/gems'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">echo</span> <span class="s1">'export GEM_HOME=$HOME/gems'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">echo</span> <span class="s1">'export PATH=$HOME/gems/bin:$PATH'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">source</span> ~/.bashrc
</code></pre></div>    </div>
  </li>
</ol>

<p>Now you can proceed with the following line and the rest of the steps.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>bundler jekyll
</code></pre></div></div>

<p>The problem is caused by the macOS Mojave update. The above solution is provided by <a href="https://talk.jekyllrb.com/t/issues-installing-jekyll-on-macos-mojave/2400/3">desiredpersona and Frank</a>. Make sure Jekyll can run normally before proceeding to step 2.</p>

<h2 id="2-set-up-github-repo">2. Set up GitHub repo</h2>
<p>Jekyll generates web pages locally; we need a GitHub repository to host our pages so that they can be accessed on the internet. For this part, setup can be done by following GitHubâ€™s official <a href="https://pages.github.com">instructions</a>. In the end, you should have a repo on GitHub called <em>username</em>.github.io, and the corresponding local folder on your computer. In my case, the name of my repo is yangxiaozhou.github.io.</p>

<p>By the end of Step 1 and 2, we have set up the local engine for generating web pages and the GitHub repo for hosting and publishing your pages. Now we proceed to the actual website construction.</p>

<h2 id="3-use-a-website-template">3. Use a website template</h2>
<p><a href="http://hyde.getpoole.com">Hyde</a> is a Jekyll website theme built on <a href="https://github.com/poole/poole">Poole</a>. They provide the template and the theme for the website. There are many themes for Jekyll, but I decided to use Hyde because I like the elegant design and itâ€™s easy to customize.</p>

<p>To get Hyde, just download <a href="https://github.com/poole/hyde">the repo</a> and move all the files into the local folder that you have just created in Step 2. Remember to clear any existing file in that folder before moving in Hyde files. From here, you just have to edit parts of those files to make the website yours (or use it as it is). I changed the following two lines in <code class="highlighter-rouge">_config.yml</code> since redcarpet and pygments are not supported anymore. Other variables can also be changed such as name, GitHub account, etc.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">markdown</span><span class="pi">:</span> <span class="s">kramdown</span>
<span class="na">highlighter</span><span class="pi">:</span> <span class="s">rouge</span>
</code></pre></div></div>
<p>At this point, it would be a good idea to learn some basics of <a href="https://jekyllrb.com/docs/">Jekyll</a>, e.g. what is a front matter, what is a page, how to create a layout, etc. After learning these, you can go ahead and customize the website as youâ€™d like.</p>

<p>One problem that I ran into is that pages look fine in local serve, but when I publish them to the web, all pages other than the home page have suddenly lost all their style elements. After searching through the internet, I realize that this has to do with the <code class="highlighter-rouge">url</code> and <code class="highlighter-rouge">baseurl</code> usage. If you also have this problem, consider doing the following:</p>
<ul>
  <li>change all the <code class="highlighter-rouge">{{ site.baseurl }}</code>
instances in <code class="highlighter-rouge">head.html</code> and <code class="highlighter-rouge">sidebar.html</code> to <code class="highlighter-rouge">{{ '/' | relative_url }}</code> so that the correct files can be located.</li>
</ul>

<h2 id="4-add-tags--categories">4. Add tags &amp; categories</h2>
<p>I want to add tags and categories to my posts and create a dedicated page where posts can be arranged according to <a href="https://yangxiaozhou.github.io/tag/supervised-learning">tags</a>/<a href="https://yangxiaozhou.github.io/categories/">categories</a>. This should be easy since tags and categories are default front matter variables that you can define in Jekyll. For example, tags and categories of my LDA post are defined like this:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">layout</span><span class="pi">:</span> <span class="s">post</span>
<span class="na">mathjax</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">title</span><span class="pi">:</span>  <span class="s2">"</span><span class="s">Linear</span><span class="nv"> </span><span class="s">discriminant</span><span class="nv"> </span><span class="s">analysis</span><span class="nv"> </span><span class="s">(LDA)</span><span class="nv"> </span><span class="s">and</span><span class="nv"> </span><span class="s">beyond"</span>
<span class="na">date</span><span class="pi">:</span>   <span class="s">2019-09-26 08:00:00 +0800</span>
<span class="na">categories</span><span class="pi">:</span> <span class="s">statistics</span>
<span class="na">tags</span><span class="pi">:</span> <span class="s">supervised-learning classification</span>
<span class="nn">---</span>
</code></pre></div></div>

<p>For <strong>categories</strong>, I created one page where posts of different categories are collected and the page is accessible through the sidebar link. To do this, just create a <code class="highlighter-rouge">category.html</code> in the root folder:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
layout: page
permalink: /categories/
title: Categories
---

<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"archives"</span><span class="nt">&gt;</span>
{% for category in site.categories %}
  <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"archive-group"</span><span class="nt">&gt;</span>
    {% capture category_name %}{{ category | first }}{% endcapture %}
    <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"#{{ category_name | slugize }}"</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;p&gt;&lt;/p&gt;</span>

    <span class="nt">&lt;h3</span> <span class="na">class=</span><span class="s">"category-head"</span><span class="nt">&gt;</span>{{ category_name }}<span class="nt">&lt;/h3&gt;</span>
    <span class="nt">&lt;a</span> <span class="na">name=</span><span class="s">"{{ category_name | slugize }}"</span><span class="nt">&gt;&lt;/a&gt;</span>
    {% for post in site.categories[category_name] %}
    <span class="nt">&lt;article</span> <span class="na">class=</span><span class="s">"archive-item"</span><span class="nt">&gt;</span>
      <span class="nt">&lt;h4&gt;&lt;a</span> <span class="na">href=</span><span class="s">"{{ site.baseurl }}{{ post.url }}"</span><span class="nt">&gt;</span>{{post.title}}<span class="nt">&lt;/a&gt;&lt;/h4&gt;</span>
    <span class="nt">&lt;/article&gt;</span>
    {% endfor %}
  <span class="nt">&lt;/div&gt;</span>
{% endfor %}
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div>

<p>For <strong>tags</strong>, I did two things:</p>
<ol>
  <li>Show the tags of a post at the end of the content.</li>
  <li>For every tag, create a page where posts are collected, i.e. <a href="https://yangxiaozhou.github.io/tag/classification">classification</a>, <a href="https://yangxiaozhou.github.io/tag/supervised-learning">supervised-learning</a>.</li>
</ol>

<p>To do 1, include the following lines after the <code class="highlighter-rouge">content</code> section in your <code class="highlighter-rouge">post.html</code>:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;span</span> <span class="na">class=</span><span class="s">"post-tags"</span><span class="nt">&gt;</span>
    {% for tag in page.tags %}
      {% capture tag_name %}{{ tag }}{% endcapture %}
      <span class="nt">&lt;a</span> <span class="na">class=</span><span class="s">"no-underline"</span> <span class="na">href=</span><span class="s">"/tag/{{ tag_name }}"</span><span class="nt">&gt;&lt;code</span> <span class="na">class=</span><span class="s">"highligher-rouge"</span><span class="nt">&gt;&lt;nobr&gt;</span>{{ tag_name }}<span class="nt">&lt;/nobr&gt;&lt;/code&gt;</span><span class="ni">&amp;nbsp;</span><span class="nt">&lt;/a&gt;</span>    
    {% endfor %}
<span class="nt">&lt;/span&gt;</span>
</code></pre></div></div>

<p>To do 2, Long Qian has written a very clear <a href="https://longqian.me/2017/02/09/github-jekyll-tag/">post</a> about it.</p>

<h2 id="5-add-mathjax">5. Add MathJax</h2>
<p>The last piece to my website is to add the support of $\LaTeX$-like math. This is done through MathJax. There are two steps to achieve it:</p>

<ol>
  <li>Create a <code class="highlighter-rouge">mathjax.html</code> file and put it in your <code class="highlighter-rouge">_includes</code> folder. Download the file <a href="https://github.com/YangXiaozhou/yangxiaozhou.github.io/blob/master/_includes/mathjax.html">here</a>.</li>
  <li>
    <p>Put the following line before <code class="highlighter-rouge">&lt;/head&gt;</code> in your <code class="highlighter-rouge">head.html</code>:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> {% include mathjax.html %}
</code></pre></div>    </div>

    <p>to enbale MathJax on the page.</p>
  </li>
</ol>

<h4 id="tips">Tips</h4>
<ul>
  <li>To use the normal dollar sign instead of the MathJax command (escape), put <code class="highlighter-rouge">&lt;span class="tex2jax_ignore"&gt;...&lt;/span&gt;</code> around the text you donâ€™t want MathJax to process.</li>
  <li>Check out currently supported Tex/LaTeX commands by MathJax <a href="https://docs.mathjax.org/en/latest/input/tex/macros/index.html">here</a>.</li>
</ul>

<h4 id="math-rendering-showcase">Math Rendering Showcase</h4>
<ul>
  <li>Inline math using <code class="highlighter-rouge">\$...\$</code>: $\mathbf{x}+\mathbf{y}$.</li>
  <li>Displayed math using <code class="highlighter-rouge">\$\$...\$\$</code> on a new paragraph:</li>
</ul>

<script type="math/tex; mode=display">\mathbf{x}+\mathbf{y} \,.</script>

<ul>
  <li>
    <p>Automatic numbering and referencing using <span class="tex2jax_ignore"><code class="highlighter-rouge">\ref{label}</code></span>:
In (\ref{eq:sample}), we find the value of an interesting integral:
\begin{align}
\int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15} \, .
\label{eq:sample}
\end{align}</p>
  </li>
  <li>
    <p>Multiline equations using <code class="highlighter-rouge">\begin{align*}</code>:
\begin{align*}
\nabla \times \vec{\mathbf{B}} -\, \frac1c\, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \,,\newline
\nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \,.
\end{align*}</p>
  </li>
</ul>

<p>Thatâ€™s it for now. Happy blogging.</p>

<p>Additional resources:</p>
<ul>
  <li>Set up <a href="https://blog.webjeda.com/jekyll-categories/">categories &amp; tags</a></li>
  <li>Set up <a href="http://joshualande.com/jekyll-github-pages-poole">Disqus comments &amp; Google Analytics</a></li>
  <li>Add in social media <a href="https://jreel.github.io/social-media-icons-on-jekyll/">icons</a></li>
  <li>kramdown <a href="https://kramdown.gettalong.org/quickref.html">basics</a></li>
  <li>MathJax <a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">basics</a></li>
</ul>

 -->
  </div>
  
  <div class="post">
    <h2 class="post-title">
      <a href="/learning/2019/01/01/recipe.html">
        èœè°±
      </a>
    </h2>

    <span class="post-date">01 Jan 2019</span>

    <p>åšè¿‡çš„å¥½åƒçš„ä¸œè¥¿ï¼Œä¸ºäº†ä¸å¿˜è®°æ€ä¹ˆåšï¼ŒæŠŠèœè°±éƒ½æ”¶é›†èµ·æ¥æ”¾åœ¨è¿™å„¿ã€‚åªè¦æ²¡æœ‰å•ç‹¬è¯´çš„ï¼Œä¸€å¾‹æŒ‰é€‚é‡åŸåˆ™ã€‚</p>

    <!-- <p>åšè¿‡çš„å¥½åƒçš„ä¸œè¥¿ï¼Œä¸ºäº†ä¸å¿˜è®°æ€ä¹ˆåšï¼ŒæŠŠèœè°±éƒ½æ”¶é›†èµ·æ¥æ”¾åœ¨è¿™å„¿ã€‚åªè¦æ²¡æœ‰å•ç‹¬è¯´çš„ï¼Œä¸€å¾‹æŒ‰é€‚é‡åŸåˆ™ã€‚</p>

<ol>
  <li><a href="#å®«ä¿é¸¡ä¸">å®«ä¿é¸¡ä¸</a></li>
  <li><a href="#æ²¹æ³¼çŒªæ‰‹">æ²¹æ³¼çŒªæ‰‹</a></li>
  <li><a href="#è…Šè‚‰ç³¯ç±³åœ†å­">è…Šè‚‰ç³¯ç±³åœ†å­</a></li>
</ol>

<h3 id="å®«ä¿é¸¡ä¸">å®«ä¿é¸¡ä¸</h3>
<p><img src="/assets/recipes/kong_pao_chicken.jpg" alt="å®«ä¿é¸¡ä¸" height="500px" /></p>

<p>å‡†å¤‡ææ–™ï¼š</p>
<ol>
  <li>è‚‰ï¼šé¸¡è…¿è‚‰å»éª¨ï¼Œåˆ‡ä¸ã€‚</li>
  <li>ä¸Šæµ†ï¼šä¸Šä¸€æ¬¡è‘±å§œæ°´ï¼ŒæŠ“åŒ€ï¼Œä¸Šè›‹æ¸…ï¼ŒæŠ“åŒ€ï¼Œå†ä¸Šä¸€æ¬¡å§œè‘±æ°´ï¼ŒæŠ“åŒ€ã€‚æ”¾å…¥ç”ŸæŠ½ã€è€æŠ½ã€èŠ±é›•é…’ï¼ŒæŠ“åŒ€ï¼Œå†æ”¾å…¥èƒ¡æ¤’ç²‰ã€ç›ã€æ·€ç²‰ï¼ŒæŠ“åŒ€ã€‚æœ€åç”¨<strong>å°‘é‡æ²¹å°ä½</strong>ã€‚</li>
  <li>å°æ–™ï¼šå¤§è‘±èŠ±ï¼Œè’œç‰‡ï¼Œå§œç‰‡ã€‚</li>
  <li>æ–™æ±å„¿ï¼šå°‘é‡ç›ï¼Œå¤šç‚¹ç³–ï¼Œèƒ¡æ¤’ç²‰ï¼Œæ–™é…’ï¼Œé…±æ²¹ï¼Œè€æŠ½ï¼Œé€‚é‡æ°´æ·€ç²‰ï¼Œæœ€ååŠ ä¸Šç‚¹<strong>è‘±å§œè’œç‰‡å°å‘³é“</strong>ã€‚</li>
  <li>èŠ±ç”Ÿï¼šçƒ­æ°´æ³¡äº†å»çš®ã€‚</li>
  <li>å¹²è¾£æ¤’ï¼šåˆ‡æ®µã€‚</li>
</ol>

<p>å¼€å§‹åšï¼š</p>
<ol>
  <li>å°ç«ç‚¸èŠ±ç”Ÿï¼Œæ…¢æ…¢ç‚¸åˆ°é¦™è„†ã€‚</li>
  <li>å¦èµ·é”…ï¼Œçƒ­é”…å†·æ²¹ï¼Œå°ç«æ»‘é¸¡ä¸ï¼Œæ»‘å¥½å€’å‡ºã€‚</li>
  <li>å°ç«ç…¸ç‚’èŠ±æ¤’ï¼Œæå‡ºèŠ±æ¤’ï¼Œç…¸å¹²è¾£æ¤’ã€‚</li>
  <li>ç…¸å¥½çš„è¾£æ¤’èŠ±æ¤’æ²¹ä¸­åŠ å…¥é¸¡ä¸ï¼Œå¼€å¤§ç«ï¼Œç¿»ç‚’å‡ ä¸‹ä¸‹è‘±å§œè’œï¼Œç‚’å‡ºé¦™å‘³ï¼Œä¸‹æ–™æ±å„¿ï¼Œçœ‹é¢œè‰²é€‚é‡æ”¾è€æŠ½ä¸Šè‰²ã€‚</li>
  <li>å‡ºé”…å‰å€’å…¥å°‘é‡èŠ±æ¤’æ²¹å’ŒèŠ±ç”Ÿã€‚</li>
  <li>ä¸Šèœï¼</li>
</ol>

<p>æ³¨æ„ï¼š</p>
<ul>
  <li>ä¸Šæµ†ä¸€å®šè¦è€å¿ƒï¼Œæ¯æ”¾ä¸€æ¬¡è°ƒæ–™å°±è¦æŠ“åŒ€ã€‚</li>
  <li>è°ƒå¥½çš„æ–™æ±å„¿éœ€è¦å°ä¸€å°ï¼Œæ ¹æ®å‘³é“å†åšè°ƒæ•´ï¼Œç¼ºå•¥è¡¥å•¥ã€‚</li>
  <li>èŠ±ç”Ÿéœ€è¦å°ç«æ…¢ç‚¸ï¼Œç‚¸åˆ°é¦™è„†ï¼Œå¾®é»„ã€‚</li>
</ul>

<h3 id="æ²¹æ³¼çŒªæ‰‹">æ²¹æ³¼çŒªæ‰‹</h3>
<p><img src="/assets/recipes/splash_oil_pig_trotters.jpg" alt="æ²¹æ³¼çŒªæ‰‹" height="500px" /></p>

<p>å‡†å¤‡ææ–™ï¼š</p>
<ol>
  <li>æ´—å‡€å°çŒªæ‰‹ã€‚</li>
  <li>å°è‘±åˆ‡æ®µï¼Œå§œåˆ‡ç‰‡ï¼Œè’œåˆ‡ç‰‡ã€‚</li>
</ol>

<p>å¼€å§‹åšï¼š</p>
<ol>
  <li>ç„¯æ°´ï¼šå°çŒªæ‰‹å†·æ°´ä¸‹é”…ç„¯æ°´ï¼Œ<strong>æ°´ä¸­æ”¾é†‹</strong>ã€‚</li>
  <li>å¤‡æ±¤ï¼šé«˜å‹é”…å†…æ”¾æ°´ï¼Œç›ï¼ŒèŠ±é›•é…’ï¼Œè‘±å¶ï¼Œå§œç‰‡ã€‚</li>
  <li>ç‚–è‚‰ï¼šç„¯å¥½æ°´çš„çŒªè„šæ‹¿å‡ºç”¨çƒ­æ°´æ¸…æ´—å¹²å‡€ï¼Œæ”¾å…¥é«˜å‹é”…ï¼Œç‚–å››ååˆ†é’Ÿå·¦å³ã€‚</li>
  <li>å°ç«ç‚¸èŠ±ç”Ÿï¼ŒæŠŠèŠ±ç”Ÿç‚¸è‡³é¦™è„†ã€‚</li>
  <li>çŒªæ‰‹ç‚–å¥½æ‹¿å‡ºæ¥æ³¡åœ¨å†°æ°´é‡Œé™æ¸©ï¼Œç”¨æ‰‹å°½é‡æ°æˆå°å—å„¿çŠ¶ï¼Œæ–¹ä¾¿å…¥å‘³ã€‚</li>
  <li>æ²¥å¹²æ°´åˆ†çš„çŒªæ‰‹ä¸­æ”¾å…¥èŠ±ç”Ÿç±³ï¼Œå°è‘±ï¼Œè’œç‰‡ï¼Œé¦™æ²¹ï¼Œç›ï¼Œé†‹ï¼Œæ…æ‹Œå‡åŒ€ã€‚</li>
  <li>å¦èµ·é”…ï¼Œæ”¾å…¥æ²¹å’ŒèŠ±æ¤’ï¼Œ<strong>å°ç«ç…¸å‡ºèŠ±æ¤’æ²¹</strong>ã€‚</li>
  <li>æ‹Œå¥½çš„çŒªæ‰‹ä¸­æ”¾å…¥è¾£æ¤’é¢ï¼ˆé‡ä¾ç…§ä¸ªäººå£å‘³ï¼‰ï¼Œæ³¼ä¸Šçƒ­çš„èŠ±æ¤’æ²¹ï¼ˆæ²¹æ³¼çŒªæ‰‹ï¼‰ã€‚</li>
  <li>ä¸Šèœï¼</li>
</ol>

<p>æ³¨æ„ï¼š</p>
<ul>
  <li>é«˜å‹é”…ç‚–ä¹…ä¸€ç‚¹èƒ½ä½¿çŒªæ‰‹æ›´è½¯ç³¯ï¼Œæ ¹æ®å–œå¥½çš„å£æ„Ÿå¯ä»¥è‡ªå·±é€‰æ‹©æ—¶é•¿ã€‚</li>
  <li>æå‰å¤‡å¥½è¶³å¤Ÿçš„å†°å—ï¼ŒçŒªæ‰‹å‡ºé”…åè®©å®ƒå°½é‡å†·å´ï¼Œä¸ç„¶æ‰‹æ°èµ·æ¥éå¸¸çƒ«ã€‚</li>
  <li>è¾£æ¤’é¢æ ¹æ®è‡ªå·±å–œå¥½æ”¾å…¥ï¼Œä¸èƒ½åƒè¾£çš„æ³¨æ„åˆ«æ”¾å¤ªå¤šã€‚</li>
</ul>

<h3 id="è…Šè‚‰ç³¯ç±³åœ†å­">è…Šè‚‰ç³¯ç±³åœ†å­</h3>
<p><img src="/assets/recipes/sticky_rice_ball.jpg" alt="è…Šè‚‰ç³¯ç±³åœ†å­" height="500px" /></p>
 -->
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

    </div>

  </body>
</html>
